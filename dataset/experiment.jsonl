{"repo": "Significant-Gravitas/AutoGPT", "pr_id": 10976, "file_path": "autogpt_platform/backend/backend/util/cloud_storage.py", "diff_hunk": "@@ -203,6 +253,14 @@ async def _retrieve_file_gcs(\n         self, path: str, user_id: str | None = None, graph_exec_id: str | None = None\n     ) -> bytes:\n         \"\"\"Retrieve file from Google Cloud Storage with authorization.\"\"\"\n+        # Log context for debugging\n+        current_task = asyncio.current_task()\n+        logger.info(\n+            f\"_retrieve_file_gcs called - path: {path}, \"", "context": "    def is_cloud_path(self, path: str) -> bool:\n        \"\"\"Check if a path is a cloud storage path.\"\"\"\n        return path.startswith((\"gcs://\", \"s3://\", \"azure://\"))\n", "comment": "Is the path user provided or  autogenerated ? if it's user provided this is a PII and we should remove it", "comment_author": "majdyz", "comment_url": "https://github.com/Significant-Gravitas/AutoGPT/pull/10976#discussion_r2373811501", "type": "security"}
{"repo": "ytdl-org/youtube-dl", "pr_id": 32987, "file_path": "youtube_dl/extractor/youtube.py", "diff_hunk": "@@ -1914,9 +1943,50 @@ def _real_extract(self, url):\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:", "context": "        if webpage:\n            player_response = self._extract_yt_initial_variable(\n                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                video_id, 'initial player response')\n        if not player_response:\n            player_response = self._call_api(\n                'player', {'videoId': video_id}, video_id)\n\n        def is_agegated(playability):\n            if not isinstance(playability, dict):\n                return\n\n            if playability.get('desktopLegacyAgeGateReason'):\n                return True", "comment": "The \"False and\" looks like a temporary stopgap. It could thus use an inline comment explaining where to read about  what problem it mitigates and what work is needed to fix it properly. Ideally the explanation would be a link to an issue thread.\r\nEdit: Now seeing the \"True or\" a few lines ahead, it may be better to just set `player_response = False` and mark that as the stopgap, rather than having two bogus conditions.\r\nAnyway, thanks for your great work! :+1:", "comment_author": "mk-pmb", "comment_url": "https://github.com/ytdl-org/youtube-dl/pull/32987#discussion_r1885483525", "type": "logic"}
{"repo": "langflow-ai/langflow", "pr_id": 9891, "file_path": "src/backend/base/langflow/main.py", "diff_hunk": "@@ -245,15 +245,15 @@ async def lifespan(_app: FastAPI):\n             await logger.adebug(f\"Total initialization time: {total_time:.2f}s\")\n \n             async def delayed_init_mcp_servers():\n-                await asyncio.sleep(3.0)\n+                await asyncio.sleep(10.0)  # Increased delay to allow starter projects to be created", "context": "def get_lifespan(*, fix_migration=False, version=None):\n    initialize_settings_service()\n    telemetry_service = get_telemetry_service()\n\n    @asynccontextmanager\n    async def lifespan(_app: FastAPI):\n        from lfx.interface.components import get_and_cache_all_types_dict\n\n        configure()\n\n        # Startup message\n        if version:\n            await logger.adebug(f\"Starting Langflow v{version}...\")\n        else:\n            await logger.adebug(\"Starting Langflow...\")\n\n        temp_dirs: list[TemporaryDirectory] = []\n        sync_flows_from_fs_task = None\n        mcp_init_task = None\n\n        try:\n            start_time = asyncio.get_event_loop().time()\n\n            await logger.adebug(\"Initializing services\")\n            await initialize_services(fix_migration=fix_migration)\n            await logger.adebug(f\"Services initialized in {asyncio.get_event_loop().time() - start_time:.2f}s\")\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Setting up LLM caching\")\n            setup_llm_caching()\n            await logger.adebug(f\"LLM caching setup in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            if get_settings_service().auth_settings.AUTO_LOGIN:\n                current_time = asyncio.get_event_loop().time()\n                await logger.adebug(\"Initializing default super user\")\n                await initialize_auto_login_default_superuser()\n                await logger.adebug(\n                    f\"Default super user initialized in {asyncio.get_event_loop().time() - current_time:.2f}s\"\n                )\n\n            await logger.adebug(\"Initializing super user\")\n            await initialize_auto_login_default_superuser()\n            await logger.adebug(f\"Super user initialized in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Loading bundles\")\n            temp_dirs, bundles_components_paths = await load_bundles_with_error_handling()\n            get_settings_service().settings.components_path.extend(bundles_components_paths)\n            await logger.adebug(f\"Bundles loaded in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Caching types\")\n            all_types_dict = await get_and_cache_all_types_dict(get_settings_service())\n            await logger.adebug(f\"Types cached in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            # Use file-based lock to prevent multiple workers from creating duplicate starter projects concurrently.\n            # Note that it's still possible that one worker may complete this task, release the lock,\n            # then another worker pick it up, but the operation is idempotent so worst case it duplicates\n            # the initialization work.\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Creating/updating starter projects\")\n            import tempfile\n\n            from filelock import FileLock\n\n            lock_file = Path(tempfile.gettempdir()) / \"langflow_starter_projects.lock\"\n            lock = FileLock(lock_file, timeout=1)\n            try:\n                with lock:\n                    await create_or_update_starter_projects(all_types_dict)\n                    await logger.adebug(\n                        f\"Starter projects created/updated in {asyncio.get_event_loop().time() - current_time:.2f}s\"\n                    )\n            except TimeoutError:\n                # Another process has the lock\n                await logger.adebug(\"Another worker is creating starter projects, skipping\")\n            except Exception as e:  # noqa: BLE001\n                await logger.awarning(\n                    f\"Failed to acquire lock for starter projects: {e}. Starter projects may not be created or updated.\"\n                )\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Starting telemetry service\")\n            telemetry_service.start()\n            await logger.adebug(f\"started telemetry service in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Starting MCP Composer service\")\n            mcp_composer_service = cast(\"MCPComposerService\", get_service(ServiceType.MCP_COMPOSER_SERVICE))\n            await mcp_composer_service.start()\n            await logger.adebug(\n                f\"started MCP Composer service in {asyncio.get_event_loop().time() - current_time:.2f}s\"\n            )\n\n            current_time = asyncio.get_event_loop().time()\n            await logger.adebug(\"Loading flows\")\n            await load_flows_from_directory()\n            sync_flows_from_fs_task = asyncio.create_task(sync_flows_from_fs())\n            queue_service = get_queue_service()\n            if not queue_service.is_started():  # Start if not already started\n                queue_service.start()\n            await logger.adebug(f\"Flows loaded in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n\n            total_time = asyncio.get_event_loop().time() - start_time\n            await logger.adebug(f\"Total initialization time: {total_time:.2f}s\")\n\n            async def delayed_init_mcp_servers():\n                await asyncio.sleep(3.0)\n                current_time = asyncio.get_event_loop().time()\n                await logger.adebug(\"Loading mcp servers for projects\")\n                try:\n                    await init_mcp_servers()\n                    await logger.adebug(f\"mcp servers loaded in {asyncio.get_event_loop().time() - current_time:.2f}s\")\n                except Exception as e:  # noqa: BLE001\n                    await logger.awarning(f\"First MCP server initialization attempt failed: {e}\")\n                    await asyncio.sleep(3.0)\n                    current_time = asyncio.get_event_loop().time()\n                    await logger.adebug(\"Retrying mcp servers initialization\")\n                    try:\n                        await init_mcp_servers()\n                        await logger.adebug(\n                            f\"mcp servers loaded on retry in {asyncio.get_event_loop().time() - current_time:.2f}s\"\n                        )\n                    except Exception as e2:  # noqa: BLE001\n                        await logger.aexception(f\"Failed to initialize MCP servers after retry: {e2}\")\n\n            # Start the delayed initialization as a background task\n            # Allows the server to start first to avoid race conditions with MCP Server startup\n            mcp_init_task = asyncio.create_task(delayed_init_mcp_servers())\n\n            yield\n\n        except asyncio.CancelledError:\n            await logger.adebug(\"Lifespan received cancellation signal\")\n        except Exception as exc:\n            if \"langflow migration --fix\" not in str(exc):\n                logger.exception(exc)\n\n                await log_exception_to_telemetry(exc, \"lifespan\")\n            raise\n        finally:\n            # Clean shutdown with progress indicator\n            # Create shutdown progress (show verbose timing if log level is DEBUG)\n            from langflow.__main__ import get_number_of_workers\n            from langflow.cli.progress import create_langflow_shutdown_progress\n\n            log_level = os.getenv(\"LANGFLOW_LOG_LEVEL\", \"info\").lower()\n            num_workers = get_number_of_workers(get_settings_service().settings.workers)\n            shutdown_progress = create_langflow_shutdown_progress(\n                verbose=log_level == \"debug\", multiple_workers=num_workers > 1\n            )\n\n            try:\n                # Step 0: Stopping Server\n                with shutdown_progress.step(0):\n                    await logger.adebug(\"Stopping server gracefully...\")\n                    # The actual server stopping is handled by the lifespan context\n                    await asyncio.sleep(0.1)  # Brief pause for visual effect\n\n                # Step 1: Cancelling Background Tasks\n                with shutdown_progress.step(1):\n                    tasks_to_cancel = []\n                    if sync_flows_from_fs_task:\n                        sync_flows_from_fs_task.cancel()\n                        tasks_to_cancel.append(sync_flows_from_fs_task)\n                    if mcp_init_task and not mcp_init_task.done():\n                        mcp_init_task.cancel()\n                        tasks_to_cancel.append(mcp_init_task)\n                    if tasks_to_cancel:\n                        # Wait for all tasks to complete, capturing exceptions\n                        results = await asyncio.gather(*tasks_to_cancel, return_exceptions=True)\n                        # Log any non-cancellation exceptions\n                        for result in results:\n                            if isinstance(result, Exception) and not isinstance(result, asyncio.CancelledError):\n                                await logger.aerror(f\"Error during task cleanup: {result}\", exc_info=result)\n\n                # Step 2: Cleaning Up Services\n                with shutdown_progress.step(2):\n                    try:\n                        await asyncio.wait_for(teardown_services(), timeout=30)\n                    except asyncio.TimeoutError:\n                        await logger.awarning(\"Teardown services timed out after 30s.\")\n\n                # Step 3: Clearing Temporary Files\n                with shutdown_progress.step(3):\n                    temp_dir_cleanups = [asyncio.to_thread(temp_dir.cleanup) for temp_dir in temp_dirs]\n                    try:\n                        await asyncio.wait_for(asyncio.gather(*temp_dir_cleanups), timeout=10)\n                    except asyncio.TimeoutError:\n                        await logger.awarning(\"Temporary file cleanup timed out after 10s.\")\n\n                # Step 4: Finalizing Shutdown\n                with shutdown_progress.step(4):\n                    await logger.adebug(\"Langflow shutdown complete\")\n\n                # Show completion summary and farewell\n                shutdown_progress.print_shutdown_summary()\n\n            except (sqlalchemy.exc.OperationalError, sqlalchemy.exc.DBAPIError) as e:\n                # Case where the database connection is closed during shutdown\n                await logger.awarning(f\"Database teardown failed due to closed connection: {e}\")\n            except asyncio.CancelledError:\n                # Swallow this - it's normal during shutdown\n                await logger.adebug(\"Teardown cancelled during shutdown.\")\n            except Exception as e:  # noqa: BLE001\n                await logger.aexception(f\"Unhandled error during cleanup: {e}\")\n                await log_exception_to_telemetry(e, \"lifespan_cleanup\")\n\n    return lifespan\n\n", "comment": "This is really long, and will affect server startup to query time. Do you have quantitative evidence that this is required?", "comment_author": "jordanrfrazier", "comment_url": "https://github.com/langflow-ai/langflow/pull/9891#discussion_r2363989016", "type": "performance"}
{"repo": "django/django", "pr_id": 17554, "file_path": "django/db/models/query_utils.py", "diff_hunk": "@@ -241,6 +242,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})", "context": "    def referenced_base_fields(self):\n        \"\"\"\n        Retrieve all base fields referenced directly or through F expressions\n        excluding any fields referenced through joins.\n        \"\"\"\n        # Avoid circular imports.\n        from django.db.models.sql import query\n\n        return {\n            child.split(LOOKUP_SEP, 1)[0] for child in query.get_children_from_q(self)\n        }\n\n", "comment": "I wonder if the set comprehension here could have some performance impact for a large number of `instances`. I'm not sure what else could be done instead though.", "comment_author": "g-nie", "comment_url": "https://github.com/django/django/pull/17554#discussion_r2062580287", "type": "performance"}
{"repo": "python/cpython", "pr_id": 140611, "file_path": "Lib/test/test_io/test_general.py", "diff_hunk": "@@ -604,10 +604,9 @@ def readinto(self, b):\n         with self.assertRaises(ValueError) as cm:\n             Misbehaved(2).read(1)\n         self.assertEqual(str(cm.exception), \"readinto returned '2' oustside buffer size '1'\")\n-        self.assertRaises(ValueError, Misbehaved(2147483647).read)\n-        self.assertRaises(ValueError, Misbehaved(sys.maxsize).read)\n-        self.assertRaises(ValueError, Misbehaved(-1).read)\n-        self.assertRaises(ValueError, Misbehaved(-1000).read)\n+        for bad_size in (2147483647, sys.maxsize, -1, -1000):\n+          with self.assertRaises(ValueError):", "context": "    def test_opener(self):\n        with self.open(os_helper.TESTFN, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"egg\\n\")\n        fd = os.open(os_helper.TESTFN, os.O_RDONLY)\n        def opener(path, flags):\n            return fd\n        with self.open(\"non-existent\", \"r\", encoding=\"utf-8\", opener=opener) as f:\n            self.assertEqual(f.read(), \"egg\\n\")\n", "comment": "Indent problem", "comment_author": "efimov-mikhail", "comment_url": "https://github.com/python/cpython/pull/140611#discussion_r2464014847", "type": "style"}
{"repo": "ansible/ansible", "pr_id": 84878, "file_path": "lib/ansible/plugins/become/__init__.py", "diff_hunk": "@@ -34,6 +34,8 @@ class BecomeBase(AnsiblePlugin):\n     # plugin requires a tty, i.e su\n     require_tty = False\n \n+    has_pipelining = True", "context": "def _gen_id(length=32):\n    \"\"\" return random string used to identify the current privilege escalation \"\"\"\n    return ''.join(choice(ascii_lowercase) for x in range(length))\n\n", "comment": "Minor suggestion: Consider adding a brief doc comment to the `has_pipelining = True`  line to clarify its purpose for future reference.", "comment_author": "a-holm", "comment_url": "https://github.com/ansible/ansible/pull/84878#discussion_r2028694438", "type": "documentation"}
{"repo": "ansible/ansible", "pr_id": 85545, "file_path": "lib/ansible/galaxy/dependency_resolution/providers.py", "diff_hunk": "@@ -47,15 +47,15 @@ class CollectionDependencyProviderBase(AbstractProvider):\n     \"\"\"Delegate providing a requirement interface for the resolver.\"\"\"\n \n     def __init__(\n-            self,  # type: CollectionDependencyProviderBase\n-            apis,  # type: MultiGalaxyAPIProxy\n-            concrete_artifacts_manager=None,  # type: ConcreteArtifactsManager\n-            preferred_candidates=None,  # type: t.Iterable[Candidate]\n-            with_deps=True,  # type: bool\n-            with_pre_releases=False,  # type: bool\n-            upgrade=False,  # type: bool\n-            include_signatures=True,  # type: bool\n-    ):  # type: (...) -> None\n+            self: CollectionDependencyProviderBase,", "context": "    def __init__(\n            self,\n            apis,  # type: MultiGalaxyAPIProxy\n            concrete_artifacts_manager=None,  # type: ConcreteArtifactsManager\n            preferred_candidates=None,  # type: t.Iterable[Candidate]\n            with_deps=True,  # type: bool\n            with_pre_releases=False,  # type: bool\n            upgrade=False,  # type: bool\n            include_signatures=True,  # type: bool", "comment": "Is there a particular reason this `self` is annotated?", "comment_author": "jborean93", "comment_url": "https://github.com/ansible/ansible/pull/85545#discussion_r2306217276", "type": "style"}
{"repo": "ansible/ansible", "pr_id": 85538, "file_path": "lib/ansible/playbook/play.py", "diff_hunk": "@@ -308,17 +308,21 @@ def compile(self):\n         t.implicit = True\n         t.set_loader(self._loader)\n \n-        if self.tags:\n-            # Avoid calling flush_handlers in case the whole play is skipped on tags,\n-            # this could be performance improvement since calling flush_handlers on\n-            # large inventories could be expensive even if no hosts are notified\n-            # since we call flush_handlers per host.\n-            # Block.filter_tagged_tasks ignores evaluating tags on implicit meta\n-            # tasks so we need to explicitly call Task.evaluate_tags here.\n-            t.tags = self.tags\n-            if t.evaluate_tags(self.only_tags, self.skip_tags, all_vars=self.vars):\n-                flush_block.block = [t]\n-        else:\n+        # Avoid calling flush_handlers in case the whole play is skipped on tags,\n+        # this could be performance improvement since calling flush_handlers on\n+        # large inventories could be expensive even if no hosts are notified\n+        # since we call flush_handlers per host.\n+        # Block.filter_tagged_tasks ignores evaluating tags on implicit meta\n+        # tasks so we need to explicitly call Task.evaluate_tags here.\n+        if (\n+            not self.tags\n+            or not self.skip_tags\n+            or t.evaluate_tags(\n+                only_tags=[],", "context": "    def compile(self):\n        \"\"\"\n        Compiles and returns the task list for this play, compiled from the\n        roles (which are themselves compiled recursively) and/or the list of\n        tasks specified in the play.\n        \"\"\"\n        # create a block containing a single flush handlers meta\n        # task, so we can be sure to run handlers at certain points\n        # of the playbook execution\n        flush_block = Block(play=self)\n\n        t = Task(block=flush_block)\n        t.action = 'meta'\n        t._resolved_action = 'ansible.builtin.meta'\n        t.args['_raw_params'] = 'flush_handlers'\n        t.implicit = True\n        t.set_loader(self._loader)\n\n        if self.tags:\n            # Avoid calling flush_handlers in case the whole play is skipped on tags,\n            # this could be performance improvement since calling flush_handlers on\n            # large inventories could be expensive even if no hosts are notified\n            # since we call flush_handlers per host.\n            # Block.filter_tagged_tasks ignores evaluating tags on implicit meta\n            # tasks so we need to explicitly call Task.evaluate_tags here.\n            t.tags = self.tags\n            if t.evaluate_tags(self.only_tags, self.skip_tags, all_vars=self.vars):\n                flush_block.block = [t]\n        else:\n            flush_block.block = [t]\n\n        # NOTE keep flush_handlers tasks even if a section has no regular tasks,\n        #      there may be notified handlers from the previous section\n        #      (typically when a handler notifies a handler defined before)\n        block_list = []\n        if self.force_handlers:\n            noop_task = Task()\n            noop_task.action = 'meta'\n            noop_task.args['_raw_params'] = 'noop'\n            noop_task.implicit = True\n            noop_task.set_loader(self._loader)\n\n            b = Block(play=self)\n            if self.pre_tasks:\n                b.block = self.pre_tasks\n            else:\n                nt = noop_task.copy(exclude_parent=True)\n                nt._parent = b\n                b.block = [nt]\n            b.always = [flush_block]\n            block_list.append(b)\n\n            tasks = self._compile_roles() + self.tasks\n            b = Block(play=self)\n            if tasks:\n                b.block = tasks\n            else:\n                nt = noop_task.copy(exclude_parent=True)\n                nt._parent = b\n                b.block = [nt]\n            b.always = [flush_block]\n            block_list.append(b)\n\n            b = Block(play=self)\n            if self.post_tasks:\n                b.block = self.post_tasks\n            else:\n                nt = noop_task.copy(exclude_parent=True)\n                nt._parent = b\n                b.block = [nt]\n            b.always = [flush_block]\n            block_list.append(b)\n\n            return block_list\n\n        block_list.extend(self.pre_tasks)\n        block_list.append(flush_block)\n        block_list.extend(self._compile_roles())\n        block_list.extend(self.tasks)\n        block_list.append(flush_block)\n        block_list.extend(self.post_tasks)\n        block_list.append(flush_block)\n\n        return block_list\n", "comment": "`only_tags=self.only_tags` ? not sure why you changed into an empty list. There are several cases `only_tags = 'a', tags ='b'`, that it should not execute or other in which it should force execution `only_tags='a', tags='b, always'`  `always` should be part of flush task unless it is overridden.", "comment_author": "bcoca", "comment_url": "https://github.com/ansible/ansible/pull/85538#discussion_r2248186812", "type": "logic"}
{"repo": "ansible/ansible", "pr_id": 78421, "file_path": "lib/ansible/plugins/connection/__init__.py", "diff_hunk": "@@ -37,6 +38,30 @@ def wrapped(self, *args, **kwargs):\n     return wrapped\n \n \n+def sanitise_become_output(become: BecomeBase, stdout: bytes, stderr: bytes) -> t.Tuple[bytes, bytes]:\n+    \"\"\"Helper function to remove the become success banner from stdout or stderr.\"\"\"\n+    if not become.success:\n+        return stdout, stderr\n+\n+    raw = {'stdout': stdout, 'stderr': stderr}\n+\n+    for stream_type in ['stdout', 'stderr']:\n+        success_idx = None\n+        lines = raw[stream_type].split(b\"\\n\")", "context": "    def wrapped(self, *args, **kwargs):\n        if not self._connected:\n            self._connect()\n        return func(self, *args, **kwargs)", "comment": "Doubling the memory usage seems potentially expensive when stdout/stderr is big. Have you considered using a generator-based streaming approach?", "comment_author": "webknjaz", "comment_url": "https://github.com/ansible/ansible/pull/78421#discussion_r941292687", "type": "performance"}
{"repo": "scikit-learn/scikit-learn", "pr_id": 32497, "file_path": "sklearn/utils/validation.py", "diff_hunk": "@@ -2148,6 +2150,8 @@ def _check_sample_weight(\n     else:\n         if force_float_dtype and dtype is None:\n             dtype = float_dtypes\n+        if ensure_same_device:\n+            sample_weight = xp.asarray(sample_weight, device=device)", "context": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    copy=False,", "comment": "We can probably do the dtype conversion there as well, no? That might spare a temporary allocation on the device in some cases.\n\n\n```suggestion\n            sample_weight = xp.asarray(sample_weight, dtype=dtype, device=device)\n```", "comment_author": "ogrisel", "comment_url": "https://github.com/scikit-learn/scikit-learn/pull/32497#discussion_r2450803667", "type": "performance"}
{"repo": "scikit-learn/scikit-learn", "pr_id": 32246, "file_path": "sklearn/calibration.py", "diff_hunk": "@@ -1083,14 +1124,26 @@ def log_loss(log_beta=0.0):\n             #   - NumPy 2+:  result.dtype is float64\n             #\n             #  This can cause dtype mismatch errors downstream (e.g., buffer dtype).\n-            raw_prediction = (np.exp(log_beta) * logits).astype(dtype_)\n-            return halfmulti_loss(y_true=labels, raw_prediction=raw_prediction)\n+            if _is_numpy_namespace(xp):\n+                raw_prediction = (np.exp(log_beta) * logits).astype(dtype_)\n+                return halfmulti_loss(\n+                    y_true=labels,\n+                    raw_prediction=raw_prediction,\n+                    sample_weight=sample_weight,\n+                )\n \n+            log_beta = xp.asarray(log_beta, dtype=dtype_, device=xp_device)\n+            raw_prediction = xp.exp(log_beta) * logits\n+            return _half_multinomial_loss(\n+                y=labels, pred=raw_prediction, sample_weight=sample_weight, xp=xp\n+            )", "context": "        def log_loss(log_beta=0.0):\n            \"\"\"Compute the log loss as a parameter of the inverse temperature\n            (beta).\n\n            Parameters\n            ----------\n            log_beta : float\n                The current logarithm of the inverse temperature value during\n                optimisation.\n\n            Returns\n            -------\n            negative_log_likelihood_loss : float\n                The negative log likelihood loss.\n\n            \"\"\"\n            # TODO: numpy 2.0\n            # Ensure raw_prediction has the same dtype as labels using .astype().\n            # Without this, dtype promotion rules differ across NumPy versions:\n            #\n            #   beta = np.float64(0)\n            #   logits = np.array([1, 2], dtype=np.float32)\n            #\n            #   result = beta * logits\n            #   - NumPy < 2: result.dtype is float32\n            #   - NumPy 2+:  result.dtype is float64\n            #\n            #  This can cause dtype mismatch errors downstream (e.g., buffer dtype).\n            raw_prediction = (np.exp(log_beta) * logits).astype(dtype_)\n            return halfmulti_loss(y_true=labels, raw_prediction=raw_prediction)\n", "comment": "We can probably call `_is_numpy_namespace(xp)` just once, instead of doing so in every iteration of the optimisation process.\r\n\r\nWe could initialise the half multinomial loss as follows:\r\n\r\n```python\r\nif _is_numpy_namespace(xp):\r\n    loss = HalfMultinomialLoss(n_classes=logits.shape[1])\r\n\r\nelse:\r\n    loss = partial(_half_multinomial_loss, xp=xp)\r\n```\r\nThen `log_loss` would look like this:\r\n\r\n```python\r\ndef log_loss(log_beta=0.0):\r\n    log_beta = xp.asarray(log_beta, dtype=dtype_, device=xp_device)\r\n    raw_prediction = xp.exp(log_beta) * logits\r\n    return loss(labels, raw_prediction, sample_weight)\r\n```\r\n\r\nThe (CUDA) CI passed locally on my machine.", "comment_author": "virchan", "comment_url": "https://github.com/scikit-learn/scikit-learn/pull/32246#discussion_r2415336128", "type": "performance"}
{"repo": "keras-team/keras", "pr_id": 21732, "file_path": "keras/src/export/onnx_test.py", "diff_hunk": "@@ -77,9 +77,11 @@ def get_model(type=\"sequential\", input_shape=(10,), layer_list=None):\n         \"backends.\"\n     ),\n )\n-@pytest.mark.skipif(testing.jax_uses_gpu(), reason=\"Leads to core dumps on CI\")\n @pytest.mark.skipif(\n-    testing.tensorflow_uses_gpu(), reason=\"Leads to core dumps on CI\"\n+    testing.jax_uses_gpu()\n+    or testing.tensorflow_uses_gpu", "context": "def get_model(type=\"sequential\", input_shape=(10,), layer_list=None):\n    layer_list = layer_list or [\n        layers.Dense(10, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n    if type == \"sequential\":\n        return models.Sequential(layer_list)\n    elif type == \"functional\":\n        input = output = tree.map_shape_structure(layers.Input, input_shape)\n        for layer in layer_list:\n            output = layer(output)\n        return models.Model(inputs=input, outputs=output)\n    elif type == \"subclass\":\n        return CustomModel(layer_list)\n    elif type == \"lstm\":\n        # https://github.com/keras-team/keras/issues/21390\n        inputs = layers.Input((4, 10))\n        x = layers.Bidirectional(\n            layers.LSTM(\n                10,\n                kernel_initializer=\"he_normal\",\n                return_sequences=True,\n                kernel_regularizer=None,\n            ),\n            merge_mode=\"sum\",\n        )(inputs)\n        outputs = layers.Bidirectional(\n            layers.LSTM(\n                10,\n                kernel_initializer=\"he_normal\",\n                return_sequences=True,\n                kernel_regularizer=None,\n            ),\n            merge_mode=\"concat\",\n        )(x)\n        return models.Model(inputs=inputs, outputs=outputs)\n\n\n@pytest.mark.skipif(\n    backend.backend() not in (\"tensorflow\", \"jax\", \"torch\"),\n    reason=(\n        \"`export_onnx` only currently supports the tensorflow, jax and torch \"\n        \"backends.\"\n    ),", "comment": "`testing.tensorflow_uses_gpu` --> `testing.tensorflow_uses_gpu()`?", "comment_author": "abheesht17", "comment_url": "https://github.com/keras-team/keras/pull/21732#discussion_r2430379392", "type": "logic"}
{"repo": "keras-team/keras", "pr_id": 21751, "file_path": "keras/src/layers/preprocessing/index_lookup.py", "diff_hunk": "@@ -382,6 +388,12 @@ def set_vocabulary(self, vocabulary, idf_weights=None):\n             )\n \n         if isinstance(vocabulary, str):\n+            if serialization_lib.in_safe_mode():\n+                raise ValueError(\n+                    \"Loading vocabulary files outside of the model archive \"\n+                    \"being reloaded is not allowed\"", "context": "    def set_vocabulary(self, vocabulary, idf_weights=None):\n        \"\"\"Sets vocabulary (and optionally document frequency) for this layer.\n\n        This method sets the vocabulary and idf weights for this layer directly,\n        instead of analyzing a dataset through `adapt`. It should be used\n        whenever the vocab (and optionally document frequency) information is\n        already known.  If vocabulary data is already present in the layer, this\n        method will replace it.\n\n        Args:\n            vocabulary: Either an array or a string path to a text file.\n                If passing an array, can pass a tuple, list,\n                1D numpy array, or 1D tensor containing the vocbulary terms.\n                If passing a file path, the file should contain one line\n                per term in the vocabulary.\n            idf_weights: A tuple, list, 1D numpy array, or 1D tensor\n                of inverse document frequency weights with equal\n                length to vocabulary. Must be set if `output_mode`\n                is `\"tf_idf\"`. Should not be set otherwise.\n        \"\"\"\n        if self.output_mode == \"tf_idf\":\n            if idf_weights is None:\n                raise ValueError(\n                    \"`idf_weights` must be set if output_mode is 'tf_idf'.\"\n                )\n        elif idf_weights is not None:\n            raise ValueError(\n                \"`idf_weights` should only be set if output_mode is \"\n                f\"`'tf_idf'`. Received: output_mode={self.output_mode} \"\n                f\"and idf_weights={idf_weights}\"\n            )\n\n        if isinstance(vocabulary, str):\n            if not tf.io.gfile.exists(vocabulary):\n                raise ValueError(\n                    f\"Vocabulary file {vocabulary} does not exist.\"\n                )\n            if self.output_mode == \"tf_idf\":\n                raise ValueError(\n                    \"output_mode `'tf_idf'` does not support loading a \"\n                    \"vocabulary from file.\"\n                )\n            self.lookup_table = self._lookup_table_from_file(vocabulary)\n            self._record_vocabulary_size()\n            return\n\n        if not tf.executing_eagerly() and (\n            tf.is_tensor(vocabulary) or tf.is_tensor(idf_weights)\n        ):\n            raise RuntimeError(\n                f\"Cannot set a tensor vocabulary on layer {self.name} \"\n                \"when not executing eagerly. \"\n                \"Create this layer or call `set_vocabulary()` \"\n                \"outside of any traced function.\"\n            )\n\n        # TODO(mattdangerw): for better performance we should rewrite this\n        # entire function to operate on tensors and convert vocabulary to a\n        # tensor here.\n        if tf.is_tensor(vocabulary):\n            vocabulary = self._tensor_vocab_to_numpy(vocabulary)\n        elif isinstance(vocabulary, (list, tuple)):\n            vocabulary = np.array(vocabulary)\n        if tf.is_tensor(idf_weights):\n            idf_weights = idf_weights.numpy()\n        elif isinstance(idf_weights, (list, tuple)):\n            idf_weights = np.array(idf_weights)\n\n        if vocabulary.size == 0:\n            raise ValueError(\n                \"Cannot set an empty vocabulary. \"\n                f\"Received: vocabulary={vocabulary}\"\n            )\n\n        oov_start = self._oov_start_index()\n        token_start = self._token_start_index()\n        special_tokens = [self.mask_token] * oov_start + [\n            self.oov_token\n        ] * self.num_oov_indices\n        found_special_tokens = np.array_equal(\n            special_tokens, vocabulary[:token_start]\n        )\n        if found_special_tokens:\n            tokens = vocabulary[token_start:]\n        else:\n            tokens = vocabulary\n\n        repeated_tokens = self._find_repeated_tokens(tokens)\n        if repeated_tokens:\n            raise ValueError(\n                \"The passed vocabulary has at least one repeated \"\n                \"term. Please uniquify your dataset. The repeated terms \"\n                f\"are: {repeated_tokens}\"\n            )\n\n        if self.mask_token is not None and self.mask_token in tokens:\n            mask_index = np.argwhere(vocabulary == self.mask_token)[-1]\n            raise ValueError(\n                \"Found reserved mask token at unexpected location in \"\n                \"`vocabulary`. Note that passed `vocabulary` does not need to \"\n                \"include the OOV and mask tokens. Either remove all mask and \"\n                \"OOV tokens, or include them only at the start of the \"\n                f\"vocabulary in precisely this order: {special_tokens}. \"\n                f\"Received: mask_token={self.mask_token} at \"\n                f\"vocabulary index {mask_index}\"\n            )\n        # Only error out for oov_token when invert=True. When invert=False,\n        # oov_token is unused during lookup.\n        if (\n            self.oov_token is not None\n            and self.invert\n            and self.oov_token in tokens\n        ):\n            oov_index = np.argwhere(vocabulary == self.oov_token)[-1]\n            raise ValueError(\n                \"Found reserved OOV token at unexpected location in \"\n                \"`vocabulary`. Note that passed `vocabulary` does not need to \"\n                \"include the OOV and mask tokens. Either remove all mask and \"\n                \"OOV tokens, or include them only at the start of the \"\n                f\"vocabulary in precisely this order: {special_tokens}. \"\n                f\"Received: oov_token={self.oov_token} at \"\n                f\"vocabulary index {oov_index}\"\n            )\n\n        new_vocab_size = token_start + len(tokens)\n        if self.max_tokens is not None and (new_vocab_size > self.max_tokens):\n            raise ValueError(\n                \"Attempted to set a vocabulary larger than the maximum vocab \"\n                f\"size. Received vocabulary size is {new_vocab_size}; \"\n                f\"`max_tokens` is {self.max_tokens}.\"\n            )\n        self.lookup_table = self._lookup_table_from_tokens(tokens)\n        self._record_vocabulary_size()\n\n        if self.output_mode == \"tf_idf\" and idf_weights is not None:\n            if len(vocabulary) != len(idf_weights):\n                raise ValueError(\n                    \"`idf_weights` must be the same length as vocabulary. \"\n                    f\"len(idf_weights) is {len(idf_weights)}; \"\n                    f\"len(vocabulary) is {len(vocabulary)}\"\n                )\n            idf_weights = self._convert_to_ndarray(idf_weights)\n            if idf_weights.ndim != 1:\n                raise ValueError(\n                    \"TF-IDF data must be a 1-index array. \"\n                    f\"Received: type(idf_weights)={type(idf_weights)}\"\n                )\n\n            # If the passed vocabulary has no special tokens, we need to pad the\n            # front of idf_weights. We don't have real document frequencies for\n            # these tokens so we will use an average of all idf_weights passed\n            # in as a reasonable default.\n            if found_special_tokens:\n                front_padding = 0\n                front_padding_value = 0\n            else:\n                front_padding = token_start\n                front_padding_value = np.average(idf_weights)\n            # If pad_to_max_tokens is true, and max_tokens is greater than our\n            # total vocab size, we need to pad the back of idf_weights with\n            # zeros as well.\n            back_padding_value = 0\n            if self.pad_to_max_tokens and self.max_tokens is not None:\n                back_padding = (\n                    self.max_tokens - front_padding - len(idf_weights)\n                )\n            else:\n                back_padding = 0\n            weights = np.pad(\n                idf_weights,\n                (front_padding, back_padding),\n                \"constant\",\n                constant_values=(front_padding_value, back_padding_value),\n            )\n            weights = tf.convert_to_tensor(weights, dtype=backend.floatx())\n            self.idf_weights = tf.Variable(\n                weights,\n                trainable=False,\n            )\n            self.idf_weights_const = self.idf_weights.value()\n", "comment": "Please add details to the error message, in particular inform users of the workaround (turning off safe mode)", "comment_author": "fchollet", "comment_url": "https://github.com/keras-team/keras/pull/21751#discussion_r2437951265", "type": "documentation"}
{"repo": "keras-team/keras", "pr_id": 21685, "file_path": "keras/src/backend/tensorflow/nn.py", "diff_hunk": "@@ -1077,3 +1077,41 @@ def dot_product_attention(\n     return _dot_product_attention_xla(\n         query, key, value, bias, mask, is_causal, scale\n     )\n+\n+\n+def unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n+    \"\"\"", "context": "def dot_product_attention(\n    query,\n    key,\n    value,\n    bias=None,\n    mask=None,\n    scale=None,\n    is_causal=False,\n    flash_attention=None,\n    attn_logits_soft_cap=None,", "comment": "Please fix docstring format", "comment_author": "fchollet", "comment_url": "https://github.com/keras-team/keras/pull/21685#discussion_r2388788260", "type": "documentation"}
{"repo": "keras-team/keras", "pr_id": 21379, "file_path": "keras/src/backend/openvino/numpy.py", "diff_hunk": "@@ -1046,7 +1056,158 @@ def maximum(x1, x2):\n \n \n def median(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`median` is not supported with openvino backend\")\n+    if np.isscalar(x):\n+        x = get_ov_output(x)\n+        return OpenVINOKerasTensor(x)\n+", "context": "def log1p(x):\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n\n    if x_type.is_integral():\n        x_type = OPENVINO_DTYPES[config.floatx()]\n        x = ov_opset.convert(x, x_type)\n\n    one_const = ov_opset.constant(1, x_type).output(0)\n    added = ov_opset.add(x, one_const).output(0)\n    result = ov_opset.log(added).output(0)\n    return OpenVINOKerasTensor(result)\n\n", "comment": "please add comments to explain the logic of the algorithm you implemented", "comment_author": "rkazants", "comment_url": "https://github.com/keras-team/keras/pull/21379#discussion_r2203487820", "type": "documentation"}
{"repo": "keras-team/keras", "pr_id": 21132, "file_path": "keras/src/backend/openvino/numpy.py", "diff_hunk": "@@ -715,7 +715,36 @@ def expm1(x):\n \n \n def flip(x, axis=None):\n-    raise NotImplementedError(\"`flip` is not supported with openvino backend\")\n+    if axis == () or axis == []:\n+        return x\n+    x = get_ov_output(x)\n+    rank = x.get_partial_shape().rank.get_length()\n+    if axis is None:\n+        axis = list(range(rank))\n+    else:\n+        if np.isscalar(axis):\n+            axis = [axis]\n+        else:\n+            axis = list(axis)\n+        axis = [ax if ax >= 0 else ax + rank for ax in axis]\n+    shape_of_x = ov_opset.shape_of(x)\n+    for ax in sorted(axis):\n+        dim_node = ov_opset.gather(\n+            shape_of_x,\n+            ov_opset.constant(ax, Type.i64).output(0),\n+            ov_opset.constant(0, Type.i64).output(0),\n+        ).output(0)\n+        start = ov_opset.subtract(\n+            dim_node, ov_opset.constant(1, Type.i64).output(0)\n+        ).output(0)\n+        stop = ov_opset.constant(-1, Type.i64).output(0)\n+        step = ov_opset.constant(-1, Type.i64).output(0)\n+\n+        reversed_indices = ov_opset.range(start, stop, step, Type.i64).output(0)\n+        x = ov_opset.gather(\n+            x, reversed_indices, ov_opset.constant(ax, Type.i64).output(0)\n+        ).output(0)", "context": "def expm1(x):\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n    if x_type.is_integral():\n        ov_type = OPENVINO_DTYPES[config.floatx()]\n        x = ov_opset.convert(x, ov_type)\n    exp_x = ov_opset.exp(x).output(0)\n    const_one = ov_opset.constant(1, exp_x.get_element_type())\n    result = ov_opset.subtract(exp_x, const_one).output(0)\n    return OpenVINOKerasTensor(result)\n\n", "comment": "it looks not optimal. We have special op for reversing a tensor along required axis. Check\r\nhttps://docs.openvino.ai/2025/api/ie_python_api/_autosummary/openvino.runtime.opset1.reverse_sequence.html#openvino.runtime.opset1.reverse_sequence", "comment_author": "rkazants", "comment_url": "https://github.com/keras-team/keras/pull/21132#discussion_r2033719783", "type": "performance"}
{"repo": "vllm-project/vllm", "pr_id": 27108, "file_path": "vllm/v1/engine/core_client.py", "diff_hunk": "@@ -385,7 +385,7 @@ def close_sockets_and_tasks():\n                         with contextlib.suppress(Exception):\n                             task.cancel()\n \n-            if in_loop(loop):\n+            if loop is not None and in_loop(loop):\n                 close_sockets_and_tasks()\n             elif loop and not loop.is_closed():\n                 loop.call_soon_threadsafe(close_sockets_and_tasks)", "context": "            def close_sockets_and_tasks():\n                close_sockets(sockets)\n                for task in tasks:\n                    if task is not None and not task.done():\n                        with contextlib.suppress(Exception):\n                            task.cancel()\n", "comment": "Maybe this instead of checking if the `loop` is not None twice?\r\n\r\n```suggestion\r\n            if loop is not None:\r\n                if in_loop(loop):\r\n                    close_sockets_and_tasks()\r\n                elif not loop.is_closed():\r\n                    loop.call_soon_threadsafe(close_sockets_and_tasks)\r\n```", "comment_author": "hmellor", "comment_url": "https://github.com/vllm-project/vllm/pull/27108#discussion_r2448639768", "type": "logic"}
{"repo": "vllm-project/vllm", "pr_id": 27230, "file_path": "vllm/model_executor/model_loader/runai_streamer_loader.py", "diff_hunk": "@@ -30,6 +30,12 @@ def __init__(self, load_config: LoadConfig):\n         if load_config.model_loader_extra_config:\n             extra_config = load_config.model_loader_extra_config\n \n+            self._is_distributed = False", "context": "    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            extra_config = load_config.model_loader_extra_config\n\n            if \"concurrency\" in extra_config and isinstance(\n                extra_config.get(\"concurrency\"), int\n            ):\n                os.environ[\"RUNAI_STREAMER_CONCURRENCY\"] = str(\n                    extra_config.get(\"concurrency\")\n                )\n\n            if \"memory_limit\" in extra_config and isinstance(\n                extra_config.get(\"memory_limit\"), int\n            ):\n                os.environ[\"RUNAI_STREAMER_MEMORY_LIMIT\"] = str(\n                    extra_config.get(\"memory_limit\")\n                )\n\n            runai_streamer_s3_endpoint = os.getenv(\"RUNAI_STREAMER_S3_ENDPOINT\")\n            aws_endpoint_url = os.getenv(\"AWS_ENDPOINT_URL\")\n            if runai_streamer_s3_endpoint is None and aws_endpoint_url is not None:\n                os.environ[\"RUNAI_STREAMER_S3_ENDPOINT\"] = aws_endpoint_url\n", "comment": "The default value of `self._is_distributed` can be `True`, according to the default behavior of the streamer.\r\n\r\nBy default, if `is_distribued` is `True`, distributed streaming is enabled only for object storage and cuda devices.\r\nThe default behavior can be changed using `RUNAI_STREAMER_DIST=0` to disable distributing or  `RUNAI_STREAMER_DIST=1` for forcing distributed streaming in other cases.", "comment_author": "noa-neria", "comment_url": "https://github.com/vllm-project/vllm/pull/27230#discussion_r2447592394", "type": "logic"}
{"repo": "vllm-project/vllm", "pr_id": 27468, "file_path": "vllm/lora/ops/triton_ops/fused_moe_lora_op.py", "diff_hunk": "@@ -210,9 +213,9 @@ def _fused_moe_lora(\n     num_tokens = M * top_k_num\n     w1_output_dim_size = w1_lora_b_stacked.shape[2]\n \n-    lora_intermediate_cache1 = torch.zeros(\n+    lora_intermediate_cache1 = torch.empty(", "context": "def _fused_moe_lora(\n    output: torch.Tensor,  # (num_tokens, top_k_num, N*len(lora_a_stacked),)\n    qcurr_hidden_states: torch.Tensor,  # (num_tokens, K,)\n    lora_a_stacked: list[\n        torch.Tensor\n    ],  # [(max_loras, num_experts, max_lora_rank, K,),...]\n    lora_b_stacked: list[\n        torch.Tensor\n    ],  # [(max_loras, num_experts, N, max_lora_rank,),...]\n    topk_weights: torch.Tensor,  # (num_tokens, top_k_num)\n    sorted_token_ids: torch.Tensor,  # (max_loras, _)\n    expert_ids: torch.Tensor,  # (max_loras, _ ,)\n    num_tokens_post_padded: torch.Tensor,  # (max_loras, )\n    max_lora_rank: int,\n    top_k_num: int,\n    block_size_m: int,\n    block_size_n: int,\n    block_size_k: int,\n    group_size_m: int,\n    mul_routed_weight: bool = False,", "comment": "Why not using zeros anymore?", "comment_author": "mgoin", "comment_url": "https://github.com/vllm-project/vllm/pull/27468#discussion_r2462337110", "type": "performance"}
{"repo": "vllm-project/vllm", "pr_id": 24274, "file_path": "vllm/compilation/backends.py", "diff_hunk": "@@ -389,6 +392,86 @@ def set_model_tag(tag: str):\n         model_tag = old_tag\n \n \n+try:\n+    from torch._dynamo.aot_compile import SerializableCallable\n+except ImportError:\n+    SerializableCallable = object\n+\n+assert isinstance(SerializableCallable, type)\n+\n+\n+class VllmCompiledFunction(SerializableCallable):\n+\n+    def __init__(self, graph_module, example_inputs, vllm_config, prefix,\n+                 optimized_call):\n+        assert isinstance(graph_module, torch.fx.GraphModule)\n+        self.graph_module = graph_module\n+        self.example_inputs = example_inputs\n+        self.vllm_config = vllm_config\n+        self.prefix = prefix\n+        self.optimized_call = optimized_call\n+\n+    def __call__(self, *args, **kwargs):\n+        return self.optimized_call(*args, **kwargs)\n+\n+    @classmethod\n+    def serialize_compile_artifacts(\n+            cls, compiled_fn: \"VllmCompiledFunction\") -> bytes:\n+        import sympy\n+        from torch._subclasses import FakeTensorMode\n+        from torch.fx._graph_pickler import GraphPickler, Options\n+        state = compiled_fn.__dict__.copy()\n+        state.pop(\"optimized_call\")\n+        for node in state[\"graph_module\"].graph.nodes:\n+            node.meta.pop(\"source_fn_stack\", None)\n+            node.meta.pop(\"nn_module_stack\", None)\n+\n+        graph_reducer_override = GraphPickler.reducer_override\n+\n+        def _graph_reducer_override(self, obj):\n+            if (inspect.isclass(obj) and issubclass(obj, sympy.Function)\n+                    and hasattr(obj, \"_torch_unpickler\")):\n+                return obj._torch_unpickler, (obj._torch_handler_name, )\n+            if isinstance(obj, FakeTensorMode):\n+                return type(None), ()\n+            return graph_reducer_override(self, obj)\n+\n+        with patch.object(GraphPickler, 'reducer_override',\n+                          _graph_reducer_override):\n+            state[\"graph_module\"] = GraphPickler.dumps(\n+                state[\"graph_module\"], Options(ops_filter=None))\n+            state[\"example_inputs\"] = GraphPickler.dumps(\n+                state[\"example_inputs\"])\n+        return pickle.dumps(state)\n+\n+    @classmethod\n+    def deserialize_compile_artifacts(cls,\n+                                      data: bytes) -> \"VllmCompiledFunction\":\n+        from torch._guards import TracingContext, tracing\n+        from torch._subclasses import FakeTensorMode\n+        from torch.fx._graph_pickler import GraphPickler\n+        from torch.fx.experimental.symbolic_shapes import ShapeEnv\n+\n+        state = pickle.loads(data)\n+        fake_mode = FakeTensorMode(shape_env=ShapeEnv())\n+        state[\"graph_module\"] = GraphPickler.loads(state[\"graph_module\"],\n+                                                   fake_mode)\n+        state[\"example_inputs\"] = GraphPickler.loads(state[\"example_inputs\"],\n+                                                     fake_mode)\n+        vllm_backend = VllmBackend(state[\"vllm_config\"], state[\"prefix\"])\n+        with tracing(TracingContext(fake_mode)):\n+            optimized_call = vllm_backend(state[\"graph_module\"],", "context": "    def call_module(\n        self,\n        target: torch.fx.node.Target,\n        args: tuple[torch.fx.node.Argument, ...],\n        kwargs: dict[str, Any],", "comment": "Shouldn't `optimized_call` here be non-wrapped function rather than VllmCompiledFunction? Won't it cause an issue in case of a new serialization/deserialization?", "comment_author": "ilmarkov", "comment_url": "https://github.com/vllm-project/vllm/pull/24274#discussion_r2325131462", "type": "logic"}
{"repo": "localstack/localstack", "pr_id": 13201, "file_path": "tests/aws/services/apigateway/test_apigateway_extended.py", "diff_hunk": "@@ -272,7 +268,7 @@ def test_get_usage_plan_api_keys(self, aws_client, apigw_create_api_key, snapsho\n         snapshot.match(\"create-api-key\", create_api_key)\n \n         create_api_key_2 = apigw_create_api_key(name=api_key_name_2)\n-        snapshot.match(\"create-api-key-2\", create_api_key)\n+        snapshot.match(\"create-api-key-2\", create_api_key_2)", "context": "    def test_get_usage_plan_api_keys(self, aws_client, apigw_create_api_key, snapshot, cleanups):\n        snapshot.add_transformers_list(\n            [\n                snapshot.transform.key_value(\"id\"),\n                snapshot.transform.key_value(\"value\"),\n                snapshot.transform.key_value(\"name\"),\n            ]\n        )\n        api_key_name = f\"test-key-{short_uid()}\"\n        api_key_name_2 = f\"test-key-{short_uid()}\"\n\n        get_api_keys = aws_client.apigateway.get_api_keys()\n        snapshot.match(\"get-api-keys\", get_api_keys)\n\n        create_api_key = apigw_create_api_key(name=api_key_name)\n        snapshot.match(\"create-api-key\", create_api_key)\n\n        create_api_key_2 = apigw_create_api_key(name=api_key_name_2)\n        snapshot.match(\"create-api-key-2\", create_api_key)\n\n        get_api_keys_after_create = aws_client.apigateway.get_api_keys()\n        snapshot.match(\"get-api-keys-after-create-1\", get_api_keys_after_create)\n\n        create_usage_plan = aws_client.apigateway.create_usage_plan(\n            name=f\"usage-plan-{short_uid()}\"\n        )\n        usage_plan_id = create_usage_plan[\"id\"]\n        cleanups.append(lambda: aws_client.apigateway.delete_usage_plan(usagePlanId=usage_plan_id))\n        snapshot.match(\"create-usage-plan\", create_usage_plan)\n\n        get_up_keys_before_create = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id\n        )\n        snapshot.match(\"get-up-keys-before-create\", get_up_keys_before_create)\n\n        create_up_key = aws_client.apigateway.create_usage_plan_key(\n            usagePlanId=usage_plan_id, keyId=create_api_key[\"id\"], keyType=\"API_KEY\"\n        )\n        snapshot.match(\"create-up-key\", create_up_key)\n\n        create_up_key_2 = aws_client.apigateway.create_usage_plan_key(\n            usagePlanId=usage_plan_id, keyId=create_api_key_2[\"id\"], keyType=\"API_KEY\"\n        )\n        snapshot.match(\"create-up-key-2\", create_up_key_2)\n\n        get_up_keys = aws_client.apigateway.get_usage_plan_keys(usagePlanId=usage_plan_id)\n        snapshot.match(\"get-up-keys\", get_up_keys)\n\n        get_up_keys_query = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id, nameQuery=\"test-key\"\n        )\n        snapshot.match(\"get-up-keys-name-query\", get_up_keys_query)\n\n        get_up_keys_query_cased = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id, nameQuery=\"TEST-key\"\n        )\n        snapshot.match(\"get-up-keys-name-query-cased\", get_up_keys_query_cased)\n\n        get_up_keys_query_name = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id, nameQuery=api_key_name\n        )\n        snapshot.match(\"get-up-keys-name-query-key-name\", get_up_keys_query_name)\n\n        get_up_keys_bad_query = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id, nameQuery=\"nothing\"\n        )\n        snapshot.match(\"get-up-keys-bad-query\", get_up_keys_bad_query)\n\n        aws_client.apigateway.delete_api_key(apiKey=create_api_key[\"id\"])\n        aws_client.apigateway.delete_api_key(apiKey=create_api_key_2[\"id\"])\n\n        get_up_keys_after_delete = aws_client.apigateway.get_usage_plan_keys(\n            usagePlanId=usage_plan_id\n        )\n        snapshot.match(\"get-up-keys-after-delete\", get_up_keys_after_delete)\n\n        get_up_keys_bad_d = aws_client.apigateway.get_usage_plan_keys(usagePlanId=\"bad-id\")\n        snapshot.match(\"get-up-keys-bad-usage-plan\", get_up_keys_bad_d)", "comment": "great catch ", "comment_author": "bentsku", "comment_url": "https://github.com/localstack/localstack/pull/13201#discussion_r2387346874", "type": "other"}
{"repo": "localstack/localstack", "pr_id": 13119, "file_path": "localstack-core/localstack/services/lambda_/invocation/lambda_models.py", "diff_hunk": "@@ -168,15 +169,17 @@ def _download_archive_to_file(self, target_file: IO) -> None:\n         )\n         target_file.flush()\n \n-    def generate_presigned_url(self, endpoint_url: str | None = None) -> str:\n+    def generate_presigned_url(self, endpoint_url: str) -> str:\n         \"\"\"\n         Generates a presigned url pointing to the code archive\n         \"\"\"\n-        s3_client = connect_to(\n+        s3_client = boto3.client(\n+            \"s3\",\n             region_name=AWS_REGION_US_EAST_1,\n             aws_access_key_id=config.INTERNAL_RESOURCE_ACCOUNT,\n+            aws_secret_access_key=INTERNAL_AWS_SECRET_ACCESS_KEY,\n             endpoint_url=endpoint_url,\n-        ).s3\n+        )", "context": "    def generate_presigned_url(self, endpoint_url: str | None = None) -> str:\n        \"\"\"\n        Generates a presigned url pointing to the code archive\n        \"\"\"\n        s3_client = connect_to(\n            region_name=AWS_REGION_US_EAST_1,\n            aws_access_key_id=config.INTERNAL_RESOURCE_ACCOUNT,\n            endpoint_url=endpoint_url,\n        ).s3\n        params = {\"Bucket\": self.s3_bucket, \"Key\": self.s3_key}\n        if self.s3_object_version:\n            params[\"VersionId\"] = self.s3_object_version\n        return s3_client.generate_presigned_url(\"get_object\", Params=params)\n", "comment": "Good thinking adding the `INTERNAL_AWS_SECRET_ACCESS_KEY`  Pre-signed URLs, due to their nature, are pretty tricky and they will only be valid if the secret access key is `test` or the credentials used are \"existing\" in LocalStack and the secret key id can be retrieved from the Access Key  the fallback to using the default secret key for any access key id was added in #10265", "comment_author": "bentsku", "comment_url": "https://github.com/localstack/localstack/pull/13119#discussion_r2333442946", "type": "security"}
{"repo": "ultralytics/ultralytics", "pr_id": 22544, "file_path": "ultralytics/data/augment.py", "diff_hunk": "@@ -1155,7 +1155,11 @@ def affine_transform(self, img: np.ndarray, border: tuple[int, int]) -> tuple[np\n \n         # Rotation and Scale\n         R = np.eye(3, dtype=np.float32)\n-        a = random.uniform(-self.degrees, self.degrees)\n+        a = 0.0\n+        if isinstance(self.degrees, list) and self.degrees and all(isinstance(x, (int, float)) for x in self.degrees):\n+            a = random.choice(self.degrees)\n+        elif isinstance(self.degrees, (int, float)):\n+            if isinstance(self.degrees, (list, tuple)) and self.degrees and all(isinstance(x, (int, float)) for x in self.degrees):", "context": "    def affine_transform(self, img: np.ndarray, border: tuple[int, int]) -> tuple[np.ndarray, np.ndarray, float]:\n        \"\"\"\n        Apply a sequence of affine transformations centered around the image center.\n\n        This function performs a series of geometric transformations on the input image, including\n        translation, perspective change, rotation, scaling, and shearing. The transformations are\n        applied in a specific order to maintain consistency.\n\n        Args:\n            img (np.ndarray): Input image to be transformed.\n            border (tuple[int, int]): Border dimensions for the transformed image.\n\n        Returns:\n            img (np.ndarray): Transformed image.\n            M (np.ndarray): 3x3 transformation matrix.\n            s (float): Scale factor applied during the transformation.\n\n        Examples:\n            >>> import numpy as np\n            >>> img = np.random.rand(100, 100, 3)\n            >>> border = (10, 10)\n            >>> transformed_img, matrix, scale = affine_transform(img, border)\n        \"\"\"\n        # Center\n        C = np.eye(3, dtype=np.float32)\n\n        C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n        C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n\n        # Perspective\n        P = np.eye(3, dtype=np.float32)\n        P[2, 0] = random.uniform(-self.perspective, self.perspective)  # x perspective (about y)\n        P[2, 1] = random.uniform(-self.perspective, self.perspective)  # y perspective (about x)\n\n        # Rotation and Scale\n        R = np.eye(3, dtype=np.float32)\n        a = random.uniform(-self.degrees, self.degrees)\n        # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n        s = random.uniform(1 - self.scale, 1 + self.scale)\n        # s = 2 ** random.uniform(-scale, scale)\n        R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n        # Shear\n        S = np.eye(3, dtype=np.float32)\n        S[0, 1] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # x shear (deg)\n        S[1, 0] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # y shear (deg)\n\n        # Translation\n        T = np.eye(3, dtype=np.float32)\n        T[0, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[0]  # x translation (pixels)\n        T[1, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[1]  # y translation (pixels)\n\n        # Combined rotation matrix\n        M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n        # Affine image\n        if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n            if self.perspective:\n                img = cv2.warpPerspective(img, M, dsize=self.size, borderValue=(114, 114, 114))\n            else:  # affine\n                img = cv2.warpAffine(img, M[:2], dsize=self.size, borderValue=(114, 114, 114))\n            if img.ndim == 2:\n                img = img[..., None]\n        return img, M, s\n", "comment": " **CRITICAL**: Setting `a = 0.0` and then only overriding it when `self.degrees` is a list means numeric configs never enter the old `random.uniform(-self.degrees, self.degrees)` path. Because the nested `isinstance(..., (list, tuple))` check inside the float branch is impossible, `a` stays 0 for all scalar degrees, effectively disabling random rotation for the default case. Please restore the scalar logic (and add list handling alongside it) so we keep the existing behavior while supporting fixed-angle lists.", "comment_author": "UltralyticsAssistant", "comment_url": "https://github.com/ultralytics/ultralytics/pull/22544#discussion_r2478657419", "type": "logic"}
{"repo": "odoo/odoo", "pr_id": 232380, "file_path": "addons/account/models/account_move_line.py", "diff_hunk": "@@ -1657,12 +1658,14 @@ def write(self, vals):\n \n             # Check the tax lock date.\n             if line.parent_state == 'posted' and any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in protected_fields['tax']):\n-                line._check_tax_lock_date()\n+                lines_to_check_tax_lock += line", "context": "    def write(self, vals):\n        if not vals:\n            return True\n        protected_fields = self._get_lock_date_protected_fields()\n        account_to_write = self.env['account.account'].browse(vals['account_id']) if 'account_id' in vals else None\n\n        # Check writing a deprecated account.\n        if account_to_write and account_to_write.deprecated:\n            raise UserError(_('You cannot use a deprecated account.'))\n\n        inalterable_fields = set(self._get_integrity_hash_fields()).union({'inalterable_hash', 'secure_sequence_number'})\n        hashed_moves = self.move_id.filtered('inalterable_hash')\n        violated_fields = set(vals) & inalterable_fields\n        if hashed_moves and violated_fields:\n            raise UserError(_(\n                \"You cannot edit the following fields: %s.\\n\"\n                \"The following entries are already hashed:\\n%s\",\n                ', '.join(f['string'] for f in self.fields_get(violated_fields).values()),\n                '\\n'.join(hashed_moves.mapped('name')),\n            ))\n\n        line_to_write = self\n        vals = self._sanitize_vals(vals)\n        for line in self:\n            if not any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in vals):\n                line_to_write -= line\n                continue\n\n            if line.parent_state == 'posted' and any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in ('tax_ids', 'tax_line_id')):\n                raise UserError(_('You cannot modify the taxes related to a posted journal item, you should reset the journal entry to draft to do so.'))\n\n            # Check the lock date.\n            if line.parent_state == 'posted' and any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in protected_fields['fiscal']):\n                line.move_id._check_fiscalyear_lock_date()\n\n            # Check the tax lock date.\n            if line.parent_state == 'posted' and any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in protected_fields['tax']):\n                line._check_tax_lock_date()\n\n            # Check the reconciliation.\n            if any(self.env['account.move']._field_will_change(line, vals, field_name) for field_name in protected_fields['reconciliation']):\n                line._check_reconciliation()\n\n        move_container = {'records': self.move_id}\n        with self.move_id._check_balanced(move_container),\\\n             self.env.protecting(self.env['account.move']._get_protected_vals(vals, self)),\\\n             self.move_id._sync_dynamic_lines(move_container),\\\n             self._sync_invoice({'records': self}):\n            self = line_to_write\n            if not self:\n                return True\n            # Tracking stuff can be skipped for perfs using tracking_disable context key\n            if not self.env.context.get('tracking_disable', False):\n                # Get all tracked fields (without related fields because these fields must be manage on their own model)\n                tracking_fields = []\n                for value in vals:\n                    field = self._fields[value]\n                    if hasattr(field, 'related') and field.related:\n                        continue # We don't want to track related field.\n                    if hasattr(field, 'tracking') and field.tracking:\n                        tracking_fields.append(value)\n                ref_fields = self.env['account.move.line'].fields_get(tracking_fields)\n\n                # Get initial values for each line\n                move_initial_values = {}\n                for line in self.filtered(lambda l: l.move_id.posted_before): # Only lines with posted once move.\n                    for field in tracking_fields:\n                        # Group initial values by move_id\n                        if line.move_id.id not in move_initial_values:\n                            move_initial_values[line.move_id.id] = {}\n                        move_initial_values[line.move_id.id].update({field: line[field]})\n\n            result = super().write(vals)\n            self.move_id._synchronize_business_models(['line_ids'])\n            if any(field in vals for field in ['account_id', 'currency_id']):\n                self._check_constrains_account_id_journal_id()\n\n            if not self.env.context.get('tracking_disable', False):\n                # Log changes to move lines on each move\n                for move_id, modified_lines in move_initial_values.items():\n                    for line in self.filtered(lambda l: l.move_id.id == move_id):\n                        tracking_value_ids = line._mail_track(ref_fields, modified_lines)[1]\n                        if tracking_value_ids:\n                            msg = _(\"Journal Item %s updated\", line._get_html_link(title=f\"#{line.id}\"))\n                            line.move_id._message_log(\n                                body=msg,\n                                tracking_value_ids=tracking_value_ids\n                            )\n\n        return result\n", "comment": "Don't `+=` recordsets in loops, use a list of (integer) ids instead", "comment_author": "william-andre", "comment_url": "https://github.com/odoo/odoo/pull/232380#discussion_r2448787622", "type": "performance"}
{"repo": "odoo/odoo", "pr_id": 233558, "file_path": "addons/mail/tests/test_mail_poll.py", "diff_hunk": "@@ -1,7 +1,10 @@\n # Part of Odoo. See LICENSE file for full copyright and licensing details.\n \n+from datetime import datetime, timedelta\n from pprint import pformat\n \n+from freezegun import freeze_time", "context": "# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\nfrom pprint import pformat\n\nfrom odoo.tests.common import HttpCase, JsonRpcException, new_test_user, tagged\n\n\n@tagged(\"post_install\", \"-at_install\")\nclass TestMailPoll(HttpCase):", "comment": "alphabetical order of import, no need for the blank line above. Ideally, we want 1 blank line between external and odoo imports.", "comment_author": "tsm-odoo", "comment_url": "https://github.com/odoo/odoo/pull/233558#discussion_r2472916202", "type": "style"}
{"repo": "odoo/odoo", "pr_id": 229010, "file_path": "addons/hr/tests/test_hr_employee.py", "diff_hunk": "@@ -608,6 +609,46 @@ def test_user_creation_from_employee_multi_emails(self):\n         self.assertEqual(params.get('message'), f\"Users {employees[1].name} creation successful\")\n         self.assertTrue(employees[1].user_id)\n \n+    def test_version_cron_update_no_fields(self):\n+        \"\"\"\n+        Employees should not see their fields be updated if the CRON does not\n+        change their version.\n+        \"\"\"\n+        # Will be used for default employee version address (contains phone)\n+        self.env.user.company_id = self.env['res.company'].create({\n+            'name': 'Pokmon Center',\n+            'phone': '+329999999999'\n+        })\n+\n+        employee_1, employee_2 = self.env['hr.employee'].create([\n+            {\n+                'name': 'Charizard',\n+                'work_phone': '+32404040404',\n+                \"distance_home_work\": 32,\n+                \"distance_home_work_unit\": 'miles',\n+            },\n+            {\n+                'name': 'Charmander',\n+            },\n+        ])\n+        employee_1_values, employee_2_values = {}, {}\n+\n+        employee_fields = self.env['hr.employee']._fields.keys()\n+        for field in employee_fields:\n+            employee_1_values[field] = employee_1[field]\n+            employee_2_values[field] = employee_2[field]\n+        employee_1_values = deepcopy(employee_1_values)\n+        employee_2_values = deepcopy(employee_2_values)", "context": "    def test_user_creation_from_employee_multi_emails(self):\n        employees = self.env['hr.employee'].create([\n            {\n                'name': 'Existing Email Employee',\n                'work_email': self.user_without_image.email,\n            }, {\n                'name': 'New Employee',\n                'work_email': 'newuser@example.com',\n            }, {\n                'name': 'Invalid Email Employee',\n                'work_email': 'invalid-email',\n            }, {\n                'name': 'Without Email Employee',\n                'work_email': False,\n            }, {\n                'name': 'Formatted Email Employee',\n                'work_email': f'\"John Doe\" <{self.user_without_image.email_normalized}>',\n            }, {\n                'name': 'Multi Email Employee',\n                'work_email': '\"Name1\" <name@test.example.com>, \"Name 2\" <name2@test.example.com>',\n            },\n        ])\n        # Add an existing employee who already has a user to the employee list\n        employees += self.employee_without_image\n        context = {'selected_ids': employees.ids}\n        confirmed_employees = self.env['hr.employee'].with_context(context).browse(employees.ids)\n        action = confirmed_employees.action_create_users()\n\n        params = action.get('params')\n        self.assertEqual(params.get('message'), f\"User already exists with the same email for Employees {employees[0].name}, {employees[4].name}\")\n        params = params.get('next').get('params')\n        self.assertEqual(params.get('message'), f\"You need to set a valid work email address for {employees[2].name}, {employees[5].name}\")\n        params = params.get('next').get('params')\n        self.assertEqual(params.get('message'), f\"You need to set the work email address for {employees[3].name}\")\n        params = params.get('next').get('params')\n        self.assertEqual(params.get('message'), f\"User already exists for Those Employees {employees[6].name}\")\n        params = params.get('next').get('params')\n        self.assertEqual(params.get('message'), f\"Users {employees[1].name} creation successful\")\n        self.assertTrue(employees[1].user_id)\n\n\n@tagged('-at_install', 'post_install')", "comment": "What's the purpose here of assigning the variable to be a deepcopy of itself ?\nAlso for this whole test, there is a default orm method called `copy_data` that kinda does what you do with a small caveat. Your approach also captures all the mixin fields, so f.ex hr.employee has the `mail.message` mixin so your approach will show all the mail_message_*?* fields which might or might not be relevant to the update_current_version thingy. \nAlso it probably already exists, but if not you should probably add a test that makes sure things change when you want them to change.", "comment_author": "Einar-eian", "comment_url": "https://github.com/odoo/odoo/pull/229010#discussion_r2402256483", "type": "performance"}
{"repo": "run-llama/llama_index", "pr_id": 20173, "file_path": "llama-index-core/llama_index/core/agent/workflow/react_agent.py", "diff_hunk": "@@ -95,17 +95,21 @@ async def _get_streaming_response(\n                 if isinstance(last_chat_response.raw, BaseModel)\n                 else last_chat_response.raw\n             )\n-            ctx.write_event_to_stream(\n-                AgentStream(\n-                    delta=last_chat_response.delta or \"\",\n-                    response=last_chat_response.message.content or \"\",\n-                    raw=raw,\n-                    current_agent_name=self.name,\n-                    thinking_delta=last_chat_response.additional_kwargs.get(\n-                        \"thinking_delta\", None\n-                    ),\n+            # some code paths (namely react agent via llm.predict_and_call for non function calling llms) pass through a context without starting the workflow.\n+            # They do so in order to conform to the interface, and share state between tools, however the events are discarded and not exposed to the caller,\n+            # so just don't write events if the context is not running.\n+            if ctx.is_running:", "context": "    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"react_header\" in prompts:\n            react_header = prompts[\"react_header\"]\n            if isinstance(react_header, str):\n                react_header = PromptTemplate(react_header)\n            self.formatter.system_header = react_header.format()\n", "comment": "ah nice little catch", "comment_author": "logan-markewich", "comment_url": "https://github.com/run-llama/llama_index/pull/20173#discussion_r2475209431", "type": "other"}
{"repo": "run-llama/llama_index", "pr_id": 20073, "file_path": "llama-index-integrations/embeddings/llama-index-embeddings-voyageai/llama_index/embeddings/voyageai/base.py", "diff_hunk": "@@ -17,14 +17,32 @@\n \n logger = logging.getLogger(__name__)\n \n-DEFAULT_VOYAGE_2_BATCH_SIZE = 72\n-DEFAULT_VOYAGE_3_LITE_BATCH_SIZE = 30\n-DEFAULT_VOYAGE_3_BATCH_SIZE = 10\n-DEFAULT_BATCH_SIZE = 7\n+DEFAULT_BATCH_SIZE = 2048\n+MAX_BATCH_SIZE = 1000", "context": "from llama_index.core.base.embeddings.base import Embedding\nfrom llama_index.core.bridge.pydantic import PrivateAttr\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.embeddings import MultiModalEmbedding\nfrom llama_index.core.schema import ImageType\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_VOYAGE_2_BATCH_SIZE = 72\nDEFAULT_VOYAGE_3_LITE_BATCH_SIZE = 30\nDEFAULT_VOYAGE_3_BATCH_SIZE = 10\nDEFAULT_BATCH_SIZE = 7\nMULTIMODAL_MODELS = [\"voyage-multimodal-3\"]\n\nSUPPORTED_IMAGE_FORMATS = {\"png\", \"jpeg\", \"jpg\", \"webp\", \"gif\"}\n\n", "comment": "Why is the default size bigger than the maximum one?", "comment_author": "AstraBert", "comment_url": "https://github.com/run-llama/llama_index/pull/20073#discussion_r2428802818", "type": "logic"}
{"repo": "run-llama/llama_index", "pr_id": 16972, "file_path": "llama-index-integrations/vector_stores/llama-index-vector-stores-chroma/llama_index/vector_stores/chroma/base.py", "diff_hunk": "@@ -235,36 +240,46 @@ def from_params(\n     def class_name(cls) -> str:\n         return \"ChromaVectorStore\"\n \n+    def move_nodes(self, from_index_id: str, to_index_id: str):\n+        all_nodes = self._collection.get(where={\"__index_id__\": from_index_id})", "context": "    def class_name(cls) -> str:\n        return \"ChromaVectorStore\"\n", "comment": "this would bring every node into memory. Not ideal tbh for large datasets", "comment_author": "logan-markewich", "comment_url": "https://github.com/run-llama/llama_index/pull/16972#discussion_r1880968123", "type": "performance"}
{"repo": "run-llama/llama_index", "pr_id": 20052, "file_path": "llama-index-integrations/readers/llama-index-readers-gitlab/llama_index/readers/gitlab/issues/base.py", "diff_hunk": "@@ -167,6 +169,7 @@ def load_data(\n             - state: State of the issues to retrieve.\n             - updated_after: Filter issues updated after the specified date.\n             - updated_before: Filter issues updated before the specified date.\n+            - get_all: Get all the items whisout pagination (for a long lists).", "context": "    def load_data(\n        self,\n        assignee: Optional[Union[str, int]] = None,\n        author: Optional[Union[str, int]] = None,\n        confidential: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        iids: Optional[List[int]] = None,\n        issue_type: Optional[IssueType] = None,\n        labels: Optional[List[str]] = None,\n        milestone: Optional[str] = None,\n        non_archived: Optional[bool] = None,\n        scope: Optional[Scope] = None,\n        search: Optional[str] = None,\n        state: Optional[IssueState] = IssueState.OPEN,\n        updated_after: Optional[datetime] = None,\n        updated_before: Optional[datetime] = None,\n        **kwargs: Any,", "comment": "Small typo here: without", "comment_author": "anoopshrma", "comment_url": "https://github.com/run-llama/llama_index/pull/20052#discussion_r2414350595", "type": "documentation"}
{"repo": "run-llama/llama_index", "pr_id": 18676, "file_path": "llama-index-core/llama_index/core/indices/vector_store/base.py", "diff_hunk": "@@ -333,6 +338,29 @@ def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n         with self._callback_manager.as_trace(\"insert_nodes\"):\n             self._insert(nodes, **insert_kwargs)\n             self._storage_context.index_store.add_index_struct(self._index_struct)\n+        self._insert(nodes, **insert_kwargs)\n+        self._storage_context.index_store.add_index_struct(self._index_struct)", "context": "    def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"\n        Insert nodes.\n\n        NOTE: overrides BaseIndex.insert_nodes.\n            VectorStoreIndex only stores nodes in document store\n            if vector store does not store text\n        \"\"\"\n        for node in nodes:\n            if isinstance(node, IndexNode):\n                try:\n                    node.dict()\n                except ValueError:\n                    self._object_map[node.index_id] = node.obj\n                    node.obj = None\n\n        with self._callback_manager.as_trace(\"insert_nodes\"):\n            self._insert(nodes, **insert_kwargs)\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n", "comment": "uhhh you've duplicated `self._insert` and `add_index_struct` here", "comment_author": "logan-markewich", "comment_url": "https://github.com/run-llama/llama_index/pull/18676#discussion_r2093421410", "type": "logic"}
{"repo": "run-llama/llama_index", "pr_id": 19856, "file_path": "llama-index-core/llama_index/core/utils.py", "diff_hunk": "@@ -657,6 +657,14 @@ def resolve_binary(\n         return BytesIO(data)\n \n     elif url is not None:\n+        if url.startswith(\"data:\"):\n+            if as_base64 and \"base64,\" in url:", "context": "def resolve_binary(\n    raw_bytes: Optional[bytes] = None,\n    path: Optional[Union[str, Path]] = None,\n    url: Optional[str] = None,\n    as_base64: bool = False,", "comment": "unfortunately, I think `base64` is valid encoded base 64, so this could maybe go wrong (probably very-very rare?). Could this check be a little more explicit about the location? Probably just some more comma/semi-colon splitting\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/URI/Reference/Schemes/data", "comment_author": "adrianlyjak", "comment_url": "https://github.com/run-llama/llama_index/pull/19856#discussion_r2345143898", "type": "security"}
{"repo": "freqtrade/freqtrade", "pr_id": 1229, "file_path": "freqtrade/tests/test_freqtradebot.py", "diff_hunk": "@@ -335,15 +336,16 @@ def test_edge_should_ignore_strategy_stoploss(limit_buy_order, fee, markets,\n     #############################################\n \n     # Create a trade with \"limit_buy_order\" price\n-    freqtrade = FreqtradeBot(default_conf)\n+    freqtrade = FreqtradeBot(edge_conf)\n+    freqtrade.active_pair_whitelist = ['NEO/BTC']\n     patch_get_signal(freqtrade)\n     freqtrade.strategy.min_roi_reached = lambda trade, current_profit, current_time: False\n     freqtrade.create_trade()\n     trade = Trade.query.first()\n     trade.update(limit_buy_order)\n     #############################################\n \n-    # stoploss shoud be hit\n+    # stoploss shoud not be hit", "context": "def test_get_min_pair_stake_amount(mocker, default_conf) -> None:\n    patch_RPCManager(mocker)\n    patch_exchange(mocker)\n    freqtrade = FreqtradeBot(default_conf)\n    freqtrade.strategy.stoploss = -0.05\n    # no pair found\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC'\n        }])\n    )\n    with pytest.raises(ValueError, match=r'.*get market information.*'):\n        freqtrade._get_min_pair_stake_amount('BNB/BTC', 1)\n\n    # no 'limits' section\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC'\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result is None\n\n    # empty 'limits' section\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {}\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result is None\n\n    # no cost Min\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {\"min\": None},\n                'amount': {}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result is None\n\n    # no amount Min\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {},\n                'amount': {\"min\": None}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result is None\n\n    # empty 'cost'/'amount' section\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {},\n                'amount': {}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result is None\n\n    # min cost is set\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {'min': 2},\n                'amount': {}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 1)\n    assert result == 2 / 0.9\n\n    # min amount is set\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {},\n                'amount': {'min': 2}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 2)\n    assert result == 2 * 2 / 0.9\n\n    # min amount and cost are set (cost is minimal)\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {'min': 2},\n                'amount': {'min': 2}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 2)\n    assert result == min(2, 2 * 2) / 0.9\n\n    # min amount and cost are set (amount is minial)\n    mocker.patch(\n        'freqtrade.exchange.Exchange.get_markets',\n        MagicMock(return_value=[{\n            'symbol': 'ETH/BTC',\n            'limits': {\n                'cost': {'min': 8},\n                'amount': {'min': 2}\n            }\n        }])\n    )\n    result = freqtrade._get_min_pair_stake_amount('ETH/BTC', 2)\n    assert result == min(8, 2 * 2) / 0.9\n\n", "comment": "good catch :+1:", "comment_author": "xmatthias", "comment_url": "https://github.com/freqtrade/freqtrade/pull/1229#discussion_r232463420", "type": "other"}
{"repo": "apache/airflow", "pr_id": 57448, "file_path": "providers/openlineage/src/airflow/providers/openlineage/utils/sql.py", "diff_hunk": "@@ -172,7 +173,9 @@ def create_information_schema_query(\n             information_schema_table,\n             uppercase_names=uppercase_names,\n         )\n-        select_statements.append(information_schema_table.select().filter(filter_clauses))\n+        select_statements.append(\n+            information_schema_table.select().filter(cast(\"ColumnElement[bool]\", filter_clauses))", "context": "def create_information_schema_query(\n    columns: list[str],\n    information_schema_table_name: str,\n    tables_hierarchy: TablesHierarchy,\n    uppercase_names: bool = False,\n    use_flat_cross_db_query: bool = False,\n    sqlalchemy_engine: Engine | None = None,", "comment": "I feel we should just do this inside `create_filter_clauses` so it returns a `ColumnElement[bool]`. It is always used for this purpose.", "comment_author": "uranusjr", "comment_url": "https://github.com/apache/airflow/pull/57448#discussion_r2476354546", "type": "logic"}
{"repo": "apache/airflow", "pr_id": 56813, "file_path": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/assets.py", "diff_hunk": "@@ -244,7 +244,7 @@ def get_asset_aliases(\n     )\n \n     return AssetAliasCollectionResponse(\n-        asset_aliases=session.scalars(asset_aliases_select),\n+        asset_aliases=list(session.scalars(asset_aliases_select)),", "context": "def get_asset_aliases(\n    limit: QueryLimit,\n    offset: QueryOffset,\n    name_pattern: QueryAssetAliasNamePatternSearch,\n    order_by: Annotated[\n        SortParam,\n        Depends(SortParam([\"id\", \"name\"], AssetAliasModel).dynamic_depends()),\n    ],\n    session: SessionDep,", "comment": "Instead of having to copy here, would it be possible to just change the model declaration from `list` to `Iterable` instead?", "comment_author": "uranusjr", "comment_url": "https://github.com/apache/airflow/pull/56813#discussion_r2446852244", "type": "performance"}
{"repo": "apache/airflow", "pr_id": 56813, "file_path": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py", "diff_hunk": "@@ -626,7 +646,7 @@ def get_list_dag_runs_batch(\n         session=session,\n     )\n \n-    dag_runs = session.scalars(dag_runs_select)\n+    dag_runs = list(session.scalars(dag_runs_select))", "context": "def get_list_dag_runs_batch(\n    dag_id: Literal[\"~\"],\n    body: DAGRunsBatchBody,\n    readable_dag_runs_filter: ReadableDagRunsFilterDep,\n    session: SessionDep,", "comment": "I am wondering if there is a better way to resolve MyPy error without coping the result?\nSince we add a lot of `list(...)`  in this PR.", "comment_author": "jason810496", "comment_url": "https://github.com/apache/airflow/pull/56813#discussion_r2448614447", "type": "performance"}
{"repo": "getsentry/sentry", "pr_id": 99873, "file_path": "src/sentry/seer/similarity/utils.py", "diff_hunk": "@@ -46,6 +53,48 @@\n \n IGNORED_FILENAMES = [\"<compiler-generated>\"]\n \n+# Path to the local tokenizer model file\n+TOKENIZER_MODEL_PATH = os.path.join(\n+    DATA_ROOT, \"models\", \"jina-embeddings-v2-base-en\", \"tokenizer.json\"\n+)\n+\n+\n+class TokenizerWrapper:\n+    \"\"\"\n+    Lazy-loaded singleton for the tokenizer to avoid expensive initialization at module load time.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._tokenizer: Tokenizer | None = None\n+        self._lock = threading.RLock()\n+\n+    def get_tokenizer(self) -> Tokenizer:\n+        \"\"\"Get the tokenizer instance, initializing it lazily if needed.\"\"\"\n+        if self._tokenizer is None:\n+            with self._lock:\n+                # Double-check pattern to avoid race conditions\n+                if self._tokenizer is None:\n+                    # Try to load from local model first, fallback to remote\n+                    if os.path.exists(TOKENIZER_MODEL_PATH):", "context": "    \"data:text/html;base64\",\n    \"data:text/javascript;base64\",\n    \"html;base64\",\n    \"javascript;base64\",\n]\n\nIGNORED_FILENAMES = [\"<compiler-generated>\"]\n\n\nclass ReferrerOptions(StrEnum):\n    INGEST = \"ingest\"\n    BACKFILL = \"backfill\"\n    DELETION = \"deletion\"", "comment": "this should always be true, no? the remote fallback might not be completely secure or successful (e.g., jinaai makes the huggingface repo private)\n\nif for some reason we do want the remote fallback, then it should supply\n\n```python\nself._tokenizer = Tokenizer.from_pretrained(\n    \"jinaai/jina-embeddings-v2-base-en\",\n    revision=\"322d4d7e2f35e84137961a65af894fda0385eb7a\",\n)\n```", "comment_author": "kddubey", "comment_url": "https://github.com/getsentry/sentry/pull/99873#discussion_r2395452287", "type": "security"}
{"repo": "getsentry/sentry", "pr_id": 101896, "file_path": "src/sentry/sentry_apps/models/sentry_app_installation.py", "diff_hunk": "@@ -141,6 +141,19 @@ def outboxes_for_update(self, shard_identifier: int | None = None) -> list[Contr\n         # these isn't so important in that case.\n         return super().outboxes_for_update(shard_identifier=self.api_application_id or 0)\n \n+    def outboxes_for_delete(self) -> list[ControlOutbox]:\n+        return [\n+            ControlOutbox(\n+                shard_scope=OutboxScope.APP_SCOPE,\n+                shard_identifier=self.id,\n+                object_identifier=self.id,\n+                category=OutboxCategory.SENTRY_APP_INSTALLATION_DELETE,\n+                region_name=region_name,\n+                payload={\"uuid\": self.uuid},\n+            )\n+            for region_name in find_all_region_names()", "context": "    def outboxes_for_update(self, shard_identifier: int | None = None) -> list[ControlOutboxBase]:\n        # Use 0 in case of bad relations from api_applicaiton_id -- the replication ordering for\n        # these isn't so important in that case.\n        return super().outboxes_for_update(shard_identifier=self.api_application_id or 0)\n", "comment": "Changes look good, just needs testing", "comment_author": "GabeVillalobos", "comment_url": "https://github.com/getsentry/sentry/pull/101896#discussion_r2449974065", "type": "other"}
{"repo": "hpcaitech/ColossalAI", "pr_id": 6250, "file_path": "applications/ColossalChat/coati/distributed/inference_backend.py", "diff_hunk": "@@ -175,27 +187,40 @@ class VLLMInferenceBackend(BaseInferenceBackend):\n     )\n     FORCE_GENERATE_CONFIG = dict(\n         logprobs=0,\n-        n=8,\n     )\n \n-    def __init__(self, model_config: Dict[str, Any], generate_config: Dict[str, Any], tokenizer: PreTrainedTokenizer):\n+    def __init__(\n+        self,\n+        model_config: Dict[str, Any],\n+        generate_config: Dict[str, Any],\n+        tokenizer: PreTrainedTokenizer,\n+        num_generations: int = 8,\n+    ):\n         if LLM is None:\n             raise ImportError(\"vllm is not installed\")\n         model_config = update_by_default(model_config, self.DEFAULT_MODEL_CONFIG)\n         path = model_config.pop(\"path\")\n-        self.llm = LLM(path, **model_config)\n+        self.llm = LLM(model=path, **model_config)\n         generate_config = generate_config.copy()\n         generate_config.update(self.FORCE_GENERATE_CONFIG)\n+        generate_config.update({\"n\": num_generations})\n         self.generate_config = SamplingParams(**generate_config)\n         self.tokenizer = tokenizer\n-        self.num_generations = self.FORCE_GENERATE_CONFIG[\"n\"]\n+        self.num_generations = num_generations\n \n     @torch.no_grad()\n     def generate(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, **kwargs) -> Dict[str, torch.Tensor]:\n         micro_batch_size = input_ids.size(0)\n         response_start_idx = input_ids.size(1)\n+        micro_batch_input_ids = input_ids.tolist()\n+        micro_batch_input_ids_no_padding = []\n+        for i in range(micro_batch_size):\n+            for j in range(input_ids.size(1)):\n+                if micro_batch_input_ids[i][j] != self.tokenizer.pad_token_id:\n+                    micro_batch_input_ids_no_padding.append(micro_batch_input_ids[i][j:])\n+                    break", "context": "    def __init__(self, model_config: Dict[str, Any], generate_config: Dict[str, Any], tokenizer: PreTrainedTokenizer):\n        if LLM is None:\n            raise ImportError(\"vllm is not installed\")\n        model_config = update_by_default(model_config, self.DEFAULT_MODEL_CONFIG)\n        path = model_config.pop(\"path\")\n        self.llm = LLM(path, **model_config)\n        generate_config = generate_config.copy()\n        generate_config.update(self.FORCE_GENERATE_CONFIG)\n        self.generate_config = SamplingParams(**generate_config)\n        self.tokenizer = tokenizer\n        self.num_generations = self.FORCE_GENERATE_CONFIG[\"n\"]\n\n    @torch.no_grad()", "comment": "Can be optimized to avoid nested for loop?", "comment_author": "TongLi3701", "comment_url": "https://github.com/hpcaitech/ColossalAI/pull/6250#discussion_r2006797474", "type": "performance"}
{"repo": "hpcaitech/ColossalAI", "pr_id": 6150, "file_path": "colossalai/shardformer/layer/__init__.py", "diff_hunk": "@@ -16,6 +23,7 @@\n     \"Linear1D_Row\",\n     \"GPT2FusedLinearConv1D_Col\",\n     \"GPT2FusedLinearConv1D_Row\",\n+    \"GPT2FusedLinearConv1D_Col\",", "context": "__all__ = [\n    \"Embedding1D\",\n    \"VocabParallelEmbedding1D\",\n    \"LinearWithGradAccum\",\n    \"Linear1D_Col\",\n    \"Linear1D_Row\",\n    \"GPT2FusedLinearConv1D_Col\",\n    \"GPT2FusedLinearConv1D_Row\",\n    \"DropoutForParallelInput\",\n    \"DropoutForReplicatedInput\",\n    \"cross_entropy_1d\",\n    \"dist_cross_entropy\",\n    \"BaseLayerNorm\",", "comment": "This line is the same as line 24.", "comment_author": "ver217", "comment_url": "https://github.com/hpcaitech/ColossalAI/pull/6150#discussion_r1896420992", "type": "style"}
{"repo": "gradio-app/gradio", "pr_id": 12069, "file_path": "client/python/gradio_client/client.py", "diff_hunk": "@@ -620,12 +620,14 @@ def _get_api_info(self):\n                     f\"Could not fetch api info for {self.src}: {fetch.text}\"\n                 )\n         info[\"named_endpoints\"] = {\n-            a: e for a, e in info[\"named_endpoints\"].items() if e.pop(\"show_api\", True)\n+            a: e\n+            for a, e in info[\"named_endpoints\"].items()\n+            if e.pop(\"api_visibility\", \"public\") != \"private\"", "context": "    def _get_api_info(self):\n        api_info_url = urllib.parse.urljoin(self.src_prefixed, utils.RAW_API_INFO_URL)\n        if self.app_version > version.Version(\"3.36.1\"):\n            r = httpx.get(\n                api_info_url,\n                headers=self.headers,\n                cookies=self.cookies,\n                verify=self.ssl_verify,\n                **self.httpx_kwargs,\n            )\n            if r.is_success:\n                info = r.json()\n            else:\n                raise ValueError(f\"Could not fetch api info for {self.src}: {r.text}\")\n        else:\n            fetch = httpx.post(\n                utils.SPACE_FETCHER_URL,\n                json={\n                    \"config\": json.dumps(self.config),\n                    \"serialize\": False,\n                },\n                headers=self.headers,\n                cookies=self.cookies,\n                verify=self.ssl_verify,\n                **self.httpx_kwargs,\n            )\n            if fetch.is_success:\n                info = fetch.json()[\"api\"]\n            else:\n                raise ValueError(\n                    f\"Could not fetch api info for {self.src}: {fetch.text}\"\n                )\n        info[\"named_endpoints\"] = {\n            a: e for a, e in info[\"named_endpoints\"].items() if e.pop(\"show_api\", True)\n        }\n        info[\"unnamed_endpoints\"] = {\n            a: e\n            for a, e in info[\"unnamed_endpoints\"].items()\n            if e.pop(\"show_api\", True)\n        }\n        return info\n", "comment": "If we use the Client to connect to Spaces running Gradio 5 or below, this will make every endpoint public?", "comment_author": "abidlabs", "comment_url": "https://github.com/gradio-app/gradio/pull/12069#discussion_r2438707679", "type": "security"}
{"repo": "gradio-app/gradio", "pr_id": 12042, "file_path": "demo/scatter_plot_demo/run.py", "diff_hunk": "@@ -65,7 +65,7 @@\n             x=\"rating\",\n             y=\"price\",\n             color=\"wait\",\n-            show_actions_button=True,\n+            buttons=[\"actions\"],", "context": "        y=\"price\",\n    )\n    with gr.Row():\n        price_by_rating = gr.ScatterPlot(\n            food_rating_data,\n            x=\"rating\",\n            y=\"price\",\n            color=\"wait\",\n            show_actions_button=True,\n        )\n        price_by_rating_color = gr.ScatterPlot(\n            food_rating_data,\n            x=\"rating\",\n            y=\"price\",", "comment": "Might be good to include the explicit `buttons` parameter in a video component demo as well", "comment_author": "freddyaboulton", "comment_url": "https://github.com/gradio-app/gradio/pull/12042#discussion_r2414035848", "type": "documentation"}
{"repo": "gradio-app/gradio", "pr_id": 11666, "file_path": "gradio/components/number.py", "diff_hunk": "@@ -166,3 +166,8 @@ def example_value(self) -> Any:\n         return self.round_to_precision(\n             3 if self.minimum is None else self.minimum, self.precision\n         )\n+\n+    def read_from_flag(self, payload: str):\n+        import ast\n+\n+        return ast.literal_eval(payload)", "context": "    def example_value(self) -> Any:\n        return self.round_to_precision(\n            3 if self.minimum is None else self.minimum, self.precision\n        )", "comment": "would be safer to do `float()` no?\r\n```suggestion\r\n        return float(payload)\r\n```", "comment_author": "abidlabs", "comment_url": "https://github.com/gradio-app/gradio/pull/11666#discussion_r2252334231", "type": "security"}
{"repo": "mindsdb/mindsdb", "pr_id": 11807, "file_path": "mindsdb/integrations/handlers/mysql_handler/settings.py", "diff_hunk": "@@ -1,48 +1,121 @@\n-from pydantic import BaseModel, AnyUrl, model_validator\n+from typing import Optional\n+from pydantic import BaseModel, AnyUrl, TypeAdapter, model_validator, field_validator, ConfigDict\n from urllib.parse import urlparse\n \n \n+_ANY_URL_ADAPTER = TypeAdapter(AnyUrl)\n+\n+\n class ConnectionConfig(BaseModel):\n-    # TODO: For now validate AnyURL since MySQLDsn wasn't working\n-    url: AnyUrl = None\n-    host: str = None\n+    \"\"\"\n+    MySQL connection configuration with validation.\n+\n+    Supports two connection methods:\n+    1. URL-based: mysql://user:password@host:port/database\n+    2. Parameter-based: individual host, port, user, password, database params\n+    \"\"\"\n+\n+    url: Optional[AnyUrl] = None\n+    host: Optional[str] = None\n     port: int = 3306\n-    user: str = None\n-    password: str = None\n-    database: str = None\n+    user: Optional[str] = None\n+    password: Optional[str] = None\n+    database: Optional[str] = None\n+\n+    @field_validator(\"port\")\n+    @classmethod\n+    def validate_port(cls, v: int) -> int:\n+        \"\"\"Validate that port is within valid range.\"\"\"\n+        if v < 1 or v > 65535:\n+            raise ValueError(f\"Port must be between 1 and 65535, got {v}\")\n+        return v\n+\n+    @field_validator(\"url\", mode=\"before\")\n+    @classmethod\n+    def validate_url(cls, v: Optional[str]) -> Optional[AnyUrl]:\n+        \"\"\"Validate URL using AnyUrl as a fallback option for MySQL DSN parsing.\"\"\"\n+        if v is None or isinstance(v, AnyUrl):\n+            return v\n+        try:\n+            return _ANY_URL_ADAPTER.validate_python(v)\n+        except ValueError as exc:\n+            raise ValueError(f\"Invalid MySQL connection URL: {v}\") from exc\n+\n+    @field_validator(\"host\")\n+    @classmethod\n+    def validate_host(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that host is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Host cannot be empty string\")\n+        return v\n+\n+    @field_validator(\"database\")\n+    @classmethod\n+    def validate_database(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that database name is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Database name cannot be empty string\")\n+        return v\n \n     @model_validator(mode=\"before\")\n+    @classmethod\n     def check_db_params(cls, values):\n         \"\"\"Ensures either URL is provided or all individual parameters are provided.\"\"\"\n         url = values.get(\"url\")\n         host = values.get(\"host\")\n         user = values.get(\"user\")\n         password = values.get(\"password\")\n         database = values.get(\"database\")\n+\n         if not url and not (host and user and password and database):\n+            missing_params = []\n+            if not host:\n+                missing_params.append(\"host\")\n+            if not user:\n+                missing_params.append(\"user\")\n+            if not password:\n+                missing_params.append(\"password\")\n+            if not database:\n+                missing_params.append(\"database\")\n+\n             raise ValueError(\n-                \"Either a valid URL or required parameters (host, user, password, database) must be provided.\"\n+                f\"Either a valid URL or all required parameters must be provided. Missing: {', '.join(missing_params)}\"", "context": "class ConnectionConfig(BaseModel):\n    # TODO: For now validate AnyURL since MySQLDsn wasn't working\n    url: AnyUrl = None\n    host: str = None\n    port: int = 3306\n    user: str = None\n    password: str = None\n    database: str = None\n\n    @model_validator(mode=\"before\")\n    def check_db_params(cls, values):\n        \"\"\"Ensures either URL is provided or all individual parameters are provided.\"\"\"\n        url = values.get(\"url\")\n        host = values.get(\"host\")\n        user = values.get(\"user\")\n        password = values.get(\"password\")\n        database = values.get(\"database\")\n        if not url and not (host and user and password and database):\n            raise ValueError(\n                \"Either a valid URL or required parameters (host, user, password, database) must be provided.\"\n            )\n\n        if url:\n            parsed = urlparse(url)\n            values[\"host\"] = parsed.hostname or host\n            values[\"port\"] = parsed.port if parsed.port is not None else 3306\n            values[\"user\"] = parsed.username or user\n            values[\"password\"] = parsed.password or password\n            values[\"database\"] = parsed.path[1:] if parsed.path else database\n\n            # mysql connector raise error if url is provided\n            values.pop(\"url\", None)\n\n            return values\n\n        if not url:\n            for param in [\"host\", \"user\", \"password\", \"database\"]:\n                if not values.get(param):\n                    raise ValueError(f\"{param} is required when URL is not provided.\")\n        return values\n\n    class Config:\n        str_min_length = 1\n        str_strip_whitespace = True", "comment": "", "comment_author": "lucas-koontz", "comment_url": "https://github.com/mindsdb/mindsdb/pull/11807#discussion_r2471044920", "type": "other"}
{"repo": "numpy/numpy", "pr_id": 29899, "file_path": "numpy/_core/getlimits.py", "diff_hunk": "@@ -303,7 +303,7 @@ def get_str(name, pad=None):\n         return fmt\n \n     def __repr__(self):\n-        if self._repr is not None:\n+        if hasattr(self, \"_repr\") and self._repr is not None:\n             return self._repr", "context": "    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n\n        c = self.__class__.__name__\n\n        # Use precision+1 digits in exponential notation\n        fmt_str = _MACHAR_PARAMS.get(self.dtype.type, {}).get('fmt', '%s')\n        if fmt_str != '%s' and hasattr(self, 'max') and hasattr(self, 'min'):\n            max_str = (fmt_str % self.max).strip()\n            min_str = (fmt_str % self.min).strip()\n        else:\n            max_str = str(self.max)\n            min_str = str(self.min)\n\n        resolution_str = str(self.resolution)\n\n        repr_str = (f\"{c}(resolution={resolution_str}, min={min_str},\"\n                    f\" max={max_str}, dtype={self.dtype})\")\n        self._repr = repr_str\n        return repr_str\n\n    @property", "comment": "```suggestion\n        if (repr_str := getattr(self, \"_repr\", None)) is not None:\n            return repr_str\n```", "comment_author": "seberg", "comment_url": "https://github.com/numpy/numpy/pull/29899#discussion_r2415956913", "type": "logic"}
{"repo": "Lightning-AI/pytorch-lightning", "pr_id": 20320, "file_path": "src/lightning/fabric/fabric.py", "diff_hunk": "@@ -266,7 +269,7 @@ def setup(\n \n         if optimizers:\n             # join both types in a tuple for API convenience\n-            return (module, *optimizers)\n+            return (module, *optimizers, scheduler)", "context": "    def setup(\n        self,\n        module: nn.Module,\n        *optimizers: Optimizer,\n        move_to_device: bool = True,\n        _reapply_compile: bool = True,", "comment": "This is a breaking change, it will cause existing user code to fail, because `scheduler` is returned unconditionally.\r\n\r\nSince `scheduler` is `Optional` in the signature, I suggest we only return it if it was not `None` as an argument, so we won't break anyone's code.", "comment_author": "lantiga", "comment_url": "https://github.com/Lightning-AI/pytorch-lightning/pull/20320#discussion_r1874605366", "type": "logic"}
{"repo": "Lightning-AI/pytorch-lightning", "pr_id": 21098, "file_path": "src/lightning/fabric/accelerators/mps.py", "diff_hunk": "@@ -72,7 +71,7 @@ def auto_device_count() -> int:\n     def is_available() -> bool:\n         \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\n         mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\n-        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")\n+        return not mps_disabled and torch.backends.mps.is_available()", "context": "    def is_available() -> bool:\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")\n\n    @classmethod\n    @override", "comment": "what's the issue with the current checks?\r\n\r\nWe cannot easily remove them. MPS marks as available on older intel-based macs as well while not actually accelerating anything.", "comment_author": "justusschock", "comment_url": "https://github.com/Lightning-AI/pytorch-lightning/pull/21098#discussion_r2290288397", "type": "logic"}
{"repo": "matplotlib/matplotlib", "pr_id": 29221, "file_path": "lib/matplotlib/colors.py", "diff_hunk": "@@ -1288,10 +1288,10 @@ def __init__(self, colormaps, combination_mode, name='multivariate colormap'):\n         combination_mode: str, 'sRGB_add' or 'sRGB_sub'\n             Describe how colormaps are combined in sRGB space\n \n-            - If 'sRGB_add' -> Mixing produces brighter colors\n-              `sRGB = sum(colors)`\n-            - If 'sRGB_sub' -> Mixing produces darker colors\n-              `sRGB = 1 - sum(1 - colors)`\n+            - If 'sRGB_add': Mixing produces brighter colors\n+              ``sRGB = sum(colors)``\n+            - If 'sRGB_sub': Mixing produces darker colors\n+              ``sRGB = 1 - sum(1 - colors)``", "context": "    def reversed(self, name=None):\n        \"\"\"\n        Return a reversed instance of the Colormap.\n\n        Parameters\n        ----------\n        name : str, optional\n            The name for the reversed colormap. If None, the\n            name is set to ``self.name + \"_r\"``.\n\n        Returns\n        -------\n        LinearSegmentedColormap\n            The reversed colormap.\n        \"\"\"\n        if name is None:\n            name = self.name + \"_r\"\n\n        # Using a partial object keeps the cmap picklable.\n        data_r = {key: (functools.partial(self._reverser, data)\n                        if callable(data) else\n                        [(1.0 - x, y1, y0) for x, y0, y1 in reversed(data)])\n                  for key, data in self._segmentdata.items()}\n\n        new_cmap = LinearSegmentedColormap(name, data_r, self.N, self._gamma)\n        # Reverse the over/under values too\n        new_cmap._rgba_over = self._rgba_under\n        new_cmap._rgba_under = self._rgba_over\n        new_cmap._rgba_bad = self._rgba_bad\n        return new_cmap\n\n", "comment": " I think this is very very clear now", "comment_author": "story645", "comment_url": "https://github.com/matplotlib/matplotlib/pull/29221#discussion_r1931385280", "type": "other"}
{"repo": "matplotlib/matplotlib", "pr_id": 23293, "file_path": "lib/matplotlib/font_manager.py", "diff_hunk": "@@ -1473,6 +1477,113 @@ def get_font(filename, hinting_factor=None):\n                      thread_id=threading.get_ident())\n \n \n+def _split_ttc(ttc_path):\n+    \"\"\"SPlit a TTC ont into TTF files\"\"\"", "context": "def get_font(filename, hinting_factor=None):\n    # Resolving the path avoids embedding the font twice in pdf/ps output if a\n    # single font is selected using two different relative paths.\n    filename = _cached_realpath(filename)\n    if hinting_factor is None:\n        hinting_factor = rcParams['text.hinting_factor']\n    # also key on the thread ID to prevent segfaults with multi-threading\n    return _get_font(filename, hinting_factor,\n                     _kerning_factor=rcParams['text.kerning_factor'],\n                     thread_id=threading.get_ident())\n\n", "comment": "```suggestion\r\n    \"\"\"Split a TTC file into TTF files\"\"\"\r\n```", "comment_author": "oscargus", "comment_url": "https://github.com/matplotlib/matplotlib/pull/23293#discussion_r902819944", "type": "documentation"}
{"repo": "wagtail/wagtail", "pr_id": 13317, "file_path": "wagtail/admin/tests/pages/test_edit_page.py", "diff_hunk": "@@ -2987,12 +2987,33 @@ def test_field_error(self):\n         )\n         self.assertEqual(response.status_code, 200)\n \n-        self.assertContains(\n-            response, \"The page could not be saved due to validation errors\"\n+        soup = self.get_soup(response.content)\n+\n+        header_messages = soup.css.select(\".messages[role='status'] ul > li\")\n+\n+        # the top level message should indicate that the page could not be saved\n+        self.assertEqual(len(header_messages), 1)\n+        message = header_messages[0]\n+        self.assertIn(\n+            \"The page could not be saved due to validation errors\", message.get_text()", "context": "    def test_field_error(self):\n        \"\"\"Field errors should be shown against the relevant fields, not in the header message\"\"\"\n        post_data = {\n            \"title\": \"\",\n            \"date_from\": \"2017-12-25\",\n            \"slug\": \"christmas\",\n            \"audience\": \"public\",\n            \"location\": \"The North Pole\",\n            \"cost\": \"Free\",\n            \"carousel_items-TOTAL_FORMS\": 0,\n            \"carousel_items-INITIAL_FORMS\": 0,\n            \"carousel_items-MIN_NUM_FORMS\": 0,\n            \"carousel_items-MAX_NUM_FORMS\": 0,\n            \"speakers-TOTAL_FORMS\": 0,\n            \"speakers-INITIAL_FORMS\": 0,\n            \"speakers-MIN_NUM_FORMS\": 0,\n            \"speakers-MAX_NUM_FORMS\": 0,\n            \"related_links-TOTAL_FORMS\": 0,\n            \"related_links-INITIAL_FORMS\": 0,\n            \"related_links-MIN_NUM_FORMS\": 0,\n            \"related_links-MAX_NUM_FORMS\": 0,\n            \"head_counts-TOTAL_FORMS\": 0,\n            \"head_counts-INITIAL_FORMS\": 0,\n            \"head_counts-MIN_NUM_FORMS\": 0,\n            \"head_counts-MAX_NUM_FORMS\": 0,\n        }\n        response = self.client.post(\n            reverse(\"wagtailadmin_pages:edit\", args=(self.christmas_page.id,)),\n            post_data,\n        )\n        self.assertEqual(response.status_code, 200)\n\n        self.assertContains(\n            response, \"The page could not be saved due to validation errors\"\n        )\n        # the error should only appear once: against the field, not in the header message\n        self.assertContains(response, \"error-message\", count=1)\n        self.assertContains(response, \"This field is required\", count=1)\n", "comment": "Nice!", "comment_author": "thibaudcolas", "comment_url": "https://github.com/wagtail/wagtail/pull/13317#discussion_r2450900605", "type": "other"}
{"repo": "wagtail/wagtail", "pr_id": 13359, "file_path": "wagtail/admin/templatetags/wagtailadmin_tags.py", "diff_hunk": "@@ -1347,25 +1347,26 @@ def keyboard_shortcuts_dialog(context):\n                     f\"{KEYS.MOD} + {KEYS.SHIFT} + z\" if is_mac else f\"{KEYS.MOD} + y\",\n                 ),\n             ],\n-            (\"actions-model\", _(\"Actions\")): [\n-                (_(\"Show keyboard shortcuts\"), \"?\"),\n+            (\"actions\", _(\"Actions\")): [\n                 (_(\"Save changes\"), f\"{KEYS.MOD} + s\"),\n                 (_(\"Preview\"), f\"{KEYS.MOD} + p\"),\n-                (_(\"Toggle sidebar\"), \"[\"),\n-                (_(\"Toggle minimap\"), \"]\"),\n-                (_(\"Search\"), \"/\"),\n                 (_(\"Add or show comments\"), f\"{KEYS.CTRL} + {KEYS.ALT} + m\")\n                 if comments_enabled\n                 else None,\n             ],\n-            (\"rich-text-content\", _(\"Text content\")): [\n-                (_(\"Insert or edit a link\"), f\"{KEYS.MOD} + k\")\n+            (\"application\", _(\"Application\")): [\n+                (_(\"Show keyboard shortcuts\"), \"?\"),\n+                (_(\"Search\"), \"/\"),\n+                (_(\"Toggle sidebar\"), \"[\"),\n+                (_(\"Toggle minimap\"), \"]\"),\n+                (_(\"Close modal dialogs (like this one)\"), f\"{KEYS.ESC}\"),\n             ],\n-            (\"rich-text-formatting\", _(\"Text formatting\")): [\n+            (\"text-formatting\", _(\"Text formatting\")): [\n+                (_(\"Bold\"), f\"{KEYS.MOD} + b\"),\n                 (_(\"Italic\"), f\"{KEYS.MOD} + i\"),\n-                (_(\"Underline\"), f\"{KEYS.MOD} + u\"),\n+                (_(\"Insert or edit a link\"), f\"{KEYS.MOD} + k\"),\n                 (_(\"Monospace (code)\"), f\"{KEYS.MOD} + j\"),\n-                (_(\"Strike-through\"), f\"{KEYS.MOD} + x\"),\n+                (_(\"Strike-through\"), f\"{KEYS.MOD}+ {KEYS.SHIFT} + x\"),", "context": "def keyboard_shortcuts_dialog(context):\n    \"\"\"\n    Renders the keyboard shortcuts dialog content with the\n    appropriate shortcuts for the user's platform.\n    Note: Shortcut keys are intentionally not translated.\n    \"\"\"\n\n    comments_enabled = get_comments_enabled()\n    user_agent = context[\"request\"].headers.get(\"User-Agent\", \"\")\n    is_mac = re.search(r\"Mac|iPod|iPhone|iPad\", user_agent)\n    KEYS = get_keyboard_key_labels_from_request(context[\"request\"])\n\n    return {\n        \"shortcuts\": {\n            (\"actions-common\", _(\"Common actions\")): [\n                (_(\"Copy\"), f\"{KEYS.MOD} + c\"),\n                (_(\"Cut\"), f\"{KEYS.MOD} + x\"),\n                (_(\"Paste\"), f\"{KEYS.MOD} + v\"),\n                (\n                    _(\"Paste and match style\")\n                    if is_mac\n                    else _(\"Paste without formatting\"),\n                    f\"{KEYS.MOD} + {KEYS.SHIFT} + v\",\n                ),\n                (_(\"Undo\"), f\"{KEYS.MOD} + z\"),\n                (\n                    _(\"Redo\"),\n                    f\"{KEYS.MOD} + {KEYS.SHIFT} + z\" if is_mac else f\"{KEYS.MOD} + y\",\n                ),\n            ],\n            (\"actions-model\", _(\"Actions\")): [\n                (_(\"Show keyboard shortcuts\"), \"?\"),\n                (_(\"Save changes\"), f\"{KEYS.MOD} + s\"),\n                (_(\"Preview\"), f\"{KEYS.MOD} + p\"),\n                (_(\"Toggle sidebar\"), \"[\"),\n                (_(\"Toggle minimap\"), \"]\"),\n                (_(\"Search\"), \"/\"),\n                (_(\"Add or show comments\"), f\"{KEYS.CTRL} + {KEYS.ALT} + m\")\n                if comments_enabled\n                else None,\n            ],\n            (\"rich-text-content\", _(\"Text content\")): [\n                (_(\"Insert or edit a link\"), f\"{KEYS.MOD} + k\")\n            ],\n            (\"rich-text-formatting\", _(\"Text formatting\")): [\n                (_(\"Italic\"), f\"{KEYS.MOD} + i\"),\n                (_(\"Underline\"), f\"{KEYS.MOD} + u\"),\n                (_(\"Monospace (code)\"), f\"{KEYS.MOD} + j\"),\n                (_(\"Strike-through\"), f\"{KEYS.MOD} + x\"),\n                (_(\"Superscript\"), f\"{KEYS.MOD} + .\"),\n                (_(\"Subscript\"), f\"{KEYS.MOD} + ,\"),\n            ],\n        }\n    }\n\n\n@register.inclusion_tag(\"wagtailadmin/shared/human_readable_date.html\")", "comment": "```suggestion\r\n                (_(\"Strike-through\"), f\"{KEYS.MOD} + {KEYS.SHIFT} + x\"),\r\n```\r\n\r\nWhite space missing.", "comment_author": "lb-", "comment_url": "https://github.com/wagtail/wagtail/pull/13359#discussion_r2367502368", "type": "style"}
{"repo": "wagtail/wagtail", "pr_id": 13188, "file_path": "wagtail/project_template/project_name/settings/base.py", "diff_hunk": "@@ -148,12 +148,8 @@\n     \"default\": {\n         \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n     },\n-    # ManifestStaticFilesStorage is recommended in production, to prevent", "context": "MEDIA_URL = \"/media/\"\n\n# Default storage settings, with the staticfiles storage updated.\n# See https://docs.djangoproject.com/en/{{ docs_version }}/ref/settings/#std-setting-STORAGES\nSTORAGES = {\n    \"default\": {\n        \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n    },\n    # ManifestStaticFilesStorage is recommended in production, to prevent\n    # outdated JavaScript / CSS assets being served from cache\n    # (e.g. after a Wagtail upgrade).\n    # See https://docs.djangoproject.com/en/{{ docs_version }}/ref/contrib/staticfiles/#manifeststaticfilesstorage\n    \"staticfiles\": {\n        \"BACKEND\": \"django.contrib.staticfiles.storage.ManifestStaticFilesStorage\",", "comment": "The comment above the setting saying \"with the staticfiles storage updated\" should also be removed, as this is no longer true.", "comment_author": "gasman", "comment_url": "https://github.com/wagtail/wagtail/pull/13188#discussion_r2172240297", "type": "documentation"}
{"repo": "wagtail/wagtail", "pr_id": 12932, "file_path": "wagtail/api/v2/filters.py", "diff_hunk": "@@ -78,49 +79,26 @@ def filter_queryset(self, request, queryset, view):\n         And random ordering\n         Eg: ?order=random\n         \"\"\"\n-        if \"order\" in request.GET:\n-            order_by_list = request.GET[\"order\"].split(\",\")\n+        order_param = request.GET.get(\"order\")\n+        if not order_param:\n+            return queryset\n \n-            # Random ordering\n-            if \"random\" in order_by_list:\n-                if len(order_by_list) > 1:\n-                    raise BadRequestError(\n-                        \"random ordering cannot be combined with other fields\"\n-                    )\n-                # Prevent ordering by random with offset\n-                if \"offset\" in request.GET:\n-                    raise BadRequestError(\n-                        \"random ordering with offset is not supported\"\n-                    )\n-\n-                return queryset.order_by(\"?\")\n-\n-            order_by_fields = []\n-            for order_by in order_by_list:\n-                # Check if reverse ordering is set\n-                if order_by.startswith(\"-\"):\n-                    reverse_order = True\n-                    order_by = order_by[1:]\n-                else:\n-                    reverse_order = False\n-\n-                # Add ordering\n-                if order_by in view.get_available_fields(queryset.model):", "context": "    def filter_queryset(self, request, queryset, view):\n        \"\"\"\n        This applies ordering to the result set with support for multiple fields.\n        Eg: ?order=title or ?order=title,created_at\n\n        It also supports reverse ordering\n        Eg: ?order=-title\n\n        And random ordering\n        Eg: ?order=random\n        \"\"\"\n        if \"order\" in request.GET:\n            order_by_list = request.GET[\"order\"].split(\",\")\n\n            # Random ordering\n            if \"random\" in order_by_list:\n                if len(order_by_list) > 1:\n                    raise BadRequestError(\n                        \"random ordering cannot be combined with other fields\"\n                    )\n                # Prevent ordering by random with offset\n                if \"offset\" in request.GET:\n                    raise BadRequestError(\n                        \"random ordering with offset is not supported\"\n                    )\n\n                return queryset.order_by(\"?\")\n\n            order_by_fields = []\n            for order_by in order_by_list:\n                # Check if reverse ordering is set\n                if order_by.startswith(\"-\"):\n                    reverse_order = True\n                    order_by = order_by[1:]\n                else:\n                    reverse_order = False\n\n                # Add ordering\n                if order_by in view.get_available_fields(queryset.model):\n                    order_by_fields.append(order_by)\n                else:\n                    # Unknown field\n                    raise BadRequestError(\n                        \"cannot order by '%s' (unknown field)\" % order_by\n                    )\n\n            # Apply ordering to the queryset\n            queryset = queryset.order_by(*order_by_fields)\n\n            # Reverse order if needed\n            if reverse_order:\n                queryset = queryset.reverse()\n\n        return queryset\n\n", "comment": "I notice the new code is no longer validating the list against `view.get_available_fields`. I think it's important to keep this - if there are private fields that exist on the model but aren't exposed in the API, being able to order by those fields could potentially leak information.\r\n\r\n(Also, we should probably be passing [the `db_fields_only` flag](https://github.com/wagtail/wagtail/blob/47fc2008640ece76be97f3d03da224b903a99047/wagtail/api/v2/views.py#L170-L178) here...)", "comment_author": "gasman", "comment_url": "https://github.com/wagtail/wagtail/pull/12932#discussion_r2088720064", "type": "security"}
{"repo": "sgl-project/sglang", "pr_id": 11910, "file_path": "python/sglang/srt/managers/scheduler.py", "diff_hunk": "@@ -1162,6 +1164,64 @@ def init_req_max_new_tokens(self, req):\n             self.max_req_len - len(req.origin_input_ids) - 1,\n         )\n \n+    def _materialize_and_broadcast_mm_inputs(\n+        self,\n+        raw_mm_inputs: Optional[dict],\n+    ):\n+        \"\"\"Materialize MultimodalInputs once on the entry rank and broadcast to others.\n+\n+        Entry rank:\n+        - constructs MultimodalInputs.from_dict(raw_mm_inputs) once\n+        - broadcasts to other ranks in self.cpu_group (if world_size > 1)\n+\n+        Non-entry ranks:\n+        - receive the object via broadcast (if world_size > 1)\n+        - otherwise (single-rank / no group) fall back to local from_dict\n+\n+        Returns:\n+            MultimodalInputs | None\n+        \"\"\"\n+        if raw_mm_inputs is None:\n+            return None\n+\n+        group_world_size = 1\n+        try:\n+            if (\n+                torch.distributed.is_available()\n+                and torch.distributed.is_initialized()\n+                and self.cpu_group is not None\n+            ):\n+                group_world_size = torch.distributed.get_world_size(\n+                    group=self.cpu_group\n+                )\n+        except Exception as e:\n+            logger.warning(\n+                \"Failed to get world size in mm_inputs handling with {e}, fallback to 1.\"", "context": "    def init_req_max_new_tokens(self, req):\n        req.sampling_params.max_new_tokens = min(\n            (\n                req.sampling_params.max_new_tokens\n                if req.sampling_params.max_new_tokens is not None\n                else 1 << 30\n            ),\n            self.max_req_len - len(req.origin_input_ids) - 1,\n        )\n", "comment": "```suggestion\n                f\"Failed to get world size in mm_inputs handling with {e}, fallback to 1.\"\n```", "comment_author": "JustinTong0323", "comment_url": "https://github.com/sgl-project/sglang/pull/11910#discussion_r2458698899", "type": "style"}
{"repo": "sgl-project/sglang", "pr_id": 11652, "file_path": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py", "diff_hunk": "@@ -71,6 +71,7 @@ def __init__(self, eagle_worker: EAGLEWorker):\n         self.seq_lens_cpu = torch.full(\n             (self.max_bs,), self.seq_len_fill_value, dtype=torch.int32\n         )\n+        self.extend_seq_lens_cpu = [self.num_tokens_per_bs] * self.max_num_token", "context": "    def __init__(self, eagle_worker: EAGLEWorker):\n        # Parse args\n        self.eagle_worker = eagle_worker\n        if not hasattr(eagle_worker, \"model_runner\"):\n            # V2: EagleDraftWorker\n            self.model_runner = model_runner = eagle_worker.draft_runner\n        else:\n            self.model_runner = model_runner = eagle_worker.model_runner\n\n        self.graphs = {}\n        self.output_buffers = {}\n        self.enable_torch_compile = model_runner.server_args.enable_torch_compile\n        self.disable_padding = model_runner.server_args.disable_cuda_graph_padding\n        self.require_gathered_buffer = require_gathered_buffer(model_runner.server_args)\n        self.require_mlp_tp_gather = require_mlp_tp_gather(model_runner.server_args)\n        self.require_mlp_sync = require_mlp_sync(model_runner.server_args)\n        self.require_attn_tp_gather = require_attn_tp_gather(model_runner.server_args)\n        self.tp_size = self.model_runner.tp_size\n        self.dp_size = model_runner.server_args.dp_size\n        self.speculative_num_steps = model_runner.server_args.speculative_num_steps\n        self.topk = model_runner.server_args.speculative_eagle_topk\n        self.enable_profile_cuda_graph = (\n            model_runner.server_args.enable_profile_cuda_graph\n        )\n        self.capture_bs, self.compile_bs = get_batch_sizes_to_capture(model_runner)\n        self.padded_static_len = -1\n        self.deepep_adapter = DeepEPCudaGraphRunnerAdapter()\n\n        # Attention backend\n        self.num_tokens_per_bs = self.speculative_num_steps + 1\n        self.max_bs = max(self.capture_bs)\n        self.max_num_token = self.max_bs * self.num_tokens_per_bs\n\n        self.eagle_worker.draft_extend_attn_backend.init_cuda_graph_state(\n            self.max_bs, self.max_num_token\n        )\n        self.seq_len_fill_value = (\n            self.eagle_worker.draft_extend_attn_backend.get_cuda_graph_seq_len_fill_value()\n        )\n        self.seq_lens_cpu = torch.full(\n            (self.max_bs,), self.seq_len_fill_value, dtype=torch.int32\n        )\n\n        if self.enable_torch_compile:\n            set_torch_compile_config()\n\n        # Graph inputs\n        with torch.device(\"cuda\"):\n            self.input_ids = torch.zeros((self.max_num_token,), dtype=torch.int64)\n            self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32)\n            self.out_cache_loc = torch.ones((self.max_num_token,), dtype=torch.int64)\n            self.positions = torch.zeros((self.max_num_token,), dtype=torch.int64)\n            self.mrope_positions = torch.zeros(\n                (3, self.max_num_token), dtype=torch.int64\n            )\n\n            if self.eagle_worker.speculative_algorithm.is_eagle3():\n                self.hidden_states = torch.zeros(\n                    (\n                        self.max_num_token,\n                        (\n                            self.model_runner.model_config.hf_config.target_hidden_size\n                            * 3\n                            if hasattr(\n                                self.model_runner.model_config.hf_config,\n                                \"target_hidden_size\",\n                            )\n                            else self.model_runner.model_config.hidden_size * 3\n                        ),\n                    ),\n                    dtype=self.model_runner.dtype,\n                )\n            else:\n                self.hidden_states = torch.zeros(\n                    (self.max_num_token, self.model_runner.model_config.hidden_size),\n                    dtype=self.model_runner.dtype,\n                )\n\n            self.seq_lens = torch.ones((self.max_bs,), dtype=torch.int32)\n            self.extend_seq_lens = torch.ones((self.max_bs,), dtype=torch.int32)\n            self.accept_length = torch.full(\n                (self.max_bs,), self.num_tokens_per_bs, dtype=torch.int32\n            )\n\n            if self.require_gathered_buffer:\n                if self.require_mlp_tp_gather:\n                    self.global_num_tokens_gpu = torch.zeros(\n                        (self.dp_size,), dtype=torch.int32\n                    )\n                    self.global_num_tokens_for_logprob_gpu = torch.zeros(\n                        (self.dp_size,), dtype=torch.int32\n                    )\n                else:\n                    assert self.require_attn_tp_gather\n                    self.global_num_tokens_gpu = torch.zeros((1,), dtype=torch.int32)\n                    self.global_num_tokens_for_logprob_gpu = torch.zeros(\n                        (1,), dtype=torch.int32\n                    )\n            else:\n                self.global_num_tokens_gpu = None\n                self.global_num_tokens_for_logprob_gpu = None\n\n            if hasattr(\n                self.model_runner.model_config.hf_config, \"draft_vocab_size\"\n            ):  # llama_eagle\n                vocab_size = self.model_runner.model_config.hf_config.draft_vocab_size\n            elif hasattr(\n                self.model_runner.model_config.hf_config, \"hot_vocab_size\"\n            ):  # llama_eagle3\n                vocab_size = self.model_runner.model_config.hf_config.hot_vocab_size\n            else:\n                vocab_size = self.model_runner.model_config.vocab_size\n\n            self.next_token_logits_buffer = torch.zeros(\n                (self.max_bs, vocab_size),\n                dtype=torch.float,\n            )\n\n        # Capture\n        try:\n            with model_capture_mode():\n                self.capture()\n        except RuntimeError as e:\n            raise Exception(\n                f\"Capture cuda graph failed: {e}\\n{CUDA_GRAPH_CAPTURE_FAILED_MSG}\"\n            )\n", "comment": "Why is the length `self.max_num_token` rather than `self.max_bs`?", "comment_author": "Fridge003", "comment_url": "https://github.com/sgl-project/sglang/pull/11652#discussion_r2434088283", "type": "logic"}
{"repo": "saltstack/salt", "pr_id": 63536, "file_path": "tests/pytests/unit/modules/test_reg.py", "diff_hunk": "@@ -142,12 +159,13 @@ def test_value_exists_unknown_key_error():\n         side_effect=win32api.error(123, \"RegOpenKeyEx\", \"Unknown error\")\n     )\n     with patch(\"salt.utils.win_reg.win32api.RegOpenKeyEx\", mock_error):\n-        with pytest.raises(win32api.error):\n-            reg.value_exists(\n-                hive=\"HKLM\",\n-                key=\"SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\",\n-                vname=\"CommonFilesDir\",\n-            )\n+        pytest.raises(", "context": "def test_value_exists_unknown_key_error():\n    \"\"\"\n    Tests the value_exists function with an unknown error when opening the\n    key\n    \"\"\"\n    mock_error = MagicMock(\n        side_effect=win32api.error(123, \"RegOpenKeyEx\", \"Unknown error\")\n    )\n    with patch(\"salt.utils.win_reg.win32api.RegOpenKeyEx\", mock_error):\n        with pytest.raises(win32api.error):\n            reg.value_exists(\n                hive=\"HKLM\",\n                key=\"SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\",\n                vname=\"CommonFilesDir\",\n            )\n\n", "comment": "Why are you preferring not to use the `pytest.raises` as a context manager?", "comment_author": "s0undt3ch", "comment_url": "https://github.com/saltstack/salt/pull/63536#discussion_r1430088421", "type": "logic"}
{"repo": "cvat-ai/cvat", "pr_id": 9694, "file_path": "cvat/apps/dataset_manager/task.py", "diff_hunk": "@@ -1133,14 +1134,8 @@ def export_job(\n     save_images=False,\n     temp_dir: str | None = None,\n ):\n-    # For big tasks dump function may run for a long time and\n-    # we dont need to acquire lock after the task has been initialized from DB.\n-    # But there is the bug with corrupted dump file in case 2 or\n-    # more dump request received at the same time:\n-    # https://github.com/cvat-ai/cvat/issues/217\n-    with transaction.atomic():\n-        job = JobAnnotation(job_id, prefetch_images=True, lock_job_in_db=True)\n-        job.init_from_db()\n+    job = JobAnnotation(job_id, prefetch_images=True, lock_job_in_db=True)", "context": "def export_job(\n    job_id: int,\n    dst_file: str,\n    *,\n    format_name: str,\n    server_url: str | None = None,\n    save_images=False,\n    temp_dir: str | None = None,", "comment": "Probably we don't need to lock now, otherwise the job will be unavailable during the export.", "comment_author": "zhiltsov-max", "comment_url": "https://github.com/cvat-ai/cvat/pull/9694#discussion_r2297819190", "type": "security"}
{"repo": "ivy-llc/ivy", "pr_id": 27090, "file_path": "ivy_tests/test_ivy/test_functional/test_core/test_gradients.py", "diff_hunk": "@@ -239,9 +239,7 @@ def func(xs):\n @pytest.mark.parametrize(\"nth\", [1, 2, 3])\n def test_grad(x, dtype, func, backend_fw, nth):\n     # ToDo: Remove skipping for paddle and jax for nth > 1\n-    if backend_fw == \"numpy\" or (\n-        (backend_fw == \"paddle\" or backend_fw == \"jax\") and nth > 1\n-    ):\n+    if backend_fw == \"numpy\" or backend_fw in [\"paddle\", \"jax\"] and nth > 1:", "context": "def test_grad(x, dtype, func, backend_fw, nth):\n    # ToDo: Remove skipping for paddle and jax for nth > 1\n    if backend_fw == \"numpy\" or (\n        (backend_fw == \"paddle\" or backend_fw == \"jax\") and nth > 1\n    ):\n        return\n\n    with BackendHandler.update_backend(backend_fw) as ivy_backend:\n        _variable_fn = ivy_backend.ivy.functional.ivy.gradients._variable\n        var = _variable_fn(ivy_backend.array(x, dtype=dtype))\n        fn = ivy_backend.grad(func)\n        if nth > 1:\n            for _ in range(1, nth):\n                fn = ivy_backend.grad(fn)\n        grad = fn(var)\n        grad_np = helpers.flatten_and_to_np(ret=grad, backend=backend_fw)\n\n    with BackendHandler.update_backend(\"tensorflow\") as gt_backend:\n        _variable_fn = gt_backend.ivy.functional.ivy.gradients._variable\n        var = _variable_fn(ivy.array(x, dtype=dtype))\n        fn = gt_backend.grad(func)\n        if nth > 1:\n            for _ in range(1, nth):\n                fn = gt_backend.grad(fn)\n\n        grad_gt = fn(var)\n        grad_np_from_gt = helpers.flatten_and_to_np(ret=grad_gt, backend=\"tensorflow\")\n\n    for grad, grad_from_gt in zip(grad_np, grad_np_from_gt):\n        assert grad.shape == grad_from_gt.shape\n        assert np.allclose(grad, grad_from_gt)\n\n", "comment": "I think this would be clearer if we left the brackets\r\n```suggestion\r\n    if backend_fw == \"numpy\" or (backend_fw in [\"paddle\", \"jax\"] and nth > 1):\r\n```", "comment_author": "KareemMAX", "comment_url": "https://github.com/ivy-llc/ivy/pull/27090#discussion_r1376734473", "type": "style"}
{"repo": "scipy/scipy", "pr_id": 23739, "file_path": "scipy/stats/tests/test_stats.py", "diff_hunk": "@@ -8766,13 +8775,14 @@ class TestPageTrendTest:\n     # results could be checked (to limited precision) against tables in\n     # scipy.stats.page_trend_test reference [1]\n \n-    np.random.seed(0)\n-    data_3_25 = np.random.rand(3, 25)\n-    data_10_26 = np.random.rand(10, 26)\n+    rng = np.random.default_rng(3113562111)\n+    data_3_25 = rng.random((3, 25))\n+    rng = np.random.default_rng(3113562111)", "context": "    def test_match_conover_examples(self):\n        # Test against the examples in [1] (Conover Practical Nonparametric\n        # Statistics Third Edition) pg 139\n\n        # Example 1\n        # Data is [189, 233, 195, 160, 212, 176, 231, 185, 199, 213, 202, 193,\n        # 174, 166, 248]\n        # Two-sided test of whether the upper quartile (p=0.75) equals 193\n        # (q=193). Conover shows that 7 of the observations are less than or\n        # equal to 193, and \"for the binomial random variable Y, P(Y<=7) =\n        # 0.0173\", so the two-sided p-value is twice that, 0.0346.\n        x = [189, 233, 195, 160, 212, 176, 231, 185, 199, 213, 202, 193,\n             174, 166, 248]\n        pvalue_expected = 0.0346\n        res = stats.quantile_test(x, q=193, p=0.75, alternative='two-sided')\n        assert_allclose(res.pvalue, pvalue_expected, rtol=1e-5)\n\n        # Example 2\n        # Conover doesn't give explicit data, just that 8 out of 112\n        # observations are 60 or less. The test is whether the median time is\n        # equal to 60 against the alternative that the median is greater than\n        # 60. The p-value is calculated as P(Y<=8), where Y is again a binomial\n        # distributed random variable, now with p=0.5 and n=112. Conover uses a\n        # normal approximation, but we can easily calculate the CDF of the\n        # binomial distribution.\n        x = [59]*8 + [61]*(112-8)\n        pvalue_expected = stats.binom(p=0.5, n=112).pmf(k=8)\n        res = stats.quantile_test(x, q=60, p=0.5, alternative='greater')\n        assert_allclose(res.pvalue, pvalue_expected, atol=1e-10)\n\n", "comment": "why is rng recreated again? `data_10_26` will have overlapping random numbers with `data_3_25`. The previous code would have those two objects with totally different random numbers.\r\n\r\ni.e. shouldn't it just be:\r\n```\r\n    rng = np.random.default_rng(3113562111)\r\n    data_3_25 = rng.random((3, 25))\r\n    data_10_26 = rng.random((10, 26))\r\n```", "comment_author": "andyfaff", "comment_url": "https://github.com/scipy/scipy/pull/23739#discussion_r2409851561", "type": "performance"}
{"repo": "scipy/scipy", "pr_id": 23135, "file_path": "scipy/interpolate/_cubic.py", "diff_hunk": "@@ -556,7 +566,9 @@ def __init__(self, x, y, axis=0, *, method: Literal[\"akima\", \"makima\"]=\"akima\",\n             f12 = f1 + f2\n \n             # These are the mask of where the slope at breakpoint is defined:\n-            ind = np.nonzero(f12 > break_mult * np.max(f12, initial=-np.inf))\n+            mmax = xp.asarray(np.max(np.asarray(f12), initial=-np.inf))\n+            ind = xp.nonzero(f12 > break_mult * mmax)", "context": "    def __init__(self, x, y, axis=0, *, method: Literal[\"akima\", \"makima\"]=\"akima\",\n                 extrapolate:bool | None = None):\n        if method not in {\"akima\", \"makima\"}:\n            raise NotImplementedError(f\"`method`={method} is unsupported.\")\n        # Original implementation in MATLAB by N. Shamsundar (BSD licensed), see\n        # https://www.mathworks.com/matlabcentral/fileexchange/1814-akima-interpolation\n        x, dx, y, axis, _ = prepare_input(x, y, axis)\n\n        if np.iscomplexobj(y):\n            msg = (\"`Akima1DInterpolator` only works with real values for `y`. \"\n                   \"If you are trying to use the real components of the passed array, \"\n                   \"use `np.real` on the array before passing to \"\n                   \"`Akima1DInterpolator`.\")\n            raise ValueError(msg)\n\n        # Akima extrapolation historically False; parent class defaults to True.\n        extrapolate = False if extrapolate is None else extrapolate\n\n        if y.shape[0] == 2:\n            # edge case: only have two points, use linear interpolation\n            xp = x.reshape((x.shape[0],) + (1,)*(y.ndim-1))\n            hk = xp[1:] - xp[:-1]\n            mk = (y[1:] - y[:-1]) / hk\n            t = np.zeros_like(y)\n            t[...] = mk\n        else:\n            # determine slopes between breakpoints\n            m = np.empty((x.size + 3, ) + y.shape[1:])\n            dx = dx[(slice(None), ) + (None, ) * (y.ndim - 1)]\n            m[2:-2] = np.diff(y, axis=0) / dx\n\n            # add two additional points on the left ...\n            m[1] = 2. * m[2] - m[3]\n            m[0] = 2. * m[1] - m[2]\n            # ... and on the right\n            m[-2] = 2. * m[-3] - m[-4]\n            m[-1] = 2. * m[-2] - m[-3]\n\n            # if m1 == m2 != m3 == m4, the slope at the breakpoint is not\n            # defined. This is the fill value:\n            t = .5 * (m[3:] + m[:-3])\n            # get the denominator of the slope t\n            dm = np.abs(np.diff(m, axis=0))\n            if method == \"makima\":\n                pm = np.abs(m[1:] + m[:-1])\n                f1 = dm[2:] + 0.5 * pm[2:]\n                f2 = dm[:-2] + 0.5 * pm[:-2]\n            else:\n                f1 = dm[2:]\n                f2 = dm[:-2]\n\n            # makima is more numerically stable for small f12,\n            # so a finite cutoff should not improve any behavior\n            # however, akima has a qualitative discontinuity near f12=0\n            # a finite cutoff moves it, but cannot remove it.\n            # the cutoff break_mult could be made a keyword argument\n            # method='akima' also benefits from a check for m2=m3\n            break_mult = 1.e-9\n\n            f12 = f1 + f2\n\n            # These are the mask of where the slope at breakpoint is defined:\n            ind = np.nonzero(f12 > break_mult * np.max(f12, initial=-np.inf))\n            x_ind, y_ind = ind[0], ind[1:]\n            # Set the slope at breakpoint\n            t[ind] = m[(x_ind + 1,) + y_ind] + (\n                (f2[ind] / f12[ind])\n                * (m[(x_ind + 2,) + y_ind] - m[(x_ind + 1,) + y_ind])\n            )\n\n        super().__init__(x, y, t, axis=0, extrapolate=extrapolate)\n        self.axis = axis\n", "comment": "Possibly?\r\n```suggestion\r\n            mmax = xp.max(f12) if f12.size > 0 else -xp.inf\r\n            ind = xp.nonzero(f12 > break_mult * mmax)\r\n```", "comment_author": "j-bowhay", "comment_url": "https://github.com/scipy/scipy/pull/23135#discussion_r2249447804", "type": "performance"}
{"repo": "scipy/scipy", "pr_id": 23504, "file_path": "scipy/stats/_continuous_distns.py", "diff_hunk": "@@ -9994,6 +9994,15 @@ def _entropy(self, c, d):\n         # the following result.\n         return 0.5 * (1.0-d+c) / (1.0+d-c) + np.log(0.5 * (1.0+d-c))\n \n+    def _fitstart(self, data):\n+        \"\"\"Override _fitstart to provide sensible default starting values for c and d.\n+        \n+        The default starting values c=0.33, d=0.66 provide better convergence\n+        than the generic defaults, especially for trapezoidal distributions\n+        that are not close to triangular.\n+        \"\"\"", "context": "    def _entropy(self, c, d):\n        # Using the parameterization from Wikipedia (van Dorp, 2003)\n        # with a=bottom left, c=top left, d=top right, b=bottom right\n        # gives a'=loc, b'=loc+c*scale, c'=loc+d*scale, d'=loc+scale,\n        # which for loc=0, scale=1 is a'=0, b'=c, c'=d, d'=1.\n        # Substituting into the entropy formula from Wikipedia gives\n        # the following result.\n        return 0.5 * (1.0-d+c) / (1.0+d-c) + np.log(0.5 * (1.0+d-c))\n\n", "comment": "This docstring isn't correct. The problem is not only with non-triangular data, but with any data, because the points in the initial simplex become c=1.05 and d=1.05., which are not valid parameters to the triangular distribution.\r\n\r\nAdditionally, I would suggest a shorter comment. The reason I would suggest that is that the comment's length should be proportional to how complex / hairy the associated code is. Proposed wording:\r\n\r\n```suggestion\r\n        # Arbitrary, but c=d=1 fails due to being on edge of bounds\r\n```", "comment_author": "nickodell", "comment_url": "https://github.com/scipy/scipy/pull/23504#discussion_r2302923717", "type": "documentation"}
{"repo": "sympy/sympy", "pr_id": 28185, "file_path": "sympy/physics/continuum_mechanics/beam.py", "diff_hunk": "@@ -765,8 +764,7 @@ def apply_load(self, value, start, order, end=None):\n         self._load += value*SingularityFunction(x, start, order)\n         self._original_load += value*SingularityFunction(x, start, order)\n \n-        if end:\n-            # load has an end point within the length of the beam.\n+        if end!=None:", "context": "    def apply_load(self, value, start, order, end=None):\n        \"\"\"\n        This method adds up the loads given to a particular beam object.\n\n        Parameters\n        ==========\n        value : Sympifyable\n            The value inserted should have the units [Force/(Distance**(n+1)]\n            where n is the order of applied load.\n            Units for applied loads:\n\n               - For moments: kN*m\n               - For point loads: kN\n               - For constant distributed load: kN/m\n               - For ramp loads: kN/m**2\n               - For parabolic ramp loads: kN/m**3\n               - And so on.\n\n        start : Sympifyable\n            The starting point of the applied load. For point moments and\n            point forces this is the location of application.\n        order : Integer\n            The order of the applied load.\n\n               - For moments, order = -2\n               - For point loads, order =-1\n               - For constant distributed load, order = 0\n               - For ramp loads, order = 1\n               - For parabolic ramp loads, order = 2\n               - ... so on.\n\n        end : Sympifyable, optional\n            An optional argument that can be used if the load has an end point\n            within the length of the beam.\n\n        Examples\n        ========\n        There is a beam of length 4 meters. A moment of magnitude 3 Nm is\n        applied in the clockwise direction at the starting point of the beam.\n        A point load of magnitude 4 N is applied from the top of the beam at\n        2 meters from the starting point and a parabolic ramp load of magnitude\n        2 N/m is applied below the beam starting from 2 meters to 3 meters\n        away from the starting point of the beam.\n\n        >>> from sympy.physics.continuum_mechanics.beam import Beam\n        >>> from sympy import symbols\n        >>> E, I = symbols('E, I')\n        >>> b = Beam(4, E, I)\n        >>> b.apply_load(-3, 0, -2)\n        >>> b.apply_load(4, 2, -1)\n        >>> b.apply_load(-2, 2, 2, end=3)\n        >>> b.load\n        -3*SingularityFunction(x, 0, -2) + 4*SingularityFunction(x, 2, -1) - 2*SingularityFunction(x, 2, 2) + 2*SingularityFunction(x, 3, 0) + 4*SingularityFunction(x, 3, 1) + 2*SingularityFunction(x, 3, 2)\n\n        \"\"\"\n        x = self.variable\n        value = sympify(value)\n        start = sympify(start)\n        order = sympify(order)\n\n        self._applied_loads.append((value, start, order, end))\n        self._load += value*SingularityFunction(x, start, order)\n        self._original_load += value*SingularityFunction(x, start, order)\n\n        if end:\n            # load has an end point within the length of the beam.\n            self._handle_end(x, value, start, order, end, type=\"apply\")\n", "comment": "The idiomatic way to write this is `if end is not None`.", "comment_author": "oscarbenjamin", "comment_url": "https://github.com/sympy/sympy/pull/28185#discussion_r2414465139", "type": "style"}
{"repo": "sympy/sympy", "pr_id": 28185, "file_path": "sympy/physics/continuum_mechanics/tests/test_beam.py", "diff_hunk": "@@ -5,7 +5,7 @@\n from sympy.sets.sets import Interval\n from sympy.simplify.simplify import simplify\n from sympy.physics.continuum_mechanics.beam import Beam\n-from sympy.functions import SingularityFunction, Piecewise, meijerg, Abs, log, sqrt\n+from sympy.functions import SingularityFunction, Piecewise, meijerg, Abs, log,sqrt,factorial", "context": "from sympy.core.function import expand\nfrom sympy.core.numbers import (Rational, pi)\nfrom sympy.core.singleton import S\nfrom sympy.core.symbol import (Symbol, symbols)\nfrom sympy.sets.sets import Interval\nfrom sympy.simplify.simplify import simplify\nfrom sympy.physics.continuum_mechanics.beam import Beam\nfrom sympy.functions import SingularityFunction, Piecewise, meijerg, Abs, log, sqrt\nfrom sympy.testing.pytest import raises\nfrom sympy.physics.units import meter, newton, kilo, giga, milli\nfrom sympy.physics.continuum_mechanics.beam import Beam3D\nfrom sympy.geometry import Circle, Polygon, Point2D, Triangle\nfrom sympy.core.sympify import sympify", "comment": "There should be a space after comma here. I don't know why you have deleted it. Please see PEP 8:\nhttps://peps.python.org/pep-0008/", "comment_author": "oscarbenjamin", "comment_url": "https://github.com/sympy/sympy/pull/28185#discussion_r2423221157", "type": "style"}
{"repo": "sympy/sympy", "pr_id": 28326, "file_path": "sympy/physics/control/lti.py", "diff_hunk": "@@ -570,7 +582,7 @@ def create_transfer_function(num, den, var, sampling_time=0):\n class TransferFunctionBase(SISOLinearTimeInvariant, ABC):\n     r\"\"\"\n     Base class for transfer tunction objects.\n-    This class is not meant to be used directly.\n+    This class is not meant to be used directly", "context": "def create_transfer_function(num, den, var, sampling_time=0):\n    \"\"\"\n    Creates a new transfer function object.\n    sampling_time == 0 means continuous time transfer function.\n    sampling_time > 0 means discrete time transfer function.\n\n    Parameters\n    ==========\n\n    num : Expr, Number\n        The numerator polynomial of the transfer function.\n    den : Expr, Number\n        The denominator polynomial of the transfer function.\n    var : Symbol\n        Complex variable of the Laplace or z transform used by the\n        polynomials of the transfer function.\n    sampling_time : Symbol, Number, optional\n        Default is 0.\n        Time interval between two consecutive sampling instants.\n        If sampling_time == 0, it is a continuous time transfer function,\n        else it is a discrete time transfer function.\n\n    Examples\n    ========\n\n    >>> from sympy.abc import s, z\n    >>> from sympy.physics.control.lti import create_transfer_function\n    >>> num = s + 5\n    >>> den = 3*s**2 + 2*s + 1\n    >>> tf = create_transfer_function(num, den, s)\n    >>> tf\n    TransferFunction(s + 5, 3*s**2 + 2*s + 1, s)\n    >>> num = z\n    >>> den = z + 1\n    >>> dtf = create_transfer_function(num, den, z, 0.1)\n    >>> dtf\n    DiscreteTransferFunction(z, z + 1, z, 0.1)\n\n    See Also\n    ========\n\n    TransferFunction, DiscreteTransferFunction\n\n    \"\"\"\n    if sampling_time == 0:\n        return TransferFunction(num, den, var)\n    return DiscreteTransferFunction(num, den, var, sampling_time)\n", "comment": "Leave the period there.", "comment_author": "moorepants", "comment_url": "https://github.com/sympy/sympy/pull/28326#discussion_r2310025106", "type": "style"}
{"repo": "scipy/scipy", "pr_id": 23662, "file_path": "scipy/signal/_savitzky_golay.py", "diff_hunk": "@@ -115,36 +118,41 @@ def savgol_coeffs(window_length, polyorder, deriv=0, delta=1.0, pos=None,\n     if use not in ['conv', 'dot']:\n         raise ValueError(\"`use` must be 'conv' or 'dot'\")\n \n+    # cf windows/_windows.py\n+    xp = np_compat if xp is None else array_namespace(xp.empty(0))\n+\n     if deriv > polyorder:\n-        coeffs = np.zeros(window_length)\n+        coeffs = xp.zeros(window_length, dtype=xp.float64, device=device)\n         return coeffs\n \n     # Form the design matrix A. The columns of A are powers of the integers\n     # from -pos to window_length - pos - 1. The powers (i.e., rows) range\n     # from 0 to polyorder. (That is, A is a vandermonde matrix, but not\n     # necessarily square.)\n-    x = np.arange(-pos, window_length - pos, dtype=float)\n+    x = xp.arange(-pos, window_length - pos, dtype=xp.float64, device=device)\n \n     if use == \"conv\":\n         # Reverse so that result can be used in a convolution.\n-        x = x[::-1]\n+        x = xp.flip(x)\n \n-    order = np.arange(polyorder + 1).reshape(-1, 1)\n+    order = xp.reshape(\n+        xp.arange(polyorder + 1, dtype=xp.float64, device=device), (-1, 1)\n+    )\n     A = x ** order\n \n     # y determines which order derivative is returned.\n-    y = np.zeros(polyorder + 1)\n+    y = xp.zeros(polyorder + 1, dtype=xp.float64, device=device)\n     # The coefficient assigned to y[deriv] scales the result to take into\n     # account the order of the derivative and the sample spacing.\n-    y[deriv] = float_factorial(deriv) / (delta ** deriv)\n+    y = xpx.at(y, deriv).set(float_factorial(deriv) / (delta ** deriv))\n \n     # Find the least-squares solution of A*c = y\n-    coeffs, _, _, _ = lstsq(A, y)\n+    coeffs = xp.linalg.pinv(A) @ y\n ", "context": "def savgol_coeffs(window_length, polyorder, deriv=0, delta=1.0, pos=None,\n                  use=\"conv\"):\n    \"\"\"Compute the coefficients for a 1-D Savitzky-Golay FIR filter.\n\n    Parameters\n    ----------\n    window_length : int\n        The length of the filter window (i.e., the number of coefficients).\n    polyorder : int\n        The order of the polynomial used to fit the samples.\n        `polyorder` must be less than `window_length`.\n    deriv : int, optional\n        The order of the derivative to compute. This must be a\n        nonnegative integer. The default is 0, which means to filter\n        the data without differentiating.\n    delta : float, optional\n        The spacing of the samples to which the filter will be applied.\n        This is only used if deriv > 0.\n    pos : int or None, optional\n        If pos is not None, it specifies evaluation position within the\n        window. The default is the middle of the window.\n    use : str, optional\n        Either 'conv' or 'dot'. This argument chooses the order of the\n        coefficients. The default is 'conv', which means that the\n        coefficients are ordered to be used in a convolution. With\n        use='dot', the order is reversed, so the filter is applied by\n        dotting the coefficients with the data set.\n\n    Returns\n    -------\n    coeffs : 1-D ndarray\n        The filter coefficients.\n\n    See Also\n    --------\n    savgol_filter\n\n    Notes\n    -----\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by\n    Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8),\n    pp 1627-1639.\n    Jianwen Luo, Kui Ying, and Jing Bai. 2005. Savitzky-Golay smoothing and\n    differentiation filter for even number data. Signal Process.\n    85, 7 (July 2005), 1429-1434.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.signal import savgol_coeffs\n    >>> savgol_coeffs(5, 2)\n    array([-0.08571429,  0.34285714,  0.48571429,  0.34285714, -0.08571429])\n    >>> savgol_coeffs(5, 2, deriv=1)\n    array([ 2.00000000e-01,  1.00000000e-01,  2.07548111e-16, -1.00000000e-01,\n           -2.00000000e-01])\n\n    Note that use='dot' simply reverses the coefficients.\n\n    >>> savgol_coeffs(5, 2, pos=3)\n    array([ 0.25714286,  0.37142857,  0.34285714,  0.17142857, -0.14285714])\n    >>> savgol_coeffs(5, 2, pos=3, use='dot')\n    array([-0.14285714,  0.17142857,  0.34285714,  0.37142857,  0.25714286])\n    >>> savgol_coeffs(4, 2, pos=3, deriv=1, use='dot')\n    array([0.45,  -0.85,  -0.65,  1.05])\n\n    `x` contains data from the parabola x = t**2, sampled at\n    t = -1, 0, 1, 2, 3.  `c` holds the coefficients that will compute the\n    derivative at the last position.  When dotted with `x` the result should\n    be 6.\n\n    >>> x = np.array([1, 0, 1, 4, 9])\n    >>> c = savgol_coeffs(5, 2, pos=4, deriv=1, use='dot')\n    >>> c.dot(x)\n    6.0\n    \"\"\"\n\n    # An alternative method for finding the coefficients when deriv=0 is\n    #    t = np.arange(window_length)\n    #    unit = (t == pos).astype(int)\n    #    coeffs = np.polyval(np.polyfit(t, unit, polyorder), t)\n    # The method implemented here is faster.\n\n    # To recreate the table of sample coefficients shown in the chapter on\n    # the Savitzy-Golay filter in the Numerical Recipes book, use\n    #    window_length = nL + nR + 1\n    #    pos = nL + 1\n    #    c = savgol_coeffs(window_length, M, pos=pos, use='dot')\n\n    if polyorder >= window_length:\n        raise ValueError(\"polyorder must be less than window_length.\")\n\n    halflen, rem = divmod(window_length, 2)\n\n    if pos is None:\n        if rem == 0:\n            pos = halflen - 0.5\n        else:\n            pos = halflen\n\n    if not (0 <= pos < window_length):\n        raise ValueError(\"pos must be nonnegative and less than \"\n                         \"window_length.\")\n\n    if use not in ['conv', 'dot']:\n        raise ValueError(\"`use` must be 'conv' or 'dot'\")\n\n    if deriv > polyorder:\n        coeffs = np.zeros(window_length)\n        return coeffs\n\n    # Form the design matrix A. The columns of A are powers of the integers\n    # from -pos to window_length - pos - 1. The powers (i.e., rows) range\n    # from 0 to polyorder. (That is, A is a vandermonde matrix, but not\n    # necessarily square.)\n    x = np.arange(-pos, window_length - pos, dtype=float)\n\n    if use == \"conv\":\n        # Reverse so that result can be used in a convolution.\n        x = x[::-1]\n\n    order = np.arange(polyorder + 1).reshape(-1, 1)\n    A = x ** order\n\n    # y determines which order derivative is returned.\n    y = np.zeros(polyorder + 1)\n    # The coefficient assigned to y[deriv] scales the result to take into\n    # account the order of the derivative and the sample spacing.\n    y[deriv] = float_factorial(deriv) / (delta ** deriv)\n\n    # Find the least-squares solution of A*c = y\n    coeffs, _, _, _ = lstsq(A, y)\n\n    return coeffs\n\n", "comment": "The numerics of [pinv] appear to me to be quite different than [lstsq] (which is based on LAPACK's [dgelsd] and friends). I have no intuition if it makes a palpable difference hereI doubt thou  that the test cases are suited to reveal such problems. \n\nIt might be helpful, to mention the change in the release notes in case somebody's unit tests break. \n\n[pinv]: https://scipy.github.io/devdocs/reference/generated/scipy.linalg.pinv.html\n[lstsq]: https://scipy.github.io/devdocs/reference/generated/scipy.linalg.lstsq.html\n[dgelsd]: https://www.netlib.org/lapack/explore-html/d9/d67/group__gelsd_ga0bee7e1b9e7e43f59ecf2419b2759c42.html", "comment_author": "DietBru", "comment_url": "https://github.com/scipy/scipy/pull/23662#discussion_r2384011349", "type": "performance"}
{"repo": "scipy/scipy", "pr_id": 23772, "file_path": "scipy/stats/_resampling.py", "diff_hunk": "@@ -1561,24 +1566,29 @@ def _calculate_null_samples(data, statistic, n_permutations, batch,\n     # strategy except the roles of samples and observations are flipped.\n     # So swap these axes, then we'll use the function for the \"pairings\"\n     # strategy to do all the work!\n-    data = np.swapaxes(data, 0, -1)\n+    data = xp.stack(data, axis=0)\n+    data = xp_swapaxes(data, 0, -1, xp=xp)\n \n     # (Of course, the user's statistic doesn't know what we've done here,\n     # so we need to pass it what it's expecting.)\n     def statistic_wrapped(*data, axis):\n-        data = np.swapaxes(data, 0, -1)\n+        # can we do this without converting back and forth between\n+        # array and list?\n+        data = xp.stack(data, axis=0)\n+        data = xp_swapaxes(data, 0, -1, xp=xp)\n         if n_samples == 1:\n-            data = data[0:1]\n+            data = data[0:1, ...]\n+        data = [data[i, ...] for i in range(data.shape[0])]\n         return statistic(*data, axis=axis)\n \n+    data = [data[i, ...] for i in range(data.shape[0])]", "context": "def _calculate_null_samples(data, statistic, n_permutations, batch,\n                            rng=None):\n    \"\"\"\n    Calculate null distribution for paired-sample tests.\n    \"\"\"\n    n_samples = len(data)\n\n    # By convention, the meaning of the \"samples\" permutations type for\n    # data with only one sample is to flip the sign of the observations.\n    # Achieve this by adding a second sample - the negative of the original.\n    if n_samples == 1:\n        data = [data[0], -data[0]]\n\n    # The \"samples\" permutation strategy is the same as the \"pairings\"\n    # strategy except the roles of samples and observations are flipped.\n    # So swap these axes, then we'll use the function for the \"pairings\"\n    # strategy to do all the work!\n    data = np.swapaxes(data, 0, -1)\n\n    # (Of course, the user's statistic doesn't know what we've done here,\n    # so we need to pass it what it's expecting.)\n    def statistic_wrapped(*data, axis):\n        data = np.swapaxes(data, 0, -1)\n        if n_samples == 1:\n            data = data[0:1]\n        return statistic(*data, axis=axis)\n\n    return _calculate_null_pairings(data, statistic_wrapped, n_permutations,\n                                    batch, rng)\n\n", "comment": "lots of manual loops -- \"ok\" for performance?", "comment_author": "tylerjereddy", "comment_url": "https://github.com/scipy/scipy/pull/23772#discussion_r2476428966", "type": "performance"}
{"repo": "pydantic/pydantic-ai", "pr_id": 2270, "file_path": "pydantic_ai_slim/pydantic_ai/messages.py", "diff_hunk": "@@ -106,11 +109,18 @@ class FileUrl(ABC):\n     - `GoogleModel`: `VideoUrl.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing\n     \"\"\"\n \n-    @property\n     @abstractmethod\n-    def media_type(self) -> str:\n+    def _infer_media_type_from_url(self) -> str:\n         \"\"\"Return the media type of the file, based on the url.\"\"\"\n \n+    @property\n+    def media_type(self) -> str:\n+        \"\"\"Return the media type of the file, based on the url or the provided `_media_type`.\"\"\"\n+        if self._media_type is not None:\n+            return self._media_type\n+        else:\n+            return self._infer_media_type_from_url()", "context": "    def otel_event(self, settings: InstrumentationSettings) -> Event:\n        return Event(\n            'gen_ai.system.message',\n            body={'role': 'system', **({'content': self.content} if settings.include_content else {})},\n        )\n", "comment": "We can use `return self._media_type or self._infer_media_type_from_url()` here", "comment_author": "DouweM", "comment_url": "https://github.com/pydantic/pydantic-ai/pull/2270#discussion_r2222881594", "type": "style"}
{"repo": "pydantic/pydantic-ai", "pr_id": 3367, "file_path": "tests/models/test_bedrock.py", "diff_hunk": "@@ -1127,11 +1198,13 @@ async def get_user_country() -> str:\n         return 'Mexico'\n \n     result = await agent.run('What is the largest city in the user country?')\n-    assert result.output == snapshot(\"\"\"\\\n+    assert result.output == snapshot(\n+        \"\"\"\\\n Based on your location in Mexico, the largest city is Mexico City (Ciudad de Mxico). It's not only the capital but also the most populous city in Mexico with a metropolitan area population of over 21 million people, making it one of the largest urban agglomerations in the world.\n \n Mexico City is an important cultural, financial, and political center for the country and has a rich history dating back to the Aztec empire when it was known as Tenochtitln.\\\n-\"\"\")\n+\"\"\"\n+    )", "context": "    def instructions() -> str:\n        return 'You are a helpful assistant.'\n", "comment": "These change are a bit weird and I'm surprised our linter isn't complaining about them. Can you please revert them and keep the `\"\"\"` on the same line as the `(` or `)`?\r\n\r\nAfter that I'll merge :)", "comment_author": "DouweM", "comment_url": "https://github.com/pydantic/pydantic-ai/pull/3367#discussion_r2519403754", "type": "style"}
{"repo": "redis/redis-py", "pr_id": 3768, "file_path": "redis/asyncio/multidb/healthcheck.py", "diff_hunk": "@@ -67,9 +71,107 @@ async def _returns_echoed_message(self, database) -> bool:\n             # For a cluster checks if all nodes are healthy.\n             all_nodes = database.client.get_nodes()\n             for node in all_nodes:\n-                actual_message = await node.redis_connection.execute_command(\"ECHO\", \"healthcheck\")\n+                actual_message = await node.execute_command(\"ECHO\" ,\"healthcheck\")", "context": "    def __init__(\n        self,\n        retry: Retry = Retry(retries=DEFAULT_HEALTH_CHECK_RETRIES, backoff=DEFAULT_HEALTH_CHECK_BACKOFF)", "comment": "the space should be after the comma", "comment_author": "petyaslavova", "comment_url": "https://github.com/redis/redis-py/pull/3768#discussion_r2351866737", "type": "style"}
{"repo": "redis/redis-py", "pr_id": 3696, "file_path": "redis/multidb/failure_detector.py", "diff_hunk": "@@ -51,12 +58,16 @@ def register_failure(self, database, exception: Exception, cmd: tuple) -> None:\n             else:\n                 self._failures_within_duration.append((datetime.now(), cmd))\n \n-        self._check_threshold(database)\n+        self._check_threshold()\n+\n+    def set_command_executor(self, command_executor) -> None:\n+        self._command_executor = command_executor\n \n-    def _check_threshold(self, database):\n+    def _check_threshold(self):\n         with self._lock:\n             if len(self._failures_within_duration) >= self._threshold:\n-                database.circuit.state = CBState.OPEN\n+                if self._command_executor and self._command_executor.active_database:", "context": "", "comment": "Is it possible to have self._command_executor = None here? Or not having an active database?", "comment_author": "petyaslavova", "comment_url": "https://github.com/redis/redis-py/pull/3696#discussion_r2225450152", "type": "logic"}
{"repo": "Lightning-AI/litgpt", "pr_id": 1891, "file_path": "litgpt/model.py", "diff_hunk": "@@ -375,7 +446,7 @@ def forward(\n         y = self.scaled_dot_product_attention(q, k, v, mask)\n \n         # Re-assemble all head outputs side by side.\n-        y = y.reshape(B, T, self.config.head_size * self.config.n_head)\n+        y = y.flatten(start_dim=-2, end_dim=-1)", "context": "    def forward(\n        self,\n        x: torch.Tensor,\n        cos: torch.Tensor,\n        sin: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        input_pos: Optional[torch.Tensor] = None,", "comment": "Here it's a bit over-engineered .\r\n\r\nOverall, I think that the existing one is easier to read and gives you info of the final shape.", "comment_author": "Andrei-Aksionov", "comment_url": "https://github.com/Lightning-AI/litgpt/pull/1891#discussion_r1899797044", "type": "style"}
{"repo": "Lightning-AI/litgpt", "pr_id": 1745, "file_path": "litgpt/model.py", "diff_hunk": "@@ -108,12 +108,39 @@ def from_name(cls, name: str, **kwargs: Any) -> Self:\n         return cls(Config.from_name(name, **kwargs))\n \n     def rope_cache(self, device: Optional[torch.device] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n+\n+        if self.config.rope_adjustments is None:\n+            extra_config = None\n+\n+        else:\n+            adjusted_params_required = [\"factor\", \"low_freq_factor\", \"high_freq_factor\", \"original_max_seq_len\"]\n+            params_present = [param in self.config.rope_adjustments for param in adjusted_params_required]\n+            num_params_present = sum(params_present)\n+\n+            if num_params_present == 0:\n+                extra_config = None  # uses standard RoPE\n+            elif num_params_present == 4:\n+                # These parameters should always be used together so that we don't interfere with standard rope\n+                extra_config = {\n+                    \"original_max_seq_len\": self.config.rope_adjustments[\"original_max_seq_len\"],\n+                    \"factor\": self.config.rope_adjustments[\"factor\"],\n+                    \"low_freq_factor\": self.config.rope_adjustments[\"low_freq_factor\"],\n+                    \"high_freq_factor\": self.config.rope_adjustments[\"high_freq_factor\"],\n+                }\n+            else:\n+                # Some but not all parameters are specified; raise an error\n+                raise ValueError(\n+                    \"The following adjusted RoPE parameters are missing in rope_adjustments.\"", "context": "    def from_name(cls, name: str, **kwargs: Any) -> Self:\n        return cls(Config.from_name(name, **kwargs))\n", "comment": "I think you forgot to add the list of missing parameters in the error message.", "comment_author": "Andrei-Aksionov", "comment_url": "https://github.com/Lightning-AI/litgpt/pull/1745#discussion_r1788549967", "type": "logic"}
{"repo": "pyinstaller/pyinstaller", "pr_id": 2720, "file_path": "PyInstaller/lib/modulegraph/util.py", "diff_hunk": "@@ -12,14 +12,7 @@\n except NameError:\n     unicode = str\n \n-\n-if sys.version_info[0] == 2:\n-    from StringIO import StringIO as BytesIO\n-    from StringIO import StringIO\n-\n-else:\n-    from io import BytesIO, StringIO\n-\n+from ._compat import StringIO, BytesIO", "context": "import marshal\nimport warnings\n\ntry:\n    unicode\nexcept NameError:\n    unicode = str\n\n\nif sys.version_info[0] == 2:\n    from StringIO import StringIO as BytesIO\n    from StringIO import StringIO\n\nelse:\n    from io import BytesIO, StringIO\n\n\n\ndef imp_find_module(name, path=None):\n    \"\"\"\n    same as imp.find_module, but handles dotted names", "comment": "This is a good thing :-)", "comment_author": "htgoebel", "comment_url": "https://github.com/pyinstaller/pyinstaller/pull/2720#discussion_r132453969", "type": "other"}
{"repo": "ludwig-ai/ludwig", "pr_id": 3965, "file_path": "ludwig/api.py", "diff_hunk": "@@ -65,6 +65,7 @@\n from ludwig.globals import (\n     LUDWIG_VERSION,\n     MODEL_HYPERPARAMETERS_FILE_NAME,\n+    MODEL_WEIGHTS_FILE_NAME,", "context": "from ludwig.data.dataset.base import Dataset\nfrom ludwig.data.postprocessing import convert_predictions, postprocess\nfrom ludwig.data.preprocessing import load_metadata, preprocess_for_prediction, preprocess_for_training\nfrom ludwig.datasets import load_dataset_uris\nfrom ludwig.features.feature_registries import update_config_with_metadata, update_config_with_model\nfrom ludwig.globals import (\n    LUDWIG_VERSION,\n    MODEL_HYPERPARAMETERS_FILE_NAME,\n    set_disable_progressbar,\n    TRAIN_SET_METADATA_FILE_NAME,\n)\nfrom ludwig.models.base import BaseModel\nfrom ludwig.models.calibrator import Calibrator", "comment": "@sanjaydasgupta Thank you for this contribution. Overall it looks really good; I just want to clarify some points.  Would it be possible for you to print out the contents of the model_weights directory -- for example, it is not clear to me how `MODEL_WEIGHTS_FILE_NAME` helps in the path construction?  Let us re-examine it.  Thank you.", "comment_author": "alexsherstinsky", "comment_url": "https://github.com/ludwig-ai/ludwig/pull/3965#discussion_r1521938387", "type": "other"}
{"repo": "ludwig-ai/ludwig", "pr_id": 3866, "file_path": "ludwig/api.py", "diff_hunk": "@@ -538,7 +539,7 @@ def on_epoch_end(self, trainer, progress_tracker, save_path):\n                 # save description\n                 if self.backend.is_coordinator():\n                     description = get_experiment_description(\n-                        self.config_obj.to_dict(),\n+                        scrub_creds(self.config_obj.to_dict()),", "context": "                    def on_epoch_end(self, trainer, progress_tracker, save_path):\n                        upload_fn()\n", "comment": "To be even stricter, can we make `.to_dict()` call `scrub_creds()`?\r\n\r\nI imagine that any callers of `confict_obj.to_dict()` can do without rendered credentials. Consumers of credentials via config_obj should use the backend object directly, or via something like `config_obj.backend.credentials`.", "comment_author": "justinxzhao", "comment_url": "https://github.com/ludwig-ai/ludwig/pull/3866#discussion_r1445676246", "type": "security"}
{"repo": "great-expectations/great_expectations", "pr_id": 11357, "file_path": "great_expectations/datasource/fluent/sql_datasource.py", "diff_hunk": "@@ -622,7 +620,7 @@ def get_batch(self, batch_request: BatchRequest) -> Batch:\n         else:\n             sql_partitioner = None\n \n-        batch_spec_kwargs: dict[str, str | dict | None]\n+        batch_spec_kwargs: Dict[str, str | dict | None]", "context": "    def get_batch(self, batch_request: BatchRequest) -> Batch:\n        \"\"\"Batch that matches the BatchRequest.\n\n        Args:\n            batch_request: A batch request for this asset. Usually obtained by calling\n                build_batch_request on the asset.\n\n        Returns:\n            A list Batch that matches the options specified in the batch request.\n        \"\"\"\n        self._validate_batch_request(batch_request)\n\n        if batch_request.partitioner:\n            sql_partitioner = self.get_partitioner_implementation(batch_request.partitioner)\n        else:\n            sql_partitioner = None\n\n        batch_spec_kwargs: dict[str, str | dict | None]\n        requests = self._fully_specified_batch_requests(batch_request)\n        unsorted_metadata_dicts = [self._get_batch_metadata_from_batch_request(r) for r in requests]\n\n        if not unsorted_metadata_dicts:\n            raise NoAvailableBatchesError()\n\n        if sql_partitioner:\n            sorted_metadata_dicts = self.sort_batch_identifiers_list(\n                unsorted_metadata_dicts, sql_partitioner\n            )\n        else:\n            sorted_metadata_dicts = unsorted_metadata_dicts\n\n        sorted_metadata_dicts = sorted_metadata_dicts[batch_request.batch_slice]\n        batch_metadata = sorted_metadata_dicts[-1]\n\n        # we've sorted the metadata, but not the requests, so we need the index of our\n        # batch_metadata from the original unsorted list so that we get the right request\n        request_index = unsorted_metadata_dicts.index(batch_metadata)\n\n        request = requests[request_index]\n        batch_spec_kwargs = self._create_batch_spec_kwargs()\n        if sql_partitioner:\n            batch_spec_kwargs[\"partitioner_method\"] = sql_partitioner.method_name\n            batch_spec_kwargs[\"partitioner_kwargs\"] = sql_partitioner.partitioner_method_kwargs()\n            # mypy infers that batch_spec_kwargs[\"batch_identifiers\"] is a collection, but\n            # it is hardcoded to a dict above, so we cast it here.\n            cast(\"Dict\", batch_spec_kwargs[\"batch_identifiers\"]).update(\n                sql_partitioner.batch_parameters_to_batch_spec_kwarg_identifiers(request.options)\n            )\n        # Creating the batch_spec is our hook into the execution engine.\n        batch_spec = self._create_batch_spec(batch_spec_kwargs)\n        execution_engine: SqlAlchemyExecutionEngine = self.datasource.get_execution_engine()\n        data, markers = execution_engine.get_batch_data_and_markers(batch_spec=batch_spec)\n\n        batch_definition = LegacyBatchDefinition(\n            datasource_name=self.datasource.name,\n            data_connector_name=_DATA_CONNECTOR_NAME,\n            data_asset_name=self.name,\n            batch_identifiers=IDDict(batch_spec[\"batch_identifiers\"]),\n            batch_spec_passthrough=None,\n        )\n\n        return Batch(\n            datasource=self.datasource,\n            data_asset=self,\n            batch_request=request,\n            data=data,\n            metadata=batch_metadata,\n            batch_markers=markers,\n            batch_spec=batch_spec,\n            batch_definition=batch_definition,\n        )\n\n    @override", "comment": "Nit: `dict` is now preferred for typing since we no longer support python 3.8.", "comment_author": "billdirks", "comment_url": "https://github.com/great-expectations/great_expectations/pull/11357#discussion_r2320485145", "type": "style"}
{"repo": "speechbrain/speechbrain", "pr_id": 2760, "file_path": "speechbrain/lobes/models/huggingface_transformers/weighted_ssl.py", "diff_hunk": "@@ -78,21 +85,28 @@ def forward(self, wav, wav_lens=None):\n         \"\"\"\n \n         feats = self.model(wav)\n-        hidden_states = torch.stack(feats.hidden_states, dim=0).detach()\n+        if self.freeze:\n+            hidden_states = torch.stack(feats.hidden_states, dim=0).detach()\n+        else:\n+            hidden_states = torch.stack(feats.hidden_states, dim=0)\n+\n         # First dimension should be equal to the number of layers in the hparams\n         assert (\n             self.num_layers == hidden_states.shape[0]\n         ), \"Num layers not equal to num hidden states\"\n-        norm_weights = torch.nn.functional.softmax(self.weights, dim=-1)\n+\n         # Layernorming the layers representations if asked\n         if self.layernorm:\n-            hidden_states = [\n-                F.layer_norm(t, (t.shape[-1],)) for t in hidden_states\n-            ]\n+            hidden_states = torch.stack(\n+                [F.layer_norm(t, (t.shape[-1],)) for t in hidden_states], dim=0\n+            )", "context": "    def forward(self, wav, wav_lens=None):\n        \"\"\"This method outputs a weighted sum of the layer representations of the SSL encoder\n\n        Arguments\n        ---------\n        wav : torch.Tensor\n            The wavs\n        wav_lens : torch.Tensor\n            The wav lengths\n\n        Returns\n        -------\n        weighted_feats : torch.Tensor\n            The weighted sum of layer representations.\n        \"\"\"\n\n        feats = self.model(wav)\n        hidden_states = torch.stack(feats.hidden_states, dim=0).detach()\n        # First dimension should be equal to the number of layers in the hparams\n        assert (\n            self.num_layers == hidden_states.shape[0]\n        ), \"Num layers not equal to num hidden states\"\n        norm_weights = torch.nn.functional.softmax(self.weights, dim=-1)\n        # Layernorming the layers representations if asked\n        if self.layernorm:\n            hidden_states = [\n                F.layer_norm(t, (t.shape[-1],)) for t in hidden_states\n            ]\n        # Summing the weighted layers\n        weighted_feats = (\n            hidden_states * norm_weights[:, None, None, None]\n        ).sum(axis=0)\n        return weighted_feats\n", "comment": "I am wondering about something, but if `hidden_states` is already a tensor, why are we looping on the tensor to extract normalized vectors? Why not directly computing the layer norm on the right axis (e.g. -1)?", "comment_author": "Adel-Moumen", "comment_url": "https://github.com/speechbrain/speechbrain/pull/2760#discussion_r1850949432", "type": "performance"}
{"repo": "kedro-org/kedro", "pr_id": 4609, "file_path": "kedro/framework/context/context.py", "diff_hunk": "@@ -238,47 +238,65 @@ def _get_catalog(\n             save_version=save_version,\n         )\n \n-        feed_dict = self._get_feed_dict()\n-        catalog.add_feed_dict(feed_dict)\n+        parameters = self._get_parameters()\n+        self._add_parameters(catalog, parameters)\n+\n         _validate_transcoded_datasets(catalog)\n+\n         self._hook_manager.hook.after_catalog_created(\n             catalog=catalog,\n             conf_catalog=conf_catalog,\n             conf_creds=conf_creds,\n-            feed_dict=feed_dict,\n+            parameters=parameters,\n             save_version=save_version,\n             load_versions=load_versions,\n         )\n         return catalog\n \n-    def _get_feed_dict(self) -> dict[str, Any]:\n-        \"\"\"Get parameters and return the feed dictionary.\"\"\"\n+    def _add_parameters(\n+        self, catalog: CatalogProtocol, parameters: dict[str, Any]\n+    ) -> None:\n+        \"\"\"Add datasets to catalog using the data provided through `parameters`.\n+        `parameters` is a dictionary where the keys represent dataset names and the values can either be raw data or", "context": "    def _get_catalog(\n        self,\n        save_version: str | None = None,\n        load_versions: dict[str, str] | None = None,", "comment": "I'm not sure this is a valid description of the parameters, as we're always adding a dictionary.", "comment_author": "ElenaKhaustova", "comment_url": "https://github.com/kedro-org/kedro/pull/4609#discussion_r2027435152", "type": "documentation"}
{"repo": "kedro-org/kedro", "pr_id": 4563, "file_path": "features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py", "diff_hunk": "@@ -2,10 +2,10 @@\n from __future__ import annotations\n \n from kedro.framework.project import find_pipelines\n-from kedro.pipeline import Pipeline, pipeline\n+from kedro.pipeline import pipeline\n \n \n-def register_pipelines() -> dict[str, Pipeline]:\n+def register_pipelines() -> dict[str, pipeline]:", "context": "\"\"\"Project pipelines.\"\"\"\nfrom __future__ import annotations\n\nfrom kedro.framework.project import find_pipelines\nfrom kedro.pipeline import Pipeline, pipeline\n\n\ndef register_pipelines() -> dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"", "comment": "```suggestion\r\ndef register_pipelines() -> dict[str, Pipeline]:\r\n```\r\n\r\nThe type is still `Pipeline`.", "comment_author": "idanov", "comment_url": "https://github.com/kedro-org/kedro/pull/4563#discussion_r2009172365", "type": "style"}
{"repo": "django-cms/django-cms", "pr_id": 8154, "file_path": "cms/models/pluginmodel.py", "diff_hunk": "@@ -321,7 +321,7 @@ def update(self, refresh=False, **fields):\n         return\n \n     def reload(self):\n-        return CMSPlugin.objects.get(pk=self.pk)\n+        return CMSPlugin.objects.select_related(\"parent\", \"placeholder\", \"placeholder\").get(pk=self.pk)", "context": "    def update(self, refresh=False, **fields):\n        CMSPlugin.objects.filter(pk=self.pk).update(**fields)\n        if refresh:\n            return self.reload()\n        return\n", "comment": "@fsbraun Is the second `placeholder` inside the `select_related` redundant?", "comment_author": "vinitkumar", "comment_url": "https://github.com/django-cms/django-cms/pull/8154#discussion_r1966573366", "type": "performance"}
{"repo": "triton-inference-server/server", "pr_id": 7842, "file_path": "build.py", "diff_hunk": "@@ -1772,6 +1772,8 @@ def create_docker_build_script(script_name, container_install_dir, container_ci_\n             runargs += [\"-v\", \"\\\\\\\\.\\pipe\\docker_engine:\\\\\\\\.\\pipe\\docker_engine\"]\n         else:\n             runargs += [\"-v\", \"/var/run/docker.sock:/var/run/docker.sock\"]\n+            if os.path.exists(os.path.expanduser(\"~/.docker/config.json\")):\n+                runargs += [\"-v\", os.path.expanduser(\"~/.docker/config.json:/root/.docker/config.json\")]", "context": "def create_docker_build_script(script_name, container_install_dir, container_ci_dir):\n    with BuildScript(\n        os.path.join(FLAGS.build_dir, script_name),\n        verbose=FLAGS.verbose,\n        desc=(\"Docker-based build script for Triton Inference Server\"),\n    ) as docker_script:\n        #\n        # Build base image... tritonserver_buildbase\n        #\n        docker_script.commentln(8)\n        docker_script.comment(\"Create Triton base build image\")\n        docker_script.comment(\n            \"This image contains all dependencies necessary to build Triton\"\n        )\n        docker_script.comment()\n\n        cachefrommap = [\n            \"tritonserver_buildbase\",\n            \"tritonserver_buildbase_cache0\",\n            \"tritonserver_buildbase_cache1\",\n        ]\n\n        baseargs = [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            \"tritonserver_buildbase\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile.buildbase\"),\n        ]\n\n        if not FLAGS.no_container_pull:\n            baseargs += [\n                \"--pull\",\n            ]\n\n        # Windows docker runs in a VM and memory needs to be specified\n        # explicitly (at least for some configurations of docker).\n        if target_platform() == \"windows\":\n            if FLAGS.container_memory:\n                baseargs += [\"--memory\", FLAGS.container_memory]\n\n        if target_platform() != \"windows\":\n            baseargs += [\"--cache-from={}\".format(k) for k in cachefrommap]\n\n        baseargs += [\".\"]\n\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(baseargs, check_exitcode=True)\n\n        #\n        # Build...\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Run build in tritonserver_buildbase container\")\n        docker_script.comment(\"Mount a directory into the container where the install\")\n        docker_script.comment(\"artifacts will be placed.\")\n        docker_script.comment()\n\n        # Don't use '-v' to communicate the built artifacts out of the\n        # build, because we want this code to work even if run within\n        # Docker (i.e. docker-in-docker) and not just if run directly\n        # from host.\n        runargs = [\n            \"docker\",\n            \"run\",\n            \"-w\",\n            \"/workspace/build\",\n            \"--name\",\n            \"tritonserver_builder\",\n        ]\n\n        if not FLAGS.no_container_interactive:\n            runargs += [\"-it\"]\n\n        if target_platform() == \"windows\":\n            if FLAGS.container_memory:\n                runargs += [\"--memory\", FLAGS.container_memory]\n            runargs += [\"-v\", \"\\\\\\\\.\\pipe\\docker_engine:\\\\\\\\.\\pipe\\docker_engine\"]\n        else:\n            runargs += [\"-v\", \"/var/run/docker.sock:/var/run/docker.sock\"]\n\n        runargs += [\"tritonserver_buildbase\"]\n\n        if target_platform() == \"windows\":\n            runargs += [\"powershell.exe\", \"-noexit\", \"-File\", \"./cmake_build.ps1\"]\n        else:\n            runargs += [\"./cmake_build\"]\n\n        # Remove existing tritonserver_builder container...\n        if target_platform() == \"windows\":\n            docker_script.cmd([\"docker\", \"rm\", \"tritonserver_builder\"])\n        else:\n            docker_script._file.write(\n                'if [ \"$(docker ps -a | grep tritonserver_builder)\" ]; then  docker rm -f tritonserver_builder; fi\\n'\n            )\n\n        docker_script.cmd(runargs, check_exitcode=True)\n\n        docker_script.cmd(\n            [\n                \"docker\",\n                \"cp\",\n                \"tritonserver_builder:/tmp/tritonbuild/install\",\n                FLAGS.build_dir,\n            ],\n            check_exitcode=True,\n        )\n        docker_script.cmd(\n            [\n                \"docker\",\n                \"cp\",\n                \"tritonserver_builder:/tmp/tritonbuild/ci\",\n                FLAGS.build_dir,\n            ],\n            check_exitcode=True,\n        )\n\n        #\n        # Final image... tritonserver\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Create final tritonserver image\")\n        docker_script.comment()\n\n        finalargs = [\n            \"docker\",\n            \"build\",\n        ]\n        if secrets != \"\":\n            finalargs += [\n                f\"--secret id=req,src={requirements}\",\n                f\"--build-arg VLLM_INDEX_URL={vllm_index_url}\",\n                f\"--build-arg PYTORCH_TRITON_URL={pytorch_triton_url}\",\n                f\"--build-arg BUILD_PUBLIC_VLLM={build_public_vllm}\",\n            ]\n        finalargs += [\n            \"-t\",\n            \"tritonserver\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile\"),\n            \".\",\n        ]\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(finalargs, check_exitcode=True)\n\n        #\n        # CI base image... tritonserver_cibase\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Create CI base image\")\n        docker_script.comment()\n\n        cibaseargs = [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            \"tritonserver_cibase\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile.cibase\"),\n            \".\",\n        ]\n\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(cibaseargs, check_exitcode=True)\n\n", "comment": "I don't really like this change. \r\nIt's kind of we allow to add something without validation. \r\nWe have no control of `~/.docker/config.json` entries there why we may be forced to review some false positive issues related to user environment.\r\n\r\nI may need a bit more context, sound like user looking for a proxy config during the build process, and I may be mistaken here but the issue happen when we run Docker-out-of-Docker configuration using `build.py` and user is behind the proxy.\r\n\r\nI suppose it's better to add a flag for users `--use-user-docker-config` and add value accordingly if flag is given.", "comment_author": "mc-nv", "comment_url": "https://github.com/triton-inference-server/server/pull/7842#discussion_r1880565285", "type": "security"}
{"repo": "voxel51/fiftyone", "pr_id": 6582, "file_path": "tests/unittests/threed/object_3d_tests.py", "diff_hunk": "@@ -314,3 +314,491 @@ def test_traverse(self):\n         self.assertListEqual(\n             list(sub.traverse(include_self=False)), objects[2:4]\n         )\n+\n+\n+class TestObject3DRemove(unittest.TestCase):", "context": "    def test_traverse(self):\n        objects = [Object3D(f\"object{i}\") for i in range(5)]\n        root = objects[0]\n        sub = objects[1]\n        sub.add(objects[2], objects[3])\n        root.add(sub, objects[4])\n\n        self.assertListEqual(list(root.traverse(include_self=True)), objects)\n        self.assertListEqual(\n            list(root.traverse(include_self=False)), objects[1:]\n        )\n\n        root.clear()\n        self.assertListEqual(list(root.traverse(include_self=False)), [])\n        self.assertListEqual(\n            list(sub.traverse(include_self=False)), objects[2:4]\n        )", "comment": "lot of tests, thanks! Looks like AI written tests lol, but if you did it all manually, good on you sir", "comment_author": "swheaton", "comment_url": "https://github.com/voxel51/fiftyone/pull/6582#discussion_r2539639840", "type": "other"}
{"repo": "voxel51/fiftyone", "pr_id": 6514, "file_path": "fiftyone/utils/torch.py", "diff_hunk": "@@ -850,8 +850,9 @@ def filter_classes(self):\n \n     @filter_classes.setter\n     def filter_classes(self, value):\n-        if value == self.classes:\n-            value = None\n+        if value and self.can_filter_classes:", "context": "    def filter_classes(self, value):\n        if value == self.classes:\n            value = None\n\n        self._filter_classes = value\n\n    @property", "comment": "The `can_filter_classes` check doesn't make sense here. We should either decide to raise an exception if the user tries to set `filter_classes` when `can_filter_classes == False`, or we should just let the setter work in all cases. With this proposal, it can be called but its behavior changes in a confusing way depending on `can_filter_classes=True/False`.", "comment_author": "brimoor", "comment_url": "https://github.com/voxel51/fiftyone/pull/6514#discussion_r2481920795", "type": "logic"}
{"repo": "voxel51/fiftyone", "pr_id": 6341, "file_path": "fiftyone/factory/repos/delegated_operation_doc.py", "diff_hunk": "@@ -95,7 +99,7 @@ def from_pymongo(self, doc: dict):\n         self.parent_id = doc.get(\"parent_id\", None)\n \n         # internal fields\n-        self.id = doc[\"_id\"]\n+        self.id = doc.get(\"_id\", doc.get(\"id\"))", "context": "    def from_pymongo(self, doc: dict):\n        # required fields\n        self.operator = doc.get(\"operator\")\n        self.queued_at = doc.get(\"queued_at\")\n        self.run_state = doc.get(\"run_state\")\n\n        # optional fields\n        self.delegation_target = doc.get(\"delegation_target\", None)\n        self.started_at = doc.get(\"started_at\", None)\n        self.completed_at = doc.get(\"completed_at\", None)\n        self.failed_at = doc.get(\"failed_at\", None)\n        self.scheduled_at = doc.get(\"scheduled_at\", None)\n        self.pinned = doc.get(\"pinned\", None)\n        self.dataset_id = doc.get(\"dataset_id\", None)\n        self.run_link = doc.get(\"run_link\", None)\n        self.log_upload_error = doc.get(\"log_upload_error\", None)\n        self.log_size = doc.get(\"log_size\", None)\n        self.log_path = doc.get(\"log_path\", None)\n        self.metadata = doc.get(\"metadata\", None)\n        self.label = doc.get(\"label\", None)\n        self.updated_at = doc.get(\"updated_at\", None)\n\n        # grouped fields\n        self.parent_id = doc.get(\"parent_id\", None)\n\n        # internal fields\n        self.id = doc[\"_id\"]\n        self._doc = doc\n\n        # nested fields\n        if (\n            \"context\" in doc\n            and doc[\"context\"] is not None\n            and \"request_params\" in doc[\"context\"]\n        ):\n            self.context = ExecutionContext(\n                request_params=doc[\"context\"][\"request_params\"],\n            )\n\n        if \"result\" in doc and doc[\"result\"] is not None:\n            res = ExecutionResult()\n            if \"result\" in doc[\"result\"]:\n                res.result = doc[\"result\"][\"result\"]\n            if \"error\" in doc[\"result\"]:\n                res.error = doc[\"result\"][\"error\"]\n\n            if res.result or res.error:\n                self.result = res\n\n        if \"status\" in doc and doc[\"status\"] is not None:\n            self.status = ExecutionProgress()\n            if \"progress\" in doc[\"status\"]:\n                self.status.progress = doc[\"status\"][\"progress\"]\n            if \"label\" in doc[\"status\"]:\n                self.status.label = doc[\"status\"][\"label\"]\n            if \"updated_at\" in doc[\"status\"]:\n                self.status.updated_at = doc[\"status\"][\"updated_at\"]\n\n        return self\n", "comment": "since _id is usually ObjectId and id is a string, are we saying self.id is either?", "comment_author": "kaixi-wang", "comment_url": "https://github.com/voxel51/fiftyone/pull/6341#discussion_r2345790467", "type": "logic"}
{"repo": "pypa/pip", "pr_id": 13570, "file_path": "src/pip/_internal/resolution/resolvelib/reporter.py", "diff_hunk": "@@ -1,19 +1,24 @@\n from __future__ import annotations\n \n from collections import defaultdict\n+from collections.abc import Mapping\n from logging import getLogger\n-from typing import Any\n+from typing import TYPE_CHECKING, Any\n \n from pip._vendor.resolvelib.reporters import BaseReporter\n \n-from .base import Candidate, Requirement\n+from .base import Candidate, Constraint, Requirement\n+\n+if TYPE_CHECKING:\n+    pass", "context": "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom typing import Any\n\nfrom pip._vendor.resolvelib.reporters import BaseReporter\n\nfrom .base import Candidate, Requirement\n\nlogger = getLogger(__name__)\n\n\nclass PipReporter(BaseReporter[Requirement, Candidate, str]):", "comment": "This `if TYPE_CHECKING` can now be removed.", "comment_author": "notatallshaw", "comment_url": "https://github.com/pypa/pip/pull/13570#discussion_r2427835177", "type": "style"}
{"repo": "pypa/pip", "pr_id": 11306, "file_path": "src/pip/_internal/commands/inspect.py", "diff_hunk": "@@ -94,4 +94,11 @@ def _dist_to_dict(self, dist: BaseDistribution) -> Dict[str, Any]:\n         # requested\n         if dist.installed_with_dist_info:\n             res[\"requested\"] = dist.requested\n+        # convert project_url to dict\n+        if \"project_url\" in res[\"metadata\"]:\n+            project_url_dict = {}\n+            for i in res[\"metadata\"][\"project_url\"]:\n+                name, url = i.split(\", \", 1)", "context": "    def _dist_to_dict(self, dist: BaseDistribution) -> Dict[str, Any]:\n        res: Dict[str, Any] = {\n            \"metadata\": dist.metadata_dict,\n            \"metadata_location\": dist.info_location,\n        }\n        # direct_url. Note that we don't have download_info (as in the installation\n        # report) since it is not recorded in installed metadata.\n        direct_url = dist.direct_url\n        if direct_url is not None:\n            res[\"direct_url\"] = direct_url.to_dict()\n        else:\n            # Emulate direct_url for legacy editable installs.\n            editable_project_location = dist.editable_project_location\n            if editable_project_location is not None:\n                res[\"direct_url\"] = {\n                    \"url\": path_to_url(editable_project_location),\n                    \"dir_info\": {\n                        \"editable\": True,\n                    },\n                }\n        # installer\n        installer = dist.installer\n        if dist.installer:\n            res[\"installer\"] = installer\n        # requested\n        if dist.installed_with_dist_info:\n            res[\"requested\"] = dist.requested\n        return res", "comment": "What if the value is incorrectly formatted? This will raise a ValueError. We shouldn't fail on invalid metadata, as there's a lot of legacy packages around that don't necessarily conform to the latest standards.", "comment_author": "pfmoore", "comment_url": "https://github.com/pypa/pip/pull/11306#discussion_r928871633", "type": "logic"}
{"repo": "pypa/pip", "pr_id": 13476, "file_path": "src/pip/_internal/metadata/pkg_resources.py", "diff_hunk": "@@ -125,7 +125,7 @@ def from_metadata_file_contents(\n         }\n         dist = pkg_resources.DistInfoDistribution(\n             location=filename,\n-            metadata=InMemoryMetadata(metadata_dict, filename),\n+            metadata=InMemoryMetadata(metadata_dict, filename),  # type: ignore[arg-type]", "context": "    def from_metadata_file_contents(\n        cls,\n        metadata_contents: bytes,\n        filename: str,\n        project_name: str,", "comment": "Personally, I always insist that a `type: ignore` is accompanied by a comment (and normally that is only allowed when there is a bug with the hints and/or `mypy`).\r\n\r\nMight be a good idea here too, as it would be easy for this to miss a real problem in the future.", "comment_author": "pelson", "comment_url": "https://github.com/pypa/pip/pull/13476#discussion_r2215271475", "type": "documentation"}
{"repo": "Theano/Theano", "pr_id": 3375, "file_path": "theano/scan_module/scan_opt.py", "diff_hunk": "@@ -568,7 +635,7 @@ def add_to_replace(y):\n         clean_replace_with_out = []\n \n         existent_nodes = [nd for nd in local_fgraph_topo\n-                          if nd not in to_remove_set]\n+                        if nd not in to_remove_set]", "context": "        def add_to_replace(y):\n            to_replace_set.add(y)\n            to_replace_map[y] = add_to_replace.n\n            add_to_replace.n += 1", "comment": "This change breaks the indentation", "comment_author": "carriepl", "comment_url": "https://github.com/Theano/Theano/pull/3375#discussion_r39203389", "type": "style"}
{"repo": "falconry/falcon", "pr_id": 2317, "file_path": "tests/test_response.py", "diff_hunk": "@@ -81,3 +74,14 @@ def test_response_option_mimetype_init(monkeypatch):\n     assert ro.static_media_types['.js'] == 'text/javascript'\n     assert ro.static_media_types['.json'] == 'application/json'\n     assert ro.static_media_types['.mjs'] == 'text/javascript'\n+\n+\n+@pytest.mark.parametrize('content', [b'', b'dummy content'])\n+def test_response_set_stream(resp, content):\n+    stream = BytesIO(content)\n+    content_length = len(content)\n+\n+    resp.set_stream(stream, content_length)\n+\n+    assert resp.stream == stream", "context": "def test_response_option_mimetype_init(monkeypatch):\n    mock = MagicMock()\n    mock.inited = False\n    mock.types_map = {'.js': 'application/javascript'}\n    monkeypatch.setattr('falcon.response.mimetypes', mock)\n\n    ro = ResponseOptions()\n\n    assert ro.static_media_types['.js'] == 'text/javascript'\n    assert ro.static_media_types['.json'] == 'application/json'\n    assert ro.static_media_types['.mjs'] == 'text/javascript'\n\n    mock.reset_mock()\n    mock.inited = True\n    ro = ResponseOptions()\n    mock.init.assert_not_called()\n\n    assert ro.static_media_types['.js'] == 'text/javascript'\n    assert ro.static_media_types['.json'] == 'application/json'\n    assert ro.static_media_types['.mjs'] == 'text/javascript'", "comment": "Thanks, looks great now bar one small nitpick: it is not very meaningful to compare instances of `BytesIO` this way. Maybe asserting `assert resp.stream is stream` would be more precise?", "comment_author": "vytas7", "comment_url": "https://github.com/falconry/falcon/pull/2317#discussion_r1741069299", "type": "other"}
{"repo": "py-pdf/pypdf", "pr_id": 3337, "file_path": "pypdf/_writer.py", "diff_hunk": "@@ -672,10 +671,12 @@ def insert_blank_page(\n                 and previous page does not exist.\n \n         \"\"\"\n-        if width is None or (height is None and index < self.get_num_pages()):\n+        if (width is None or height is None) and index < self.get_num_pages():", "context": "    def add_blank_page(\n        self, width: Optional[float] = None, height: Optional[float] = None", "comment": "Doesn't this change when this is executed?", "comment_author": "stefan6419846", "comment_url": "https://github.com/py-pdf/pypdf/pull/3337#discussion_r2173185367", "type": "logic"}
{"repo": "py-pdf/pypdf", "pr_id": 3209, "file_path": "pypdf/_writer.py", "diff_hunk": "@@ -3105,6 +3106,7 @@ def find_outline_item(\n                 o = cast(TreeObject, o[\"/Next\"])\n             else:\n                 return None\n+        return", "context": "    def reset_translation(\n        self, reader: Union[None, PdfReader, IndirectObject] = None", "comment": "There are two issues here:\r\n\r\n* We always return `None` - why does this check trigger at all?\r\n* If this is a valid case, we should have a test for it as well.", "comment_author": "stefan6419846", "comment_url": "https://github.com/py-pdf/pypdf/pull/3209#discussion_r2007799046", "type": "logic"}
{"repo": "py-pdf/pypdf", "pr_id": 3332, "file_path": "pypdf/filters.py", "diff_hunk": "@@ -95,6 +95,23 @@ def decompress(data: bytes) -> bytes:\n             # For larger files, use decompression object to enable buffered reading\n             return zlib.decompressobj().decompress(data)\n         except zlib.error:\n+            # First quick approach for known issue with faulty added bytes to the\n+            # tail of the encoded stream from early Adobe Distiller or Pitstop versions\n+            # with CR char as the default line separator (assumed by reverse engeneering)\n+            # that breaks the decoding process in the end.\n+            #\n+            # Try first to cut off some of the tail byte by byte, however limited to not\n+            # iterate through too many loops and kill the performance for large streams,\n+            # to then allow the final fall back to run. Added this intermediate attempt,\n+            # because starting from the head of the stream byte by byte kills completely\n+            # the performace for large streams (e.g. 6 MB) with the tail-byte-issue\n+            # and takes ages. This solution is really fast:\n+            max_tail_cut_off_bytes: int = 8\n+            for i in range(1, (max_tail_cut_off_bytes + 1 if len(data) > max_tail_cut_off_bytes else len(data))):", "context": "def decompress(data: bytes) -> bytes:\n    \"\"\"\n    Decompress the given data using zlib.\n\n    Attempts to decompress the input data using zlib.\n    If the decompression fails due to a zlib error, it falls back\n    to using a decompression object with a larger window size.\n\n    Args:\n        data: The input data to be decompressed.\n\n    Returns:\n        The decompressed data.\n\n    \"\"\"\n    try:\n        return zlib.decompress(data)\n    except zlib.error:\n        try:\n            # For larger files, use decompression object to enable buffered reading\n            return zlib.decompressobj().decompress(data)\n        except zlib.error:\n            # If still failing, then try with increased window size\n            d = zlib.decompressobj(zlib.MAX_WBITS | 32)\n            result_str = b\"\"\n            for b in [data[i : i + 1] for i in range(len(data))]:\n                try:\n                    result_str += d.decompress(b)\n                except zlib.error:\n                    pass\n            return result_str\n\n", "comment": "```suggestion\r\n            for i in range(1, min(max_tail_cut_off_bytes + 1, len(data))):\r\n```", "comment_author": "stefan6419846", "comment_url": "https://github.com/py-pdf/pypdf/pull/3332#discussion_r2168920865", "type": "performance"}
{"repo": "open-mmlab/mmsegmentation", "pr_id": 3300, "file_path": "tests/test_models/test_losses/test_inverseform_loss.py", "diff_hunk": "@@ -0,0 +1,56 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+\n+import os\n+\n+import pytest\n+import requests\n+import torch\n+from tqdm import tqdm\n+\n+\n+def test_inverseform_loss():\n+    from mmseg.models.losses import InverseFormLoss\n+\n+    # construct test data\n+    pred = torch.zeros(3, 1, 4, 4)\n+    target = torch.zeros(3, 1, 4, 4)\n+\n+    for i in range(3):\n+        pred[i, 0, i, :] = 1\n+        target[i, 0, :, i] = 1\n+        print(f'pred is:{pred[i]}, \\n target is {target[i]}')\n+\n+    # download pretrained model\n+    repo = 'https://github.com/Qualcomm-AI-research/InverseForm/'\n+    download_folder = 'releases/download/v1.0/distance_measures_regressor.pth'\n+    pretraind_model_url = repo + download_folder\n+    print(pretraind_model_url)\n+    os.makedirs('./checkpoints', exist_ok=True)\n+    inverseNet_path = './checkpoints/distance_measures_regressor.pth'\n+    if not os.path.exists(inverseNet_path):\n+        response = requests.get(pretraind_model_url, stream=True)\n+        total_size_in_bytes = int(response.headers.get('content-length', 0))\n+        block_size = 1024  # 1 Kibibyte\n+        progress_bar = tqdm(\n+            total=total_size_in_bytes, unit='iB', unit_scale=True)\n+        with open(inverseNet_path, 'wb') as file:\n+            for data in response.iter_content(block_size):\n+                progress_bar.update(len(data))\n+                file.write(data)\n+        progress_bar.close()\n+        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n+            print('ERROR, something went wrong')\n+\n+    # test inverseform_loss\n+    loss_class = InverseFormLoss(\n+        inverseNet_path=inverseNet_path, map_location='cpu')\n+    loss = loss_class(pred, target)\n+    print(loss)", "context": "", "comment": "might remove `print`", "comment_author": "xiexinch", "comment_url": "https://github.com/open-mmlab/mmsegmentation/pull/3300#discussion_r1349639875", "type": "style"}
{"repo": "open-mmlab/mmsegmentation", "pr_id": 3365, "file_path": "mmseg/datasets/hsi_drive.py", "diff_hunk": "@@ -0,0 +1,22 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+from mmseg.datasets import BaseSegDataset\n+from mmseg.registry import DATASETS\n+\n+classes_exp = ('unlabelled', 'road', 'road marks', 'vegetation',\n+               'painted metal', 'sky', 'concrete', 'pedestrian', 'water',\n+               'unpainted metal', 'glass')\n+palette_exp = [[0, 0, 0], [77, 77, 77], [255, 255, 255], [0, 255, 0],\n+               [255, 0, 0], [0, 0, 255], [102, 51, 0], [255, 255, 0],\n+               [0, 207, 250], [255, 166, 0], [0, 204, 204]]\n+\n+\n+@DATASETS.register_module()\n+class HSIDrive20Dataset(BaseSegDataset):\n+    METAINFO = dict(classes=classes_exp, palette=palette_exp)\n+", "context": "", "comment": "Please add a docstring to introduce this dataset, including a reference (paper link or website link).", "comment_author": "xiexinch", "comment_url": "https://github.com/open-mmlab/mmsegmentation/pull/3365#discussion_r1446835118", "type": "documentation"}
{"repo": "sktime/sktime", "pr_id": 8671, "file_path": "sktime/transformations/series/summarize.py", "diff_hunk": "@@ -219,6 +219,10 @@ class WindowSummarizer(BaseTransformer):\n         # CI and test flags\n         # -----------------\n         \"tests:core\": True,  # should tests be triggered by framework changes?\n+        # move skip config from tests._config to estimator tag per #8515", "context": "        \"enforce_index_type\": None,  # index type that needs to be enforced in X/y\n        \"fit_is_empty\": False,  # is fit empty and can be skipped? Yes = True\n        \"transform-returns-same-time-index\": False,\n        # does transform return have the same time index as input X\n        \"remember_data\": True,  # remember all data seen as _X\n        # CI and test flags\n        # -----------------\n        \"tests:core\": True,  # should tests be triggered by framework changes?\n    }\n\n    def __init__(\n        self,\n        lag_feature=None,", "comment": "please remove the comment that it has been moved, instead add a reference to the original issue that describes why it was skipped", "comment_author": "fkiraly", "comment_url": "https://github.com/sktime/sktime/pull/8671#discussion_r2263430152", "type": "documentation"}
{"repo": "sktime/sktime", "pr_id": 8642, "file_path": "sktime/classification/foundation_models/momentfm.py", "diff_hunk": "@@ -390,6 +390,17 @@ def get_test_params(cls, parameter_set=\"default\"):\n             `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n             `create_test_instance` uses the first (or only) dictionary in `params`\n         \"\"\"\n+        import platform\n+\n+        os = platform.system()\n+        if os == \"Darwin\":\n+            if _check_soft_dependencies(\"torch\", severity=\"none\"):\n+                import torch\n+\n+                torch.backends.mps.is_available = lambda: False", "context": "    def get_test_params(cls, parameter_set=\"default\"):\n        \"\"\"Return testing parameter settings for the estimator.\n\n        Parameters\n        ----------\n        parameter_set : str, default=\"default\"\n            Name of the set of test parameters to return, for use in tests. If no\n            special parameters are defined for a value, will return `\"default\"` set.\n\n        Returns\n        -------\n        params : dict or list of dict, default = {}\n            Parameters to create testing instances of the class\n            Each dict are parameters to construct an \"interesting\" test instance, i.e.,\n            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n            `create_test_instance` uses the first (or only) dictionary in `params`\n        \"\"\"\n        params_set = []\n        params1 = {\"to_cpu_after_fit\": True, \"train_val_split\": 0.0, \"batch_size\": 64}\n        params_set.append(params1)\n        params2 = {\n            \"batch_size\": 128,\n            \"to_cpu_after_fit\": True,\n            \"train_val_split\": 0.0,\n        }\n        params_set.append(params2)\n\n        return params_set\n\n", "comment": "what is this line doing? Is it overwriting the local `torch` installation? Seems dangerous. Is this an official or recommended way to interact with the backend?", "comment_author": "fkiraly", "comment_url": "https://github.com/sktime/sktime/pull/8642#discussion_r2249313365", "type": "logic"}
{"repo": "sktime/sktime", "pr_id": 8509, "file_path": "sktime/classification/deep_learning/inceptiontime.py", "diff_hunk": "@@ -104,6 +108,7 @@ def __init__(\n         verbose=False,\n         loss=\"categorical_crossentropy\",\n         metrics=None,\n+        class_weight=None", "context": "    def __init__(\n        self,\n        n_epochs=1500,\n        batch_size=64,\n        kernel_size=40,\n        n_filters=32,\n        use_residual=True,\n        use_bottleneck=True,\n        bottleneck_size=32,\n        depth=6,\n        callbacks=None,\n        random_state=None,\n        verbose=False,\n        loss=\"categorical_crossentropy\",\n        metrics=None,", "comment": "please add this to the docstring", "comment_author": "fkiraly", "comment_url": "https://github.com/sktime/sktime/pull/8509#discussion_r2198489150", "type": "documentation"}
{"repo": "unit8co/darts", "pr_id": 2766, "file_path": "darts/timeseries.py", "diff_hunk": "@@ -1018,9 +1024,10 @@ def from_group_dataframe(\n         extract_time_col = [] if time_col is None else [time_col]\n \n         if value_cols is None:\n-            value_cols = df.columns.drop(\n-                static_cov_cols + extract_metadata_cols + extract_time_col\n-            ).tolist()\n+            value_cols = list(\n+                set(df.columns)", "context": "    def from_group_dataframe(\n        cls,\n        df: pd.DataFrame,\n        group_cols: Union[list[str], str],\n        time_col: Optional[str] = None,\n        value_cols: Optional[Union[list[str], str]] = None,\n        static_cols: Optional[Union[list[str], str]] = None,\n        metadata_cols: Optional[Union[list[str], str]] = None,\n        fill_missing_dates: Optional[bool] = False,\n        freq: Optional[Union[str, int]] = None,\n        fillna_value: Optional[float] = None,\n        drop_group_cols: Optional[Union[list[str], str]] = None,\n        n_jobs: Optional[int] = 1,\n        verbose: Optional[bool] = False,\n        copy: bool = True,", "comment": "Using a `set()` might change the order of the columns, that is not desired", "comment_author": "dennisbader", "comment_url": "https://github.com/unit8co/darts/pull/2766#discussion_r2051440105", "type": "logic"}
{"repo": "bridgecrewio/checkov", "pr_id": 7056, "file_path": "checkov/terraform/checks/resource/aws/UnpatchedAuroraPostgresDB.py", "diff_hunk": "@@ -0,0 +1,21 @@\n+from checkov.common.models.enums import CheckResult, CheckCategories\n+from checkov.terraform.checks.resource.base_resource_check import BaseResourceCheck\n+\n+\n+class UnpatchedAuroraPostgresDB(BaseResourceCheck):", "context": "", "comment": "Looks good! Please add an inspected key for version", "comment_author": "tsmithv11", "comment_url": "https://github.com/bridgecrewio/checkov/pull/7056#discussion_r2087567065", "type": "other"}
{"repo": "bridgecrewio/checkov", "pr_id": 6992, "file_path": "checkov/common/goget/github/get_git.py", "diff_hunk": "@@ -81,17 +80,26 @@ def do_get(self) -> str:\n         return clone_dir\n \n     def _clone(self, git_url: str, clone_dir: str) -> None:\n-        self.logger.debug(f\"cloning {self.url if '@' not in self.url else self.url.split('@')[1]} to {clone_dir}\")\n+        logging.info(f\"cloning {git_url} to {clone_dir}\")", "context": "    def do_get(self) -> str:\n        if git_import_error is not None:\n            raise ImportError(\"Unable to load git module (is the git executable available?)\") \\\n                from git_import_error\n\n        git_url, internal_dir = self._source_subdir()\n\n        clone_dir = self.temp_dir + \"/clone/\" if self.create_clone_and_res_dirs else self.temp_dir\n        self._clone(git_url, clone_dir)\n\n        if internal_dir:\n            clone_dir = clone_dir + internal_dir\n\n        if self.create_clone_and_res_dirs:\n            result_dir = self.temp_dir + \"/result/\"\n            shutil.copytree(clone_dir, result_dir)\n            return result_dir\n\n        return clone_dir\n", "comment": "Why not use self.logger.info?", "comment_author": "omriyoffe-panw", "comment_url": "https://github.com/bridgecrewio/checkov/pull/6992#discussion_r1943032398", "type": "style"}
